{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8d3989f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "git+ ('ti+m', 'Verb+Past+A1sg')\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, request\n",
    "import re\n",
    "%run FiilEkAyrimi.ipynb\n",
    "#from FiilEkAyrimi import deger\n",
    "from difflib import get_close_matches\n",
    "import numpy as np\n",
    "#%run Untitled1.ipynb\n",
    "import random\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "## global tanımlamalar (ekler)\n",
    "\n",
    "ek_dosyalar = {\n",
    "        \"Opt\":[\"yalım\",\"yelim\",\"alım\",\"elim\"],\n",
    "        \"Opt+A2sg\":[\"yasın\",\"yesin\",\"asın\",\"esin\"],\n",
    "        \"Opt+A2pl\":[\"yasınız\",\"yesiniz\",\"asınız\",\"esiniz\"],\n",
    "        \"Opt+A1sg\":[\"yayım\",\"yeyim\",\"ayım\",\"eyim\"],\n",
    "        ## noun suffixes\n",
    "        \"Pl\": [\"lar\", \"ler\"],\n",
    "        ## case suffixes\n",
    "        \"Dat\": [\"ya\",\"ye\",\"e\",\"a\",\"na\",\"ne\"],\n",
    "        \"Loc\": [\"da\",\"de\",\"ta\",\"te\",\"nde\",\"nda\"],\n",
    "        \"Abl\": [\"dan\",\"den\",\"tan\",\"ten\",\"ndan\",\"nden\"],\n",
    "        \"Gen\": [\"nın\",\"nin\",\"nun\",\"nün\",\"ın\",\"in\",\"un\",\"ün\",\"yın\",\"yin\",\"yun\",\"yün\",\"ım\",\"im\",\"um\",\"üm\",\"m\"],\n",
    "        \"Acc\": [\"yı\",\"yi\",\"yu\",\"yü\",\"ı\",\"i\",\"u\",\"ü\",\"nı\",\"ni\",\"nu\",\"nü\"],\n",
    "        \"Inst\": [\"yla\",\"yle\",\"la\",\"le\"],\n",
    "        \"P1sg\": [\"ım\",\"im\",\"um\",\"üm\",\"m\",\"yum\"],\n",
    "        \"P2sg\": [\"ın\",\"in\",\"un\",\"ün\",\"n\",\"yun\"],\n",
    "        \"P3sg\": [\"sı\",\"si\",\"su\",\"sü\",\"ı\",\"i\",\"u\",\"ü\",\"yu\"],\n",
    "        \"P1pl\": [\"ımız\",\"imiz\",\"umuz\",\"ümüz\",\"mız\",\"miz\",\"muz\",\"müz\",\"yumuz\"],\n",
    "        \"P2pl\": [\"ınız\",\"iniz\",\"unuz\",\"ünüz\",\"nız\",\"niz\",\"nuz\",\"nüz\",\"yunuz\"],\n",
    "        \"P3pl\": [\"ı\", \"i\",\"u\",\"ü\",\"ları\",\"leri\"],\n",
    "        \"Dim\": [\"cık\",\"cik\",\"cuk\",\"cük\",\"çık\",\"çik\",\"çuk\",\"çük\",\"cığ\",\"ciğ\",\"cuğ\",\"cüğ\",\"çığ\",\"çiğ\",\"çuğ\",\"çüğ\"],\n",
    "        \"Dim2\": [\"cağız\", \"ceğiz\"],\n",
    "        \"With\":[\"lı\",\"li\",\"lu\",\"lü\"],\n",
    "        \"Without\": [\"sız\", \"siz\",\"suz\",\"süz\"],\n",
    "        \"Since\":[\"dır\",\"dir\",\"dur\",\"dür\",\"tır\",\"tir\",\"tur\",\"tür\"],\n",
    "        \"Related\": [\"sal\", \"sel\"],\n",
    "        \"Ness\":[\"lık\",\"lik\",\"luk\",\"lük\",\"lığ\",\"liğ\",\"luğ\",\"lüğ\"],\n",
    "        \"Acquire\": [\"lan\", \"len\"],\n",
    "        \"Apply\":[\"la\",\"le\"],\n",
    "        \"Resemb\": [\"ımsı\",\"imsi\",\"umsu\",\"ümsü\",\"msı\",\"msi\",\"msu\",\"msü\"],\n",
    "        \"Rel\":[\"kı\",\"ki\",\"ku\",\"kü\"],\n",
    "        \"By\": [\"ca\",\"ce\"],\n",
    "        \"Cmp\":[\"ca\",\"ce\"],\n",
    "        \"Agt\":[\"cı\",\"ci\",\"cu\",\"cü\",\"yıcı\",\"yici\",\"yucu\",\"yücü\"],\n",
    "        ## Adjective Suffixes\n",
    "        \"Become\":[\"laş\",\"leş\"],\n",
    "        \"Ly\":[\"ca\",\"ce\"],\n",
    "        \"Quite\":[\"ca\",\"ce\"],\n",
    "        ## number-person agreement suffixes\n",
    "        \"A1sg\": [\"yım\",\"yim\",\"yum\",\"yüm\",\"m\",\"ım\",\"im\",\"um\",\"üm\"],\n",
    "        \"A2sg\":[\"sın\",\"sin\",\"sun\",\"sün\",\"n\",\"ın\",\"in\",\"un\",\"ün\"],\n",
    "        \"A3sg\":[\"\"],\n",
    "        \"A1pl\": [\"yız\",\"yiz\",\"k\",\"yuz\",\"yüz\",\"ız\",\"iz\",\"uz\",\"üz\"],\n",
    "        \"A2pl\":[\"sınız\",\"siniz\",\"sunuz\",\"sünüz\",\"nız\",\"niz\",\"nuz\",\"nüz\",\"ınız\",\"iniz\",\"unuz\",\"ünüz\"],\n",
    "        \"A3pl\": [\"lar\",\"ler\"],\n",
    "        ### Numeral suffixes\n",
    "        \"Grouping\": [\"şar\",\"şer\",\"ız\",\"iz\",\"ar\",\"er\"],\n",
    "        \"Ordinal\":[\"ıncı\",\"inci\",\"uncu\",\"üncü\",\"ncı\",\"nci\",\"ncu\",\"ncü\"],\n",
    "        \"Division\":[\"da\",\"de\",\"ta\",\"te\"],\n",
    "\n",
    "        ### Derivational Suffixes\n",
    "        \n",
    "        \"Trap_Eki\":[\"a\",\"b\",\"c\",\"ç\",\"d\",\"e\",\"f\",\"g\",\"ğ\",\"h\",\"ı\",\"i\",\"j\",\"k\",\"l\",\"m\",\"n\",\"o\",\"ö\",\"p\",\"r\",\"s\",\"ş\",\"t\",\"u\",\"ü\",\"v\",\"y\",\"z\"],\n",
    "        \n",
    "    }\n",
    "\n",
    "\n",
    "## global tanimlamalar sonu\n",
    "\n",
    "def get_random_sentence(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        sentences = [line.strip() for line in file]\n",
    "    return random.choice(sentences)\n",
    "\n",
    "def find_longest_root(input_word, root_list):\n",
    "    longest_root = ''\n",
    "    for root in root_list:\n",
    "        if input_word.startswith(root) and len(root) > len(longest_root):\n",
    "            longest_root = root\n",
    "   \n",
    "    return longest_root\n",
    "\n",
    "def correct_input(input_word, root_list, suffix_list):\n",
    "    longest_root = find_longest_root(input_word, root_list)\n",
    "    print(longest_root,\"bu longest_root\")\n",
    "    geriyeKalanEk = input_word[len(longest_root):]\n",
    "    print(geriyeKalanEk,\"geriye kalan ek\")\n",
    "        \n",
    "    if(geriyeKalanEk):\n",
    "        suffix_correction = get_close_matches(geriyeKalanEk, suffix_list, n=1, cutoff=0.4)\n",
    "        corrected_suffix = suffix_correction[0] if suffix_correction else geriyeKalanEk\n",
    "        corrected_word = longest_root + corrected_suffix\n",
    "        \n",
    "    else:\n",
    "        corrected_word = input_word\n",
    "\n",
    "    return corrected_word\n",
    "\n",
    "\n",
    "def sentence_splitter(text):\n",
    "    sentence_endings = r\"[.!?]\"\n",
    "    sentences = re.split(sentence_endings, text)\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    return sentences\n",
    "\n",
    "def tokenizer(text):\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "def parcala_ve_kontrol_et(input_str):\n",
    "    \n",
    "    ozel_durumlar = {\n",
    "        #\"ımız\": [\"ım\", \"ız\"],\n",
    "        #\"sinler\":[\"s\",\"i\",\"n\",\"ler\"],\n",
    "        #\"leri\":[\"ler\",\"i\"],\n",
    "        #\"sının\":[\"sı\",\"nın\"],\n",
    "        #\"elim\":[\"e\",\"lim\"],\n",
    "\n",
    "    }\n",
    "    \n",
    "\n",
    "    sonuclar = []\n",
    "    en_uzun_ekler = []\n",
    "\n",
    "    while input_str:\n",
    "        bulundu = False\n",
    "        ozelBulundu = False\n",
    "        en_uzun_ek = \"\"\n",
    "        ek_adi_list = []\n",
    "\n",
    "        # Özel durum kontrolü\n",
    "        for ozel_ek, ayrilma_sekli in ozel_durumlar.items():\n",
    "            if input_str.startswith(ozel_ek):\n",
    "                for ayrilma in ayrilma_sekli:\n",
    "                    for ad, ekler in ek_dosyalar.items():\n",
    "                        if ayrilma in ekler and input_str.startswith(ayrilma):\n",
    "                            en_uzun_ek = ayrilma\n",
    "                            ek_adi = f\"({ad})\"\n",
    "                            ozelBulundu = True\n",
    "                            if ozelBulundu:\n",
    "                                input_str = input_str[len(en_uzun_ek):].strip()\n",
    "                                ek_adi_list.append(ek_adi)\n",
    "\n",
    "        if not ozelBulundu:\n",
    "            for ad, ekler in ek_dosyalar.items():\n",
    "                for ek in ekler:\n",
    "                    if input_str.startswith(ek) and len(ek) >= len(en_uzun_ek):\n",
    "                        en_uzun_ek = ek\n",
    "                        ek_adi = f\"({ad})\"\n",
    "                        bulundu = True\n",
    "\n",
    "            if bulundu:\n",
    "                input_str = input_str[len(en_uzun_ek):].strip()\n",
    "                for ad, ekler in ek_dosyalar.items():\n",
    "                    if en_uzun_ek in ekler:\n",
    "                        ek_adi_list.append(f\"({ad})\")\n",
    "            else:\n",
    "                ek_adi_list.append(f\"({input_str})\")\n",
    "\n",
    "        if ek_adi_list:\n",
    "            sonuclar.append((f\"-{en_uzun_ek} \", ek_adi_list))\n",
    "            en_uzun_ek = \"\"\n",
    "            ek_adi_list = []\n",
    "\n",
    "    print(sonuclar,\"YKAUP BABA SONUCLAR\")\n",
    "    return sonuclar\n",
    "\n",
    "\n",
    "def endswithKontrolu(input_str):\n",
    "    \n",
    "    ozel_durumlar = {\n",
    "        #\"ımız\": [\"ım\", \"ız\"],\n",
    "        #\"sinler\":[\"s\",\"i\",\"n\",\"ler\"],\n",
    "        #\"leri\":[\"ler\",\"i\"],\n",
    "        #\"sının\":[\"sı\",\"nın\"],\n",
    "        #\"elim\":[\"e\",\"lim\"],\n",
    "\n",
    "    }\n",
    "\n",
    "    sonuclar = []\n",
    "    en_uzun_ekler = []\n",
    "    \n",
    "    while input_str:\n",
    "        bulundu = False\n",
    "        ozelBulundu = False\n",
    "        en_uzun_ek = \"\"\n",
    "        ek_adi_list = []\n",
    "\n",
    "        # Özel durum kontrolü\n",
    "        for ozel_ek, ayrilma_sekli in ozel_durumlar.items():\n",
    "            if input_str.startswith(ozel_ek):\n",
    "                for ayrilma in ayrilma_sekli:\n",
    "                    for ad, ekler in ek_dosyalar.items():\n",
    "                        if ayrilma in ekler and input_str.startswith(ayrilma):\n",
    "                            en_uzun_ek = ayrilma\n",
    "                            ek_adi = f\"({ad})\"\n",
    "                            ozelBulundu = True\n",
    "                            if ozelBulundu:\n",
    "                                input_str = input_str[len(en_uzun_ek):].strip()\n",
    "                                ek_adi_list.append(ek_adi)\n",
    "\n",
    "        if not ozelBulundu:\n",
    "            for ad, ekler in ek_dosyalar.items():\n",
    "                for ek in ekler:\n",
    "                    if input_str.endswith(ek) and len(ek) >= len(en_uzun_ek):\n",
    "                        en_uzun_ek = ek\n",
    "                        ek_adi = f\"({ad})\"\n",
    "                        bulundu = True\n",
    "\n",
    "            if bulundu:\n",
    "                input_str = input_str[:-len(en_uzun_ek)].strip()\n",
    "                for ad, ekler in ek_dosyalar.items():\n",
    "                    if en_uzun_ek in ekler:\n",
    "                        ek_adi_list.append(f\"({ad})\")\n",
    "            else:\n",
    "                ek_adi_list.append(f\"({input_str})\")\n",
    "\n",
    "        if ek_adi_list:\n",
    "            sonuclar.insert(0, (f\"-{en_uzun_ek} \", ek_adi_list))\n",
    "            en_uzun_ek = \"\"\n",
    "            ek_adi_list = []\n",
    "\n",
    "    print(sonuclar,\"PUKAY BABA SONUCLAR\")\n",
    "    return sonuclar\n",
    "    \n",
    "def find_and_process_suffixes_v2(base_word, dataset_path):\n",
    "    def find_suffixes(word, suffixes):\n",
    "        found_suffixes = [suffix for suffix in suffixes if word.endswith(suffix)]\n",
    "        for x in found_suffixes:\n",
    "            print(x,\"yakup ek\")\n",
    "        return found_suffixes\n",
    "\n",
    "    def process_dataset(base_word, suffixes):\n",
    "        matches = []\n",
    "        suffix_categories = []\n",
    "        matches2 = []\n",
    "        suffix_categories2 = []\n",
    "        new_base_word=\"\"\n",
    "        \n",
    "        \n",
    "        print(base_word,\"process_dataya gelen base_word\")\n",
    "        print(suffixes,\"bu da process_dataya gelen suffixes\")\n",
    "        with open(dataset_path, 'r', encoding='utf-8') as dataset_file:\n",
    "            for line in dataset_file:\n",
    "                parts = line.strip().split()\n",
    "\n",
    "                if len(parts[0]):\n",
    "                    current_word, word_type = parts[0], parts[1:]\n",
    "                    \n",
    "                    if not suffixes:\n",
    "                        if base_word == current_word:\n",
    "                            matches.append(current_word)\n",
    "                            suffix_categories.append(f\"({word_type})\")\n",
    "                    \n",
    "                                \n",
    "                        \n",
    "                        \n",
    "                    for suffix in suffixes:\n",
    "\n",
    "                        \n",
    "                        if base_word == current_word:\n",
    "\n",
    "                            matches.append(current_word)\n",
    "                            suffix_categories.append(f\"({word_type})\")\n",
    "                            matches2.append(current_word)\n",
    "                            suffix_categories2.append(f\"({word_type})\")\n",
    "                            \n",
    "                            \n",
    "                            if any('CL_FIIL' in category for category in word_type):\n",
    "                                return matches, [\"CL_FIIL\"],\"\",\"\"\n",
    "                                print(\"eyw\")\n",
    "                            else:\n",
    "                                sonuclar = parcala_ve_kontrol_et(suffix)\n",
    "                                print(sonuclar,\"bunlar da sonuclar\")\n",
    "                                sonuclar2 = endswithKontrolu(suffix)\n",
    "                                print(sonuclar2,\"SONUCLAR 2\")\n",
    "                                for ek, dosya_adi in sonuclar:\n",
    "                                    matches.append(ek)\n",
    "                                    suffix_categories.append(f\"({dosya_adi})\")\n",
    "                                for ek,dosya_adi in sonuclar2:\n",
    "                                    print(\"BURAYA GIRIYON MU PEKI\")\n",
    "                                    matches2.append(ek)\n",
    "                                    suffix_categories2.append(f\"({dosya_adi})\")\n",
    "                        else:\n",
    "                            if base_word.endswith('b'):\n",
    "                                new_base_word = base_word[:-1] + 'p'\n",
    "                            elif base_word.endswith('c'):\n",
    "                                new_base_word = base_word[:-1] + 'ç'\n",
    "                            elif base_word.endswith('d'):\n",
    "                                new_base_word = base_word[:-1] + 't'\n",
    "                            elif base_word.endswith('ğ'):\n",
    "                                new_base_word = base_word[:-1] + 'k'\n",
    "                            if new_base_word == current_word:\n",
    "                                matches.append(current_word)\n",
    "                                suffix_categories.append(f\"({word_type})\")\n",
    "\n",
    "                                sonuclar = parcala_ve_kontrol_et(suffix)\n",
    "                                print(sonuclar,\"bunlar da sonuclar\")\n",
    "                                sonuclar2 = endswithKontrolu(suffix)\n",
    "                                print(sonuclar2,\"SONUCLAR 2\")\n",
    "                                for ek, dosya_adi in sonuclar:\n",
    "                                    matches.append(ek)\n",
    "                                    suffix_categories.append(f\"({dosya_adi})\")\n",
    "                                for ek,dosya_adi in sonuclar2:\n",
    "                                    print(\"BURAYA GIRIYON MU PEKI\")\n",
    "                                    matches2.append(ek)\n",
    "                                    suffix_categories2.append(f\"({dosya_adi})\")\n",
    "                                    \n",
    "        print(matches2,\"RETURN ETMEDEN ONCEKI MATCHES2\")\n",
    "        return matches, suffix_categories,matches2,suffix_categories2\n",
    "\n",
    "    ihtimalVarmi = False\n",
    "    fiilDondu=False\n",
    "    \n",
    "    unlu_dusmesi={\n",
    "        \"ağz\":\"ağız\",\n",
    "        \"aln\":\"alın\",\n",
    "        \"burn\":\"burun\",\n",
    "        \"bağr\":\"bağır\",\n",
    "        \"beyn\":\"beyin\",\n",
    "        \"göğs\":\"göğüs\",\n",
    "        \"karn\":\"karın\",\n",
    "        \"omz\":\"omuz\",\n",
    "        \"oğl\":\"oğul\",\n",
    "        \"gönl\":\"gönül\",\n",
    "        \"akl\":\"akıl\",\n",
    "        \"fikr\":\"fikir\",\n",
    "        \"cism\":\"cisim\",\n",
    "        \"ayr\":\"ayır\",\n",
    "        \"devr\":\"devir\",\n",
    "        \"çevr\":\"çevir\",\n",
    "        \"sıyr\":\"sıyır\",\n",
    "        \"kıvr\":\"kıvır\",\n",
    "        \"devr\":\"devir\",\n",
    "        \"kayb\":\"kayıp\",\n",
    "        \"haps\":\"hapis\",\n",
    "        \"sabr\":\"sabır\",\n",
    "        \"ufk\":\"ufuk\",\n",
    "        \"benz\":\"beniz\",\n",
    "        \"boyn\":\"boyun\",\n",
    "        \"bağr\":\"bağır\",\n",
    "        \"genz\":\"geniz\",\n",
    "        \"gönl\":\"gönül\",\n",
    "        \"oğl\":\"oğul\",\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "    with open('C:/endings.txt', 'r', encoding='utf-8') as endings_file:\n",
    "        suffixes = [line.strip() for line in endings_file]\n",
    "\n",
    "    eksizEslesme, suffix_categories,eksizEslesme2,suffix_categories2 = process_dataset(base_word, [])\n",
    "        \n",
    "    if any('CL_FIIL' in category for category in suffix_categories):\n",
    "        #return 'Bu bir fiil oldugu icin burasi lokmandan gelecek'\n",
    "        return deger(base_word)\n",
    "\n",
    "    all_suffixes = find_suffixes(base_word, suffixes)\n",
    "\n",
    "    if eksizEslesme:\n",
    "        return ''.join(eksizEslesme), ''.join(suffix_categories)\n",
    "    elif all_suffixes:\n",
    "        result_suffixes = []\n",
    "        \n",
    "        for suffix in sorted(all_suffixes, key=len, reverse=True):\n",
    "            print(suffix,\"yakuk\")\n",
    "            remaining_word = base_word[:-len(suffix)]\n",
    "            print(remaining_word,\"bu da remaining word\")\n",
    "            if remaining_word.endswith((\"iyor\", \"iyo\", \"üyor\", \"üyo\")):\n",
    "                if(not fiilDondu):\n",
    "                    remaining_word = remaining_word.replace(\"iyor\", \"e\").replace(\"iyo\", \"e\").replace(\"üyor\", \"e\").replace(\"üyo\", \"e\", 1)\n",
    "                    print(remaining_word,\"yeni remaining word bu\")\n",
    "            if remaining_word.endswith((\"ıyor\", \"ıyo\", \"uyor\", \"uyo\")):\n",
    "                if(not fiilDondu):\n",
    "                    remaining_word = remaining_word.replace(\"ıyor\", \"a\").replace(\"ıyo\", \"a\").replace(\"uyor\", \"a\").replace(\"uyo\", \"a\", 1)\n",
    "                    print(remaining_word,\"yeni remaining word bu\")\n",
    "                    \n",
    "            if remaining_word in unlu_dusmesi:\n",
    "                remaining_word = unlu_dusmesi[remaining_word]\n",
    "            \n",
    "            matches, suffix_categories,matches2,suffix_categories2 = process_dataset(remaining_word, [suffix])\n",
    "            print(matches,\"bu matches\")\n",
    "            print(suffix_categories,\"bu sofik kategori\")\n",
    "            print(matches2,\"BU MATCHES2\")\n",
    "\n",
    "            if matches:\n",
    "\n",
    "                if any(any(keyword in category for keyword in ('CL_FIIL', 'Apply', 'Become')) for category in suffix_categories):\n",
    "                    #return 'Bu bir fiil oldugu icin burasi lokmandan gelecek'\n",
    "                    if(ihtimalVarmi):\n",
    "                            result_suffixes.extend('<br>')\n",
    "                            result_suffixes.extend(f' <br> Eşleşen diğer bir seçenek: ')\n",
    "                            result_suffixes.extend(deger(base_word))\n",
    "                            result_suffixes.extend(' ')\n",
    "                            tempBaseWord=base_word\n",
    "                            fiilDondu=True\n",
    "                            return ''.join(result_suffixes)\n",
    "                        \n",
    "                    else:\n",
    "                        ihtimalVarmi = True\n",
    "                        print(deger(base_word),\"SFAJGJSDAGJASDLGASDKGJKSDLJL\")\n",
    "                        result_suffixes.extend(deger(base_word))\n",
    "                        result_suffixes.extend(' ')\n",
    "                        result_suffixes.extend('<br>')\n",
    "                        fiilDondu=True\n",
    "                        return ''.join(result_suffixes)\n",
    "                    \n",
    "                else:\n",
    "                    if(ihtimalVarmi):\n",
    "                        #if all(\"Trap_Eki\" not in suffix_category for suffix_category in suffix_categories):\n",
    "                            result_suffixes.extend('<br>')\n",
    "                            result_suffixes.extend(f' <br> Eşleşen diğer bir seçenek: ')\n",
    "                            result_suffixes.extend(matches)\n",
    "                            result_suffixes.extend(suffix_categories)\n",
    "                            result_suffixes.extend(' ')\n",
    "                            if(matches != matches2 and suffix_categories != suffix_categories2):\n",
    "                                result_suffixes.extend('<br>')\n",
    "                                result_suffixes.extend('<br>')\n",
    "                                result_suffixes.extend(matches2)\n",
    "                                result_suffixes.extend(suffix_categories2)\n",
    "                                result_suffixes.extend(' ')\n",
    "                    else:\n",
    "                        #if all(\"Trap_Eki\" not in suffix_category for suffix_category in suffix_categories):\n",
    "                        ihtimalVarmi=True\n",
    "                        result_suffixes.extend(matches)\n",
    "                        result_suffixes.extend(suffix_categories)\n",
    "                        result_suffixes.extend(' ')\n",
    "                        if(matches != matches2 and suffix_categories != suffix_categories2):\n",
    "                            result_suffixes.extend('<br>')\n",
    "                            result_suffixes.extend('<br>')\n",
    "                            result_suffixes.extend(matches2)\n",
    "                            result_suffixes.extend(suffix_categories2)\n",
    "                            result_suffixes.extend(' ')\n",
    "                        \n",
    "\n",
    "            else:\n",
    "                #kontrolVar=deger(base_word)\n",
    "                #if(kontrolVar):\n",
    "                    #result_suffixes.extend(deger(base_word))\n",
    "               # else:\n",
    "                    #return f\"{base_word} kelimesi için eşleşme bulunamadı.\"\n",
    "                print(\"sebze\")\n",
    "\n",
    "                        \n",
    "\n",
    "        return ''.join(result_suffixes)\n",
    "\n",
    "        \n",
    "        \n",
    "@app.route(\"/\", methods=[\"GET\", \"POST\"])\n",
    "def index():\n",
    "    results = []\n",
    "    root_list_path = 'C:/dataset.txt'\n",
    "    suffix_list_path = 'C:/endings.txt'\n",
    "    with open(root_list_path, 'r', encoding='utf-8') as root_file:\n",
    "        root_list = [line.strip().split()[0] for line in root_file]\n",
    "\n",
    "    with open(suffix_list_path, 'r', encoding='utf-8') as suffix_file:\n",
    "        suffix_list = [line.strip() for line in suffix_file]\n",
    "\n",
    "    if request.method == \"POST\":\n",
    "        if request.form.get(\"generate_random_sentence\"):\n",
    "            user_input = get_random_sentence('C:/turkceCumleler.txt')\n",
    "        else:\n",
    "            user_input = request.form.get(\"user_input\", \"\")\n",
    "    else:\n",
    "        user_input = \"\"\n",
    "\n",
    "    user_input = user_input.lower()\n",
    "    user_input = user_input.replace(\"'\", ' ').replace('\"', '').replace(',',' ').replace(';',' ')\n",
    "    normalizasyonAktif = request.form.get(\"normalizasyonAktif\")\n",
    "\n",
    "    sentences = sentence_splitter(user_input)\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokens = tokenizer(sentence)\n",
    "        processed_tokens = []\n",
    "\n",
    "        for token in tokens:\n",
    "            if normalizasyonAktif == \"True\":\n",
    "                corrected_word = correct_input(token, root_list, suffix_list)\n",
    "                result = find_and_process_suffixes_v2(corrected_word, 'C:/dataset.txt')\n",
    "                processed_tokens.append((corrected_word, result))\n",
    "            elif normalizasyonAktif == \"False\":\n",
    "                result = find_and_process_suffixes_v2(token, 'C:/dataset.txt')\n",
    "                processed_tokens.append((token, result))\n",
    "\n",
    "        results.append(processed_tokens)\n",
    "            \n",
    "    return render_template(\"index.html\", results=results, user_input=user_input)\n",
    "\n",
    "\n",
    "@app.route('/developers')\n",
    "def developers():\n",
    "    return render_template('developers.html')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True, use_reloader=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5777eac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e1e650",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
