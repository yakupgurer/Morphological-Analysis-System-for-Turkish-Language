{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5f2a7a0",
   "metadata": {},
   "source": [
    "# Sentence Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b4f68f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [sentence\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mif\u001b[39;00m sentence\u001b[38;5;241m.\u001b[39mstrip()] \n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sentences\n\u001b[1;32m---> 10\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTürkçe metin giriniz:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m sentences \u001b[38;5;241m=\u001b[39m sentence_splitter(text)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1202\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1200\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[0;32m   1203\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[0;32m   1204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1206\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1207\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1245\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1242\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1243\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1244\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1246\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def sentence_splitter(text):\n",
    "    sentence_endings = r\"[.!?]\" \n",
    "    sentences = re.split(sentence_endings, text)\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()] \n",
    "    return sentences\n",
    "\n",
    "\n",
    "text = input(\"Türkçe metin giriniz:\")\n",
    "\n",
    "sentences = sentence_splitter(text)\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3692c473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bf0338",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b987de0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472676ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9251dd9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7827ea3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f066eb3b",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e5e5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "text = input(\"Türkçe metin giriniz: \")\n",
    "\n",
    "tokens = tokenizer(turkish_text)\n",
    "\n",
    "\n",
    "for token in tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9a913a",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf640588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalizer(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    \n",
    "    text = re.sub(r'[çÇ]', 'c', text)  \n",
    "    text = re.sub(r'[ğĞ]', 'g', text)  \n",
    "    text = re.sub(r'[ıİ]', 'i', text)  \n",
    "    text = re.sub(r'[öÖ]', 'o', text)  \n",
    "    text = re.sub(r'[şŞ]', 's', text)  \n",
    "    text = re.sub(r'[üÜ]', 'u', text)  \n",
    "    \n",
    "   \n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "turkish_text = input(\"Lütfen Türkçe metin girin: \")\n",
    "\n",
    "\n",
    "normalized_text = normalizer(turkish_text)\n",
    "\n",
    "\n",
    "print(normalized_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453f56e2",
   "metadata": {},
   "source": [
    "# Deasciifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309f701b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deasciifier(text):\n",
    "    \n",
    "    text = text.replace('c', 'ç')\n",
    "    text = text.replace('C', 'Ç')\n",
    "    text = text.replace('g', 'ğ')\n",
    "    text = text.replace('G', 'Ğ')\n",
    "    text = text.replace('i', 'ı')\n",
    "    text = text.replace('I', 'İ')\n",
    "    text = text.replace('o', 'ö')\n",
    "    text = text.replace('O', 'Ö')\n",
    "    text = text.replace('s', 'ş')\n",
    "    text = text.replace('S', 'Ş')\n",
    "    text = text.replace('u', 'ü')\n",
    "    text = text.replace('U', 'Ü')\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "turkish_text = input(\"Lütfen Türkçe metin girin: \")\n",
    "\n",
    "\n",
    "deasciified_text = deasciifier(turkish_text)\n",
    "\n",
    "\n",
    "print(deasciified_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e20bd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9db35862",
   "metadata": {},
   "source": [
    "# Spelling Corrector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc069da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spelling_correction_dict = {\n",
    "    \"tehlike\": \"tehlike\",\n",
    "    \"kumputer\": \"bilgisayar\",\n",
    "    \"gramer\": \"gramer\",\n",
    "    \"yazılımm\": \"yazılım\",\n",
    "    \"pyhton\": \"Python\"\n",
    "}\n",
    "\n",
    "def custom_spelling_corrector(text):\n",
    "    words = text.split()\n",
    "    corrected_words = []\n",
    "    for word in words:\n",
    "        if word in spelling_correction_dict:\n",
    "            corrected_words.append(spelling_correction_dict[word])\n",
    "        else:\n",
    "            corrected_words.append(word)\n",
    "    corrected_text = \" \".join(corrected_words)\n",
    "    return corrected_text\n",
    "\n",
    "\n",
    "turkish_text = input(\"Lütfen Türkçe metin girin: \")\n",
    "\n",
    "\n",
    "corrected_text = custom_spelling_corrector(turkish_text)\n",
    "\n",
    "\n",
    "print(corrected_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9830fb",
   "metadata": {},
   "source": [
    "# isTurkish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96099ac1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def isTurkish(word):\n",
    "\n",
    "    turkish_words = [\"merhaba\", \"insanlar\", \"hur\", \"haysiyet\", \"ve\", \"haklar\", \"bakımından\", \"eşit\"]\n",
    "    word = word.lower()\n",
    "    if word in turkish_words:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def checkTextTurkish(text):\n",
    "    text = text.lower()\n",
    "    words = text.split()\n",
    "    results = []\n",
    "    for word in words:\n",
    "        results.append(isTurkish(word))\n",
    "    return results\n",
    "\n",
    "text = input(\"Metin girin: \")\n",
    "results = checkTextTurkish(text)\n",
    "\n",
    "for word, result in zip(text.split(), results):\n",
    "    print(f\"{word}: {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f74db86",
   "metadata": {},
   "source": [
    "# Morphological Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2f21aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78057050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770ad952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a98586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5b3603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def sentence_splitter(text):\n",
    "    sentence_endings = r\"[.!?]\"\n",
    "    sentences = re.split(sentence_endings, text)\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    return sentences\n",
    "\n",
    "def tokenizer(text):\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "text = input(\"Türkçe metin giriniz:\")\n",
    "\n",
    "sentences = sentence_splitter(text)\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokens = tokenizer(sentence)\n",
    "    for token in tokens:\n",
    "        print(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9012a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85040a8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371e42d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3026a706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494a2165",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d485d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbfe889",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2eb667",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8efa407b",
   "metadata": {},
   "source": [
    "# Kelimede Harf Tekrarını Engelleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd260a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repeated_letters(word):\n",
    "    original_word = word[0]\n",
    "    for i in range(1, len(word)):\n",
    "        if word[i] != word[i - 1]:\n",
    "            original_word += word[i]\n",
    "    return original_word\n",
    "\n",
    "text = \"ben seni afFFFFFFFFFFFFFFFFFFFFFFFfetmeye çalışıyorum\"\n",
    "original = remove_repeated_letters(text)\n",
    "print(original)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed5590a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ben seni afFFFFFFFFFFFFFFFFFFFFFFFfetmeye çalışıyorum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aefd01d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd0df50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230577d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb550fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9572a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_repeated_letters(word):\n",
    "    original_word = word[0]\n",
    "    for i in range(1, len(word)):\n",
    "        if word[i] != word[i - 1]:\n",
    "            original_word += word[i]\n",
    "    return original_word\n",
    "\n",
    "def sentence_splitter(text):\n",
    "    sentence_endings = r\"[.!?]\"\n",
    "    sentences = re.split(sentence_endings, text)\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    return sentences\n",
    "\n",
    "def tokenizer(text):\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "text = input(\"Türkçe metin giriniz:\")\n",
    "text = text.lower() \n",
    "\n",
    "sentences = sentence_splitter(text)\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokens = tokenizer(sentence)\n",
    "    for token in tokens:\n",
    "        original_token = remove_repeated_letters(token)\n",
    "        print(original_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971865c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2340c56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db9268a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904ef40e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ffebef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_dataset(file_path):\n",
    "    word_data = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.read().split('\\n')\n",
    "        for line in lines:\n",
    "            if line:\n",
    "                original, corrected = line.split(',')\n",
    "                word_data[original] = corrected\n",
    "    return word_data\n",
    "\n",
    "\n",
    "word_dataset = load_word_dataset('C:/ALL_NEW_TURKISH_WORDS.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb37a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_repeated_letters(word):\n",
    "    original_word = word[0]\n",
    "    for i in range(1, len(word)):\n",
    "        if word[i] != word[i - 1]:\n",
    "            original_word += word[i]\n",
    "    return original_word\n",
    "\n",
    "def sentence_splitter(text):\n",
    "    sentence_endings = r\"[.!?]\"\n",
    "    sentences = re.split(sentence_endings, text)\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    return sentences\n",
    "\n",
    "def tokenizer(text):\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "\n",
    "\n",
    "text = input(\"Türkçe metin giriniz: \")\n",
    "sentences = sentence_splitter(text)\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokens = tokenizer(sentence)\n",
    "    for token in tokens:\n",
    "        original_token = remove_repeated_letters(token)\n",
    "        \n",
    "        \n",
    "        if original_token in word_dataset:\n",
    "            original_token = word_dataset[original_token]\n",
    "        \n",
    "        print(original_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bf8a7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a6a208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2a4f48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1a0409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2cea2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07272787",
   "metadata": {},
   "outputs": [],
   "source": [
    "word='Ben'\n",
    "print(word[0])\n",
    "\n",
    "print('##')\n",
    "for i in range(1,3):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2109d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def sentence_splitter(text):\n",
    "    sentence_endings = r\"[.!?]\"\n",
    "    sentences = re.split(sentence_endings, text)\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    return sentences\n",
    "\n",
    "def tokenizer(text):\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "def remove_repeated_letters(word):\n",
    "    if len(word) <= 1:\n",
    "        return word\n",
    "    original_word = word[0]\n",
    "    for i in range(1, len(word)):\n",
    "        if word[i] != word[i - 1]:\n",
    "            original_word += word[i]\n",
    "    return original_word\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        dataset = set(word.strip() for word in file)\n",
    "    return dataset\n",
    "\n",
    "def find_original_word(word, dataset):\n",
    "    word_vector = vectorizer.transform([word])\n",
    "    similarities = cosine_similarity(X, word_vector)\n",
    "    most_similar_idx = similarities.argmax()\n",
    "    original_word = dataset[most_similar_idx]\n",
    "    return original_word\n",
    "\n",
    "\n",
    "with open('C:/ALL_NEW_TURKISH_WORDS.txt', 'r', encoding='utf-8') as file:\n",
    "    dataset = [line.strip() for line in file]\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(dataset)\n",
    "\n",
    "text = input(\"Türkçe metin giriniz:\")\n",
    "\n",
    "sentences = sentence_splitter(text)\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokens = tokenizer(sentence)\n",
    "    for token in tokens:\n",
    "        original_token = find_original_word(token, dataset)\n",
    "        print(original_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed524985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def sentence_splitter(text):\n",
    "    sentence_endings = r\"[.!?]\"\n",
    "    sentences = re.split(sentence_endings, text)\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    return sentences\n",
    "\n",
    "def tokenizer(text):\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "def remove_repeated_letters(word):\n",
    "    if len(word) <= 1:\n",
    "        return word\n",
    "    original_word = word[0]\n",
    "    for i in range(1, len(word)):\n",
    "        if word[i] != word[i - 1]:\n",
    "            original_word += word[i]\n",
    "    return original_word\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        dataset = [line.strip() for line in file]\n",
    "    return dataset\n",
    "\n",
    "def find_original_word(word, dataset):\n",
    "    word_vector = vectorizer.transform([remove_repeated_letters(word)])\n",
    "    print(word_vector,\"bu word vector\")\n",
    "    similarities = cosine_similarity(X, word_vector)\n",
    "    print(similarities,\"bu similarities\")\n",
    "    most_similar_idx = similarities.argmax()\n",
    "    print(most_similar_idx,\"bu most similar idx\")\n",
    "    if similarities[most_similar_idx] < 0.5:\n",
    "        print(similarities[most_similar_idx],\"bu oran\")\n",
    "        return word  \n",
    "    original_word = dataset[most_similar_idx]\n",
    "    return original_word\n",
    "\n",
    "\n",
    "with open('C:/ALL_NEW_TURKISH_WORDS.txt', 'r', encoding='utf-8') as file:\n",
    "    dataset = [line.strip() for line in file]\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform([remove_repeated_letters(word) for word in dataset])\n",
    "\n",
    "text = input(\"Türkçe metin giriniz:\")\n",
    "\n",
    "sentences = sentence_splitter(text)\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokens = tokenizer(sentence)\n",
    "    for token in tokens:\n",
    "        original_token = find_original_word(token, dataset)\n",
    "        print(original_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f17b7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_original_word(word, dataset):\n",
    "    word_vector = vectorizer.transform([remove_repeated_letters(word)])\n",
    "\n",
    "    similarities = cosine_similarity(X, word_vector)\n",
    "  \n",
    "    most_similar_idx = similarities.argmax()\n",
    "  \n",
    "    if similarities[most_similar_idx] < 0.5:\n",
    "\n",
    "        return word  \n",
    "    original_word = dataset[most_similar_idx]\n",
    "    return original_word\n",
    "\n",
    "text = input(\"Türkçe metin giriniz:\")\n",
    "\n",
    "sentences = sentence_splitter(text)\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokens = tokenizer(sentence)\n",
    "    for token in tokens:\n",
    "        original_token = find_original_word(token, dataset)\n",
    "        print(original_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad814009",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[3596790] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5393489f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = input(\"Türkçe metin giriniz:\")\n",
    "\n",
    "sentences = sentence_splitter(text)\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokens = tokenizer(sentence)\n",
    "    for token in tokens:\n",
    "        original_token = find_original_word(token, dataset)\n",
    "        print(original_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7acafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = input(\"Türkçe metin giriniz:\")\n",
    "\n",
    "sentences = sentence_splitter(text)\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokens = tokenizer(sentence)\n",
    "    for token in tokens:\n",
    "        original_token = find_original_word(token, dataset)\n",
    "        print(original_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b5b2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[1782401]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ada2e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[172288]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbf6ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_base_form(input_word, dataset):\n",
    "    for i in range(len(input_word), 0, -1):\n",
    "        truncated_word = input_word[:i]\n",
    "        original_word = find_original_word(truncated_word, dataset)\n",
    "        print(original_word,\"BU ORIGINAL WORD\")\n",
    "        if original_word != truncated_word:\n",
    "            return original_word\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bac6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = input(\"Türkçe metin giriniz:\")\n",
    "\n",
    "sentences = sentence_splitter(text)\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokens = tokenizer(sentence)\n",
    "    for token in tokens:\n",
    "        original_token = find_base_form(token, dataset)\n",
    "        print(original_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5cbfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import Levenshtein\n",
    "\n",
    "def sentence_splitter(text):\n",
    "    sentence_endings = r\"[.!?]\"\n",
    "    sentences = re.split(sentence_endings, text)\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    return sentences\n",
    "\n",
    "def tokenizer(text):\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        dataset = [line.strip() for line in file]\n",
    "    return dataset\n",
    "\n",
    "def find_original_word(word, dataset):\n",
    "    min_distance = float(1.2)  \n",
    "    most_similar_word = word\n",
    "\n",
    "    for candidate in dataset:\n",
    "        distance = Levenshtein.distance(word, candidate)\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            most_similar_word = candidate\n",
    "\n",
    "    return most_similar_word\n",
    "\n",
    "with open('C:/ALL_NEW_TURKISH_WORDS.txt', 'r', encoding='utf-8') as file:\n",
    "    dataset = [line.strip() for line in file]\n",
    "\n",
    "text = input(\"Türkçe metin giriniz:\")\n",
    "\n",
    "sentences = sentence_splitter(text)\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokens = tokenizer(sentence)\n",
    "    for token in tokens:\n",
    "        original_token = find_original_word(token, dataset)\n",
    "        print(original_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f8372d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "document1 = \"ornek ev araba top otel bakkal sebze\"\n",
    "document2 = \"örnek\"\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform([document1, document2])\n",
    "\n",
    "\n",
    "similarity = cosine_similarity(X)\n",
    "\n",
    "print(\"Cosine Similarity:\")\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6a6b47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f945bd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def sentence_splitter(text):\n",
    "    sentence_endings = r\"[.!?]\"\n",
    "    sentences = re.split(sentence_endings, text)\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    return sentences\n",
    "\n",
    "def tokenizer(text):\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "def remove_repeated_letters(word):\n",
    "    if len(word) <= 1:\n",
    "        return word\n",
    "    original_word = word[0]\n",
    "    for i in range(1, len(word)):\n",
    "        if word[i] != word[i - 1]:\n",
    "            original_word += word[i]\n",
    "    return original_word\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        dataset = [line.strip() for line in file]\n",
    "    return dataset\n",
    "\n",
    "def find_original_word(word, dataset):\n",
    "    word_vector = vectorizer.transform([remove_repeated_letters(word)])\n",
    "    similarities = cosine_similarity(X, word_vector)\n",
    "    most_similar_idx = similarities.argmax()\n",
    "    if similarities[most_similar_idx] < 0.5:\n",
    "        return word  \n",
    "    original_word = dataset[most_similar_idx]\n",
    "    return original_word\n",
    "\n",
    "\n",
    "with open('C:/ALL_NEW_TURKISH_WORDS.txt', 'r', encoding='utf-8') as file:\n",
    "    dataset = [line.strip() for line in file]\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform([remove_repeated_letters(word) for word in dataset])\n",
    "\n",
    "text = input(\"Türkçe metin giriniz:\")\n",
    "\n",
    "sentences = sentence_splitter(text)\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokens = tokenizer(sentence)\n",
    "    for token in tokens:\n",
    "        original_token = find_original_word(token, dataset)\n",
    "        print(original_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96968643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "input_texts = [\"affetmek\", \"okumak\", \"yapacağız\", \"gelişiyorsunuz\"]\n",
    "target_texts = [\"afetmek\", \"okumak\", \"yapıcaz\", \"gelişiyorsunuz\"]\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(filters='', char_level=True)\n",
    "tokenizer.fit_on_texts(input_texts + target_texts)\n",
    "\n",
    "num_encoder_tokens = len(tokenizer.word_index) + 1\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "for i, input_text in enumerate(input_texts):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, tokenizer.word_index[char]] = 1.\n",
    "\n",
    "\n",
    "latent_dim = 256\n",
    "\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_encoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, encoder_input_data], encoder_input_data, epochs=100, batch_size=64)\n",
    "\n",
    "\n",
    "def predict_original_word(input_word, model, tokenizer):\n",
    "    input_seq = tokenizer.texts_to_sequences([input_word])[0]\n",
    "    input_seq = pad_sequences([input_seq], maxlen=max_encoder_seq_length, padding='post')\n",
    "    decoded_seq = model.predict([input_seq, input_seq])[0]\n",
    "    original_word = tokenizer.sequences_to_texts([decoded_seq.argmax(axis=-1)])[0]\n",
    "    return original_word\n",
    "\n",
    "\n",
    "text = \"affetmeyk çalışıyorum\"\n",
    "original_tokens = []\n",
    "\n",
    "for token in text.split():\n",
    "    original_token = predict_original_word(token, model, tokenizer)\n",
    "    original_tokens.append(original_token)\n",
    "\n",
    "print(\" \".join(original_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba50c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "\n",
    "word1 = \"ornek\"\n",
    "word2 = \"trabzon\"\n",
    "\n",
    "distance = Levenshtein.distance(word1, word2)\n",
    "\n",
    "print(\"Levenshtein Distance:\", distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53967a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9461d7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "def load_turkish_dataset(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        dataset = [line.strip() for line in file]\n",
    "    return dataset\n",
    "\n",
    "\n",
    "turkish_dataset = load_turkish_dataset('C:/ALL_NEW_TURKISH_WORDS.txt')\n",
    "\n",
    "\n",
    "input_texts = turkish_dataset\n",
    "target_texts = turkish_dataset\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(filters='', char_level=True)\n",
    "tokenizer.fit_on_texts(input_texts + target_texts)\n",
    "\n",
    "num_encoder_tokens = len(tokenizer.word_index) + 1\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "for i, input_text in enumerate(input_texts):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, tokenizer.word_index[char]] = 1.\n",
    "\n",
    "\n",
    "latent_dim = 256\n",
    "\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_encoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, encoder_input_data], encoder_input_data, epochs=100, batch_size=64)\n",
    "\n",
    "\n",
    "def predict_original_word(input_word, model, tokenizer):\n",
    "    input_seq = tokenizer.texts_to_sequences([input_word])[0]\n",
    "    input_seq = pad_sequences([input_seq], maxlen=max_encoder_seq_length, padding='post')\n",
    "    decoded_seq = model.predict([input_seq, input_seq])[0]\n",
    "    original_word = tokenizer.sequences_to_texts([decoded_seq.argmax(axis=-1)])[0]\n",
    "    return original_word\n",
    "\n",
    "text = \"affetmeyk çalışıyorum\"\n",
    "original_tokens = []\n",
    "\n",
    "for token in text.split():\n",
    "    original_token = predict_original_word(token, model, tokenizer)\n",
    "    original_tokens.append(original_token)\n",
    "\n",
    "print(\" \".join(original_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f30f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0e4c66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8839bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31f992a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66e40ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709d3aa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8b97c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def sentence_splitter(text):\n",
    "    sentence_endings = r\"[.!?]\"\n",
    "    sentences = re.split(sentence_endings, text)\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    return sentences\n",
    "\n",
    "def tokenizer(text):\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "def remove_repeated_letters(word):\n",
    "    if len(word) <= 1:\n",
    "        return word\n",
    "    original_word = word[0]\n",
    "    for i in range(1, len(word)):\n",
    "        if word[i] != word[i - 1]:\n",
    "            original_word += word[i]\n",
    "    return original_word\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        dataset = [line.strip() for line in file]\n",
    "    return dataset\n",
    "\n",
    "def find_original_word(word, dataset):\n",
    "    word_vector = vectorizer.transform([remove_repeated_letters(word)])\n",
    "    similarities = cosine_similarity(X, word_vector)\n",
    "    most_similar_idx = similarities.argmax()\n",
    "    if similarities[most_similar_idx] < 0.5:\n",
    "        return word  \n",
    "    original_word = dataset[most_similar_idx]\n",
    "    return original_word\n",
    "\n",
    "\n",
    "with open('C:/ALL_NEW_TURKISH_WORDS.txt', 'r', encoding='utf-8') as file:\n",
    "    dataset = [line.strip() for line in file]\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform([remove_repeated_letters(word) for word in dataset])\n",
    "\n",
    "text = input(\"Türkçe metin giriniz:\")\n",
    "\n",
    "sentences = sentence_splitter(text)\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokens = tokenizer(sentence)\n",
    "    for token in tokens:\n",
    "        original_token = find_original_word(token, dataset)\n",
    "        print(original_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff617a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = input(\"Türkçe metin giriniz:\")\n",
    "\n",
    "sentences = sentence_splitter(text)\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokens = tokenizer(sentence)\n",
    "    for token in tokens:\n",
    "        original_token = find_original_word(token, dataset)\n",
    "        print(original_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc810fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7fb0bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bc0d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# İsim çekim ekleri\n",
    "cokluk_ekleri = [\"lar\", \"ler\"]\n",
    "durum_ekleri = [\"i\", \"ı\", \"u\", \"ü\"]\n",
    "ilgi_ekleri = [\"de\", \"den\", \"ki\", \"kim\", \"lik\"]\n",
    "iyelik_ekleri = [\"m\", \"n\", \"sı\", \"si\", \"mız\", \"nız\", \"ları\"]\n",
    "esitlik_ekleri = [\"ca\", \"ce\", \"cağ\", \"ceğ\"]\n",
    "\n",
    "# Kelimeyi kullanıcıdan al\n",
    "kelime = input(\"Bir Türkçe kelime girin: \")\n",
    "\n",
    "# İsim çekim eklerine uygunluğu kontrol et\n",
    "def is_isme_uygun(kelime):\n",
    "    for ek in cokluk_ekleri:\n",
    "        if kelime.endswith(ek):\n",
    "            return f\"{kelime[:-len(ek)]}(kök)-{ek}(çokluk eki)\"\n",
    "    \n",
    "    for ek in durum_ekleri:\n",
    "        if kelime.endswith(ek):\n",
    "            return f\"{kelime[:-len(ek)]}(kök)-{ek}(durum eki)\"\n",
    "    \n",
    "    for ek in ilgi_ekleri:\n",
    "        if kelime.endswith(ek):\n",
    "            return f\"{kelime[:-len(ek)]}(kök)-{ek}(ilgi eki)\"\n",
    "    \n",
    "    for ek in iyelik_ekleri:\n",
    "        if kelime.endswith(ek):\n",
    "            return f\"{kelime[:-len(ek)]}(kök)-{ek}(iyelik eki)\"\n",
    "    \n",
    "    for ek in esitlik_ekleri:\n",
    "        if kelime.endswith(ek):\n",
    "            return f\"{kelime[:-len(ek)]}(kök)-{ek}(eşitlik eki)\"\n",
    "    \n",
    "    return \"Uygun çekim eki bulunamadı.\"\n",
    "\n",
    "sonuc = is_isme_uygun(kelime)\n",
    "print(sonuc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2a3edc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bb10d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff368575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d59c76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ee17f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# İsim çekim ekleri\n",
    "cokluk_ekleri = [\"lar\", \"ler\"]\n",
    "durum_ekleri = [\"i\", \"ı\", \"u\", \"ü\"]\n",
    "ilgi_ekleri = [\"de\", \"den\", \"ki\", \"kim\", \"lik\"]\n",
    "iyelik_ekleri = [\"m\", \"n\", \"sı\", \"si\", \"mız\", \"nız\", \"ları\"]\n",
    "esitlik_ekleri = [\"ca\", \"ce\", \"cağ\", \"ceğ\"]\n",
    "\n",
    "def is_isme_uygun(kelime):\n",
    "    for ek in cokluk_ekleri:\n",
    "        if kelime.endswith(ek):\n",
    "            return f\"{kelime[:-len(ek)]}(kök)-{ek}(çokluk eki)\"\n",
    "    \n",
    "    for ek in durum_ekleri:\n",
    "        if kelime.endswith(ek):\n",
    "            return f\"{kelime[:-len(ek)]}(kök)-{ek}(durum eki)\"\n",
    "    \n",
    "    for ek in ilgi_ekleri:\n",
    "        if kelime.endswith(ek):\n",
    "            return f\"{kelime[:-len(ek)]}(kök)-{ek}(ilgi eki)\"\n",
    "    \n",
    "    for ek in iyelik_ekleri:\n",
    "        if kelime.endswith(ek):\n",
    "            return f\"{kelime[:-len(ek)]}(kök)-{ek}(iyelik eki)\"\n",
    "    \n",
    "    for ek in esitlik_ekleri:\n",
    "        if kelime.endswith(ek):\n",
    "            return f\"{kelime[:-len(ek)]}(kök)-{ek}(eşitlik eki)\"\n",
    "    \n",
    "    return \"Uygun çekim eki bulunamadı.\"\n",
    "\n",
    "def sentence_splitter(text):\n",
    "    sentence_endings = r\"[.!?]\"\n",
    "    sentences = re.split(sentence_endings, text)\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    return sentences\n",
    "\n",
    "def tokenizer(text):\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "def remove_repeated_letters(word):\n",
    "    if len(word) <= 1:\n",
    "        return word\n",
    "    original_word = word[0]\n",
    "    for i in range(1, len(word)):\n",
    "        if word[i] != word[i - 1]:\n",
    "            original_word += word[i]\n",
    "    return original_word\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        dataset = [line.strip() for line in file]\n",
    "    return dataset\n",
    "\n",
    "def find_original_word(word, dataset):\n",
    "    word_vector = vectorizer.transform([remove_repeated_letters(word)])\n",
    "    similarities = cosine_similarity(X, word_vector)\n",
    "    most_similar_idx = similarities.argmax()\n",
    "    if similarities[most_similar_idx] < 0.5:\n",
    "        return word  \n",
    "    original_word = dataset[most_similar_idx]\n",
    "    return original_word\n",
    "\n",
    "with open('C:/ALL_NEW_TURKISH_WORDS.txt', 'r', encoding='utf-8') as file:\n",
    "    dataset = [line.strip() for line in file]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform([remove_repeated_letters(word) for word in dataset])\n",
    "\n",
    "text = input(\"Türkçe metin giriniz:\")\n",
    "\n",
    "sentences = sentence_splitter(text)\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokens = tokenizer(sentence)\n",
    "    for token in tokens:\n",
    "        original_token = find_original_word(token, dataset)\n",
    "        isim_cekim_eki = is_isme_uygun(original_token)\n",
    "        print(f\"Kelime: {original_token}, İsim Çekim Eki: {isim_cekim_eki}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0084f32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = input(\"Türkçe metin giriniz:\")\n",
    "\n",
    "sentences = sentence_splitter(text)\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokens = tokenizer(sentence)\n",
    "    for token in tokens:\n",
    "        original_token = find_original_word(token, dataset)\n",
    "        isim_cekim_eki = is_isme_uygun(original_token)\n",
    "        print(f\"Kelime: {original_token}, İsim Çekim Eki: {isim_cekim_eki}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2fa4ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dbbd3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fee78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def kök_bul(kelime):\n",
    "  \"\"\"\n",
    "  Girilen kelimenin kökünü bulur.\n",
    "\n",
    "  Args:\n",
    "    kelime: Analiz edilecek kelime.\n",
    "\n",
    "  Returns:\n",
    "    Kelime kökü.\n",
    "  \"\"\"\n",
    "\n",
    "  # Son harfin ünlü olup olmadığını kontrol eder.\n",
    "\n",
    "  if not re.match(r\"[aeıioöü]\", kelime[-1]):\n",
    "    # Son harf ünlü ise, kökü kelimenin sonuna kadar alır.\n",
    "    return kelime\n",
    "  else:\n",
    "    # Son harf ünsüz ise, kökü son ünsüz harfe kadar alır.\n",
    "    return kelime[:-1]\n",
    "\n",
    "def ek_bul(kelime):\n",
    "  \"\"\"\n",
    "  Girilen kelimenin kökünü ve eklerinin listesini döndürür.\n",
    "\n",
    "  Args:\n",
    "    kelime: Analiz edilecek kelime.\n",
    "\n",
    "  Returns:\n",
    "    Kelime kökü ve eklerinin listesi.\n",
    "  \"\"\"\n",
    "\n",
    "  # Kelime kökünü bulur.\n",
    "\n",
    "  kök = kök_bul(kelime)\n",
    "\n",
    "  # Kelime kökünden başlayarak, her harfin ünlü olup olmadığını kontrol eder.\n",
    "\n",
    "  ek_listesi = []\n",
    "  i = 0\n",
    "  while i < len(kelime):\n",
    "    if not re.match(r\"[aeıioöü]\", kelime[i]):\n",
    "      ek_listesi.append(kelime[i:i + 1])\n",
    "      i += 1\n",
    "    else:\n",
    "      i += 1\n",
    "\n",
    "  # Kök ek_listesine eklenir.\n",
    "\n",
    "  ek_listesi.insert(0, kök)\n",
    "\n",
    "  return ek_listesi\n",
    "\n",
    "def main():\n",
    "  \"\"\"\n",
    "  Ana program.\n",
    "  \"\"\"\n",
    "\n",
    "  # Kullanıcıdan bir kelime girmesini ister.\n",
    "\n",
    "  kelime = input(\"Bir kelime giriniz: \")\n",
    "\n",
    "  # Girilen kelimenin morfolojik analizini yapar.\n",
    "\n",
    "  ek_listesi = ek_bul(kelime)\n",
    "\n",
    "  # Kelime kökünü ve eklerinin listesini yazdırır.\n",
    "\n",
    "  print(\"Kelime kökü:\", ek_listesi[0])\n",
    "  print(\"Ek listesi:\", ek_listesi[1:])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb794be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849edecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d219b6f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c414a6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install morfolog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e15bb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class TurkishMorphologicalAnalysis:\n",
    "    def __init__(self, word, root, pos, suffixes):\n",
    "        self.word = word\n",
    "        self.root = root\n",
    "        self.pos = pos\n",
    "        self.suffixes = suffixes\n",
    "\n",
    "class TurkishPos:\n",
    "    Noun = \"Noun\"\n",
    "    Verb = \"Verb\"\n",
    "    Adjective = \"Adjective\"\n",
    "    Adverb = \"Adverb\"\n",
    "    Conjunction = \"Conjunction\"\n",
    "    Pronoun = \"Pronoun\"\n",
    "    Numeral = \"Numeral\"\n",
    "    # Daha fazla dil bilgisi kategorisi ekleyebilirsiniz.\n",
    "\n",
    "def analyze_turkish(text):\n",
    "    word_list = text.split()\n",
    "\n",
    "    analyses = []\n",
    "\n",
    "    for word in word_list:\n",
    "        root, pos, suffixes = analyze_word(word)\n",
    "        analyses.append(TurkishMorphologicalAnalysis(word, root, pos, suffixes))\n",
    "\n",
    "    return analyses\n",
    "\n",
    "def analyze_word(word):\n",
    "    # Burada daha karmaşık bir kelime analizi yapabilirsiniz.\n",
    "    # Örneğin, kelimenin kökünü ve dil bilgisi kategorisini daha doğru bir şekilde belirlemek için bir kelime veritabanı kullanabilirsiniz.\n",
    "\n",
    "    # Örnek olarak, kelime sonundaki ünlü harfi alarak ekleri belirleyebilirsiniz.\n",
    "    suffixes = re.findall(r'[aeıioöuü]+$', word)\n",
    "\n",
    "    # Bu, sadece örnek bir dil bilgisi kategorisi belirleme yöntemidir.\n",
    "    pos = guess_pos(word)\n",
    "\n",
    "    # Kök kelime, kelimenin sonundaki ekler çıkarılarak belirlenebilir.\n",
    "    root = word\n",
    "    for suffix in suffixes:\n",
    "        root = root[:-(len(suffix))]\n",
    "\n",
    "    return root, pos, suffixes\n",
    "\n",
    "def guess_pos(word):\n",
    "    # Bu, örnek bir dil bilgisi kategorisi tahminleme yöntemidir.\n",
    "    # Daha iyi sonuçlar için gelişmiş bir veritabanı veya doğal dil işleme teknikleri kullanılmalıdır.\n",
    "    if word.endswith(\"mak\") or word.endswith(\"mek\"):\n",
    "        return TurkishPos.Verb\n",
    "    elif word.endswith(\"lar\") or word.endswith(\"ler\"):\n",
    "        return TurkishPos.Noun\n",
    "    elif word.endswith(\"lü\"):\n",
    "        return TurkishPos.Adjective\n",
    "    elif word.endswith(\"ca\"):\n",
    "        return TurkishPos.Adverb\n",
    "    else:\n",
    "        return TurkishPos.Noun  # Varsayılan olarak isim olarak kabul ediyoruz.\n",
    "\n",
    "input_text = \"Kitaplar okunmalıdır\"\n",
    "analyses = analyze_turkish(input_text)\n",
    "\n",
    "for analysis in analyses:\n",
    "    print(f\"Kelime: {analysis.word}\")\n",
    "    print(f\"Kök Kelime: {analysis.root}\")\n",
    "    print(f\"Dil Bilgisi Kategorisi: {analysis.pos}\")\n",
    "    print(f\"Ekler: {', '.join(analysis.suffixes)}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9edaed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbba80a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pd.read_csv('C:/archive/uniq_tr_en_dict.csv')\n",
    "except pd.errors.ParserError as e:\n",
    "    print(f\"Hata: {e}\")\n",
    "    # Hata aldığınız satırları göz ardı edin veya işlem yapın\n",
    "    df = pd.read_csv('C:/archive/uniq_tr_en_dict.csv', error_bad_lines=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e5acba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/archive/uniq_tr_en_dict.csv', error_bad_lines=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f6806e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def check_word_existence(word):\n",
    "    url = f\"https://sozluk.gov.tr/gts?ara={word}\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "\n",
    "    if \"error\" in data:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "# Example usage:\n",
    "word_to_check = \"araba\"  # Replace this with the word you want to check\n",
    "exists = check_word_existence(word_to_check)\n",
    "\n",
    "if exists:\n",
    "    print(f\"The word '{word_to_check}' exists in the dictionary.\")\n",
    "else:\n",
    "    print(f\"The word '{word_to_check}' does not exist in the dictionary.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d01291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def check_word_existence(word):\n",
    "    url = f\"https://sozluk.gov.tr/gts?ara={word}\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "\n",
    "    if \"error\" in data:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "exists=check_word_existence(243)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd822fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def check_word_existence(word):\n",
    "    with requests.Session() as session:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n",
    "        }\n",
    "        url = f\"https://sozluk.gov.tr/gts?ara={word}\"\n",
    "        response = session.get(url, headers=headers)\n",
    "        data = response.json()\n",
    "\n",
    "        if \"error\" in data:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "\n",
    "word_to_check = input(\"Lutfen kontrol edilecek kelimeyi giriniz...\") \n",
    "exists = check_word_existence(word_to_check)\n",
    "\n",
    "\n",
    "\n",
    "if exists:\n",
    "    print(f\"'{word_to_check}' sözlükte bulunuyor.\")\n",
    "else:\n",
    "    print(f\"'{word_to_check}' sözlükte bulunmuyor.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481663ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd61148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import requests\n",
    "\n",
    "def sentence_splitter(text):\n",
    "    sentence_endings = r\"[.!?]\"\n",
    "    sentences = re.split(sentence_endings, text)\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    return sentences\n",
    "\n",
    "def tokenizer(text):\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "def remove_repeated_letters(word):\n",
    "    if len(word) <= 1:\n",
    "        return word\n",
    "    original_word = word[0]\n",
    "    for i in range(1, len(word)):\n",
    "        if word[i] != word[i - 1]:\n",
    "            original_word += word[i]\n",
    "    return original_word\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        dataset = [line.strip() for line in file]\n",
    "    return dataset\n",
    "\n",
    "def find_original_word(word, dataset):\n",
    "    word_vector = vectorizer.transform([remove_repeated_letters(word)])\n",
    "    similarities = cosine_similarity(X, word_vector)\n",
    "    most_similar_idx = similarities.argmax()\n",
    "    if similarities[most_similar_idx] < 0.5:\n",
    "        return word  \n",
    "    original_word = dataset[most_similar_idx]\n",
    "    return original_word\n",
    "\n",
    "# Load the dataset\n",
    "with open('C:/ALL_NEW_TURKISH_WORDS.txt', 'r', encoding='utf-8') as file:\n",
    "    dataset = [line.strip() for line in file]\n",
    "\n",
    "# Create a vectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform([remove_repeated_letters(word) for word in dataset])\n",
    "\n",
    "text = input(\"Türkçe metin giriniz:\")\n",
    "sentences = sentence_splitter(text)\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokens = tokenizer(sentence)\n",
    "    for token in tokens:\n",
    "        original_token = find_original_word(token, dataset)\n",
    "        word_to_check = original_token  # Use the found word as the input for the next part\n",
    "        with requests.Session() as session:\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n",
    "            }\n",
    "            url = f\"https://sozluk.gov.tr/gts?ara={word_to_check}\"\n",
    "            response = session.get(url, headers=headers)\n",
    "            data = response.json()\n",
    "            if \"error\" in data:\n",
    "                print(f\"'{word_to_check}' sözlükte bulunmuyor.\")\n",
    "            else:\n",
    "                print(f\"'{word_to_check}' sözlükte bulunuyor.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d649f08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = input(\"Türkçe metin giriniz:\")\n",
    "sentences = sentence_splitter(text)\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokens = tokenizer(sentence)\n",
    "    for token in tokens:\n",
    "        original_token = find_original_word(token, dataset)\n",
    "        word_to_check = original_token  # Use the found word as the input for the next part\n",
    "        with requests.Session() as session:\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n",
    "            }\n",
    "            url = f\"https://sozluk.gov.tr/gts?ara={word_to_check}\"\n",
    "            response = session.get(url, headers=headers)\n",
    "            data = response.json()\n",
    "            if \"error\" in data:\n",
    "                print(f\"'{word_to_check}' sözlükte bulunmuyor.\")\n",
    "            else:\n",
    "                print(f\"'{word_to_check}' sözlükte bulunuyor.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eef73bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab3db69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def sentence_splitter(text):\n",
    "    sentence_endings = r\"[.!?]\"\n",
    "    sentences = re.split(sentence_endings, text)\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    return sentences\n",
    "\n",
    "def tokenizer(text):\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "def remove_repeated_letters(word):\n",
    "    if len(word) <= 1:\n",
    "        return word\n",
    "    original_word = word[0]\n",
    "    for i in range(1, len(word)):\n",
    "        if word[i] != word[i - 1]:\n",
    "            original_word += word[i]\n",
    "    return original_word\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        dataset = [line.strip() for line in file]\n",
    "    return dataset\n",
    "\n",
    "def find_original_word(word, dataset):\n",
    "    word_vector = vectorizer.transform([remove_repeated_letters(word)])\n",
    "    similarities = cosine_similarity(X, word_vector)\n",
    "    most_similar_idx = similarities.argmax()\n",
    "    if similarities[most_similar_idx] < 0.5:\n",
    "        return word  \n",
    "    original_word = dataset[most_similar_idx]\n",
    "    return original_word\n",
    "\n",
    "\n",
    "with open('C:/ALL_NEW_TURKISH_WORDS.txt', 'r', encoding='utf-8') as file:\n",
    "    dataset = [line.strip() for line in file]\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform([remove_repeated_letters(word) for word in dataset])\n",
    "\n",
    "text = input(\"Türkçe metin giriniz:\")\n",
    "\n",
    "sentences = sentence_splitter(text)\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokens = tokenizer(sentence)\n",
    "    for token in tokens:\n",
    "        original_token = find_original_word(token, dataset)\n",
    "        print(original_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f867490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2088aca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install vngrs-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd6037a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vnlp import StemmerAnalyzer\n",
    "stemmer = StemmerAnalyzer()\n",
    "stemmer.predict(\"Üniversite sınavlarına canla başla çalışıyorlardı.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f4a695",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.predict(\"affediyorum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c478c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b03c144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def kelime_kokunu_bul(kelime):\n",
    "    url = f\"https://sozluk.gov.tr/gts?ara={kelime}\"\n",
    "\n",
    "    sesli_harfler = \"aeıioöuü\"\n",
    "\n",
    "    kelime_koku = None\n",
    "\n",
    "    with requests.Session() as session:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n",
    "        }\n",
    "\n",
    "        kelime_uzunluk = len(kelime)\n",
    "\n",
    "        for i in range(1, kelime_uzunluk + 1):\n",
    "            for ek in [\"mek\", \"mak\"]:\n",
    "                yeni_kelime = kelime[:i] + ek\n",
    "                for j in range(i, kelime_uzunluk):\n",
    "                    for harf in sesli_harfler:\n",
    "                        olasi_kelime = yeni_kelime[:j] + harf + yeni_kelime[j+1:]\n",
    "                        tam_url = f\"https://sozluk.gov.tr/gts?ara={olasi_kelime}\"\n",
    "\n",
    "                        response = session.get(tam_url, headers=headers)\n",
    "                        veri = response.json()\n",
    "\n",
    "                        # Sözlükte sonuç bulunduysa ve doğru sonuçsa\n",
    "                        if veri and len(veri) > 0:\n",
    "                            # Kökü bulduk\n",
    "                            kelime_koku = olasi_kelime.replace(ek, \"\")\n",
    "                            return kelime_koku\n",
    "\n",
    "    # Kök bulunamadıysa\n",
    "    return None\n",
    "\n",
    "# Test için kullanım\n",
    "kelime = \"istiyorum\"\n",
    "kok = kelime_kokunu_bul(kelime)\n",
    "\n",
    "if kok:\n",
    "    print(f\"{kelime} kelimesinin kökü: {kok}\")\n",
    "else:\n",
    "    print(f\"{kelime} kelimesinin kökü bulunamadı.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475733cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a26863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e716e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def kelime_duzelt(kelime):\n",
    "    if \"iyor\" in kelime:\n",
    "        index = kelime.rindex(\"iyor\")  \n",
    "        kelime = kelime[:index] \n",
    "    else:\n",
    "        return \"Geçersiz kelime\"\n",
    "\n",
    "    sesli_harfler = [\"a\", \"e\", \"ı\", \"i\", \"o\", \"ö\", \"u\", \"ü\"]\n",
    "    for harf in sesli_harfler:\n",
    "        for ek in [\"mek\", \"mak\"]:\n",
    "            yeni_kelime = kelime + harf + ek\n",
    "            print(yeni_kelime,\"------ASDFSA\")\n",
    "            url = f\"https://sozluk.gov.tr/gts?ara={yeni_kelime}\"\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n",
    "            }\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                sonuc = response.json()\n",
    "                if sonuc and \"error\" not in sonuc:\n",
    "                    index = kelime.rindex(\"mak\")\n",
    "                    index = kelime.rindex(\"mek\")\n",
    "                    return yeni_kelime\n",
    "\n",
    "    return \"Uygun kelime bulunamadı\"\n",
    "\n",
    "input_kelime = input(\"Kelimeyi girin: \")\n",
    "duzeltilmis_kelime = kelime_duzelt(input_kelime)\n",
    "print(\"Düzeltildi: \", duzeltilmis_kelime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac18bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def kelime_duzelt(kelime):\n",
    "    sesli_harfler = [\"e\", \"a\", \"ı\", \"i\", \"o\", \"ö\", \"u\", \"ü\"]\n",
    "    \n",
    "    eklenecekler = [\"ayor\",\"eyor\",\"ıyor\",\"iyor\",\"oyor\",\"öyor\",\"uyor\",\"üyor\"]\n",
    "    for ek in eklenecekler:\n",
    "            if ek in kelime:\n",
    "                index = kelime.rindex(ek)  \n",
    "                kelime = kelime[:index]  \n",
    "\n",
    "                break\n",
    "     # \"-acak, -ecek, -ılacak, -ilecek, -ülecek, -ulacak, -olacak, -ülecek\" eklerini kontrol et\n",
    "    ekler = [\"acak\", \"ecek\"]\n",
    "    for ek in ekler:\n",
    "        if kelime.endswith(ek):\n",
    "            kelime = kelime[:-len(ek)]  # Eki sil\n",
    "    if len(kelime) < 0:  \n",
    "        return \"Geçersiz kelime\"\n",
    "    \n",
    "    sesli_harfler = [\"e\", \"a\", \"ı\", \"i\", \"o\", \"ö\", \"u\", \"ü\"]\n",
    "    for harf in sesli_harfler:\n",
    "        \n",
    "        \n",
    "        for ek in [\"mek\", \"mak\"]:\n",
    "            yeni_kelime = kelime + ek\n",
    "            url = f\"https://sozluk.gov.tr/gts?ara={yeni_kelime}\"\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n",
    "            }\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                sonuc = response.json()\n",
    "                if sonuc and \"error\" not in sonuc:\n",
    "                    return yeni_kelime\n",
    "            \n",
    "            yeni_kelime = kelime + harf + ek\n",
    "            print(yeni_kelime,\"------SEBZE\")\n",
    "            url = f\"https://sozluk.gov.tr/gts?ara={yeni_kelime}\"\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n",
    "            }\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                sonuc = response.json()\n",
    "                if sonuc and \"error\" not in sonuc:\n",
    "                    return yeni_kelime\n",
    "\n",
    "    return \"Uygun kelime bulunamadı\"\n",
    "\n",
    "input_kelime = input(\"Kelimeyi girin: \")\n",
    "duzeltilmis_kelime = kelime_duzelt(input_kelime)\n",
    "print(\"Düzeltildi: \", duzeltilmis_kelime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d10d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_word = \"istiyorum\"\n",
    "print(input_word[:4] + \"mek\")\n",
    "\n",
    "sebze=\"isti\"\n",
    "new_word = sebze.rstrip(\"e\")\n",
    "print(new_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b823e307",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010ca05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def kelime_duzelt(kelime):\n",
    "    sesli_harfler = [\"a\", \"e\", \"ı\", \"i\", \"o\", \"ö\", \"u\", \"ü\"]\n",
    "    \n",
    "    for i in range(3):\n",
    "        for harf in sesli_harfler:\n",
    "            yeni_kelime = kelime[:i+1] + harf\n",
    "            print(yeni_kelime,\"ALSKSLK\")\n",
    "            for ek in [\"mek\", \"mak\"]:\n",
    "                deneme_kelime = yeni_kelime + ek\n",
    "                url = f\"https://sozluk.gov.tr/gts?ara={deneme_kelime}\"\n",
    "                headers = {\n",
    "                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n",
    "                }\n",
    "                response = requests.get(url, headers=headers)\n",
    "                if response.status_code == 200:\n",
    "                    sonuc = response.json()\n",
    "                    if sonuc and \"error\" not in sonuc:\n",
    "                        return deneme_kelime\n",
    "\n",
    "        for ek in [\"b\", \"c\", \"d\", \"ğ\", \"p\", \"ç\", \"t\", \"k\"]:\n",
    "        if kelime.endswith(ek):\n",
    "            for harf in sesli_harfler:\n",
    "                sonuc = kontrol_et(kelime + harf)\n",
    "                if sonuc:\n",
    "                    return sonuc\n",
    "                \n",
    "    return \"Uygun kelime bulunamadı\"\n",
    "\n",
    "input_kelime = input(\"Kelimeyi girin: \")\n",
    "duzeltilmis_kelime = kelime_duzelt(input_kelime)\n",
    "print(\"Düzeltildi: \", duzeltilmis_kelime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b108a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def kelime_duzelt(kelime):\n",
    "    sesli_harfler = [\"a\", \"e\", \"ı\", \"i\", \"o\", \"ö\", \"u\", \"ü\"]\n",
    "    kaynastirma_harfler = [\"p\", \"ç\", \"t\", \"k\"]\n",
    "    kaynastirma_harfler2 = [\"b\", \"c\", \"d\", \"ğ\"]\n",
    "\n",
    "    \n",
    "                    \n",
    "    def kontrol_et(kelime):\n",
    "            for ek in [\"mek\", \"mak\"]:\n",
    "                jacob=kelime+ek\n",
    "                url = f\"https://sozluk.gov.tr/gts?ara={jacob}\"\n",
    "                headers = {\n",
    "                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n",
    "                }\n",
    "                response = requests.get(url, headers=headers)\n",
    "                if response.status_code == 200:\n",
    "                    sonuc = response.json()\n",
    "                    if sonuc and \"error\" not in sonuc:\n",
    "                        return jacob\n",
    "                deneme_kelime = kelime  + ek\n",
    "                print(deneme_kelime,\"YAKUP\")\n",
    "                url = f\"https://sozluk.gov.tr/gts?ara={deneme_kelime}\"\n",
    "                headers = {\n",
    "                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n",
    "                }\n",
    "                response = requests.get(url, headers=headers)\n",
    "                if response.status_code == 200:\n",
    "                    sonuc = response.json()\n",
    "                    if sonuc and \"error\" not in sonuc:\n",
    "                        return deneme_kelime\n",
    "       \n",
    "    if(len(kelime) >=8):\n",
    "        x=4\n",
    "    else:\n",
    "        x=2\n",
    "    \n",
    "    for i in range(x, 7):\n",
    "        for harf in sesli_harfler:\n",
    "            if(kelime[:i].endswith(\"b\")):\n",
    "                yeni_kelime=kelime[:i-1] + \"p\"\n",
    "                sonuc = kontrol_et(yeni_kelime)\n",
    "                if sonuc:\n",
    "                    return sonuc\n",
    "            if(kelime[:i].endswith(\"c\")):\n",
    "                yeni_kelime=kelime[:i-1] + \"ç\"\n",
    "                sonuc = kontrol_et(yeni_kelime)\n",
    "                if sonuc:\n",
    "                    return sonuc\n",
    "            if(kelime[:i].endswith(\"d\")):\n",
    "                yeni_kelime=kelime[:i-1] + \"t\"\n",
    "                sonuc = kontrol_et(yeni_kelime)\n",
    "                if sonuc:\n",
    "                    return sonuc\n",
    "            if(kelime[:i].endswith(\"ğ\")):\n",
    "                yeni_kelime=kelime[:i-1] + \"k\"\n",
    "                sonuc = kontrol_et(yeni_kelime)\n",
    "                if sonuc:\n",
    "                    return sonuc\n",
    "            \n",
    "            sonuc = kontrol_et(kelime[:i])\n",
    "            if sonuc:\n",
    "                return sonuc\n",
    "            yeni_kelime = kelime[:i] + harf\n",
    "            print(yeni_kelime,\"YENI KELIME\")\n",
    "            sonuc = kontrol_et(yeni_kelime)\n",
    "            if sonuc:\n",
    "                return sonuc\n",
    "\n",
    "\n",
    "    return \"Uygun kelime bulunamadı\"\n",
    "\n",
    "input_kelime = input(\"Kelimeyi girin: \")\n",
    "duzeltilmis_kelime = kelime_duzelt(input_kelime)\n",
    "print(\"Fiil: \", duzeltilmis_kelime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6e6843",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafcbc80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721614bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return set(file.read().splitlines())\n",
    "\n",
    "def find_longest_suffix(word, suffixes):\n",
    "    longest_suffix = \"\"\n",
    "    for suffix in suffixes:\n",
    "        if word.endswith(suffix) and len(suffix) > len(longest_suffix):\n",
    "            longest_suffix = suffix\n",
    "    return longest_suffix\n",
    "\n",
    "def remove_suffix(word, suffix):\n",
    "    if word.endswith(suffix):\n",
    "        return word[:-len(suffix)]\n",
    "    return word\n",
    "\n",
    "def remove_mek_mak_from_dictionary(dictionary):\n",
    "    updated_dictionary = set()\n",
    "\n",
    "    for entry in dictionary:\n",
    "        without_mek = remove_suffix(entry, 'mek')\n",
    "        without_mak = remove_suffix(entry, 'mak')\n",
    "        updated_dictionary.add(without_mek)\n",
    "        updated_dictionary.add(without_mak)\n",
    "\n",
    "    return updated_dictionary\n",
    "\n",
    "def find_root(word, dictionary):\n",
    "    if word in dictionary:\n",
    "        return word\n",
    "\n",
    "    min_distance = float('inf')\n",
    "    closest_word = \"\"\n",
    "\n",
    "    for entry in dictionary:\n",
    "        distance = Levenshtein.distance(word, entry)\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            closest_word = entry\n",
    "\n",
    "    return closest_word\n",
    "\n",
    "def transform_root(root):\n",
    "    last_char = root[-1]\n",
    "    transformations = {'b': 'p', 'c': 'ç', 'd': 't', 'g': 'k'}\n",
    "    return root[:-1] + transformations.get(last_char, last_char)\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    endings_path = 'C:/endings.txt'\n",
    "    dictionary_path = 'C:/KelimeSozluk.txt'\n",
    "\n",
    "    endings = load_dataset(endings_path)\n",
    "    raw_dictionary = load_dataset(dictionary_path)\n",
    "\n",
    "    # 'mek' ve 'mak' eklerini kaldıran fonksiyonu kullanarak sözlüğü güncelle\n",
    "    dictionary = remove_mek_mak_from_dictionary(raw_dictionary)\n",
    "\n",
    "    word_to_process = input(\"Lütfen işlem yapılacak kelimeyi girin: \")\n",
    "\n",
    "    longest_suffix = find_longest_suffix(word_to_process, endings)\n",
    "    \n",
    "    print(longest_suffix,\" uzun suffix\")\n",
    "    \n",
    "    if longest_suffix:\n",
    "        processed_word = word_to_process[:-len(longest_suffix)]\n",
    "    else:\n",
    "        processed_word = word_to_process\n",
    "\n",
    "    print(processed_word,\"islenmis kelime bu\")\n",
    "    transformed_root = transform_root(processed_word)\n",
    "    print(transformed_root,\" donusturulmus kelime bu\")\n",
    "    root_word = find_root(transformed_root, dictionary)\n",
    "    \n",
    "    print(f\"Kelimenin kökü: {root_word}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce38d7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dictionary = load_dataset('C:/KelimeSozluk.txt')\n",
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393bce26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return set(file.read().splitlines())\n",
    "\n",
    "def find_longest_suffix(word, suffixes):\n",
    "    longest_suffix = \"\"\n",
    "    for suffix in suffixes:\n",
    "        if word.endswith(suffix) and len(suffix) > len(longest_suffix):\n",
    "            longest_suffix = suffix\n",
    "    return longest_suffix\n",
    "\n",
    "def remove_suffix(word, suffix):\n",
    "    if word.endswith(suffix):\n",
    "        return word[:-len(suffix)]\n",
    "    return word\n",
    "\n",
    "def remove_mek_mak_from_dictionary(dictionary):\n",
    "    updated_dictionary = set()\n",
    "\n",
    "    for entry in dictionary:\n",
    "        without_mek = remove_suffix(entry, 'mek')\n",
    "        without_mak = remove_suffix(entry, 'mak')\n",
    "        updated_dictionary.add(without_mek)\n",
    "        updated_dictionary.add(without_mak)\n",
    "\n",
    "    return updated_dictionary\n",
    "\n",
    "def find_root(word, dictionary):\n",
    "    if word in dictionary:\n",
    "        return word\n",
    "\n",
    "    min_distance = float('inf')\n",
    "    closest_word = \"\"\n",
    "\n",
    "    for entry in dictionary:\n",
    "        distance = Levenshtein.distance(word, entry)\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            closest_word = entry\n",
    "\n",
    "    return closest_word\n",
    "\n",
    "def transform_root(root):\n",
    "    last_char = root[-1]\n",
    "    transformations = {'b': 'p', 'c': 'ç', 'd': 't', 'g': 'k'}\n",
    "    return root[:-1] + transformations.get(last_char, last_char)\n",
    "\n",
    "def load_endings(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return set(file.read().splitlines())\n",
    "\n",
    "def parcala_ve_kontrol_et(input_str):\n",
    "    \n",
    "    ek_dosyalar = [\n",
    "        (\"C:/nlp/yeterlilikEki.txt\", \"Yeterlilik Fiili Eki\"),\n",
    "        (\"C:/nlp/daralmaEki.txt\", \"Daralma Eki\"),\n",
    "        (\"C:/nlp/olumsuzlukEki.txt\", \"Olumsuzluk Eki\"),\n",
    "        (\"C:/nlp/bilinenGecmisZaman.txt\", \"Bilinen Geçmiş Zaman\"),\n",
    "        (\"C:/nlp/ogrenilenGecmisZaman.txt\", \"Öğrenilen Geçmiş Zaman\"),\n",
    "        (\"C:/nlp/simdikiZaman.txt\", \"Şimdiki Zaman\"),\n",
    "        (\"C:/nlp/gelecekZaman.txt\", \"Gelecek Zaman\"),\n",
    "        (\"C:/nlp/genisZaman.txt\", \"Geniş Zaman\"),\n",
    "        (\"C:/nlp/gereklilikEki.txt\", \"Gereklilik Eki\"),\n",
    "        (\"C:/nlp/dilekSart.txt\", \"Dilek Şart\"),\n",
    "        (\"C:/nlp/istekKipi.txt\", \"İstek Kipi\"),\n",
    "        (\"C:/nlp/kaynastirmaEki.txt\", \"Kaynaştırma Eki\"),\n",
    "        (\"C:/nlp/hikayeGecmisZaman.txt\", \"Hikaye Geçmiş Zaman Eki\"),\n",
    "        (\"C:/nlp/rivayetZamanEki.txt\", \"Rivayet Zaman Eki\"),\n",
    "        (\"C:/nlp/sartZaman.txt\", \"Şart Zaman Eki\"),\n",
    "        (\"C:/nlp/soruEki.txt\", \"Soru Eki\"),\n",
    "        (\"C:/nlp/sahisEkleriBirlesik.txt\", \"Şahıs Eki Zaman Eki\")\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "    sonuclar = []\n",
    "\n",
    "    for dosya_yolu, ek_adi in ek_dosyalar:\n",
    "     \n",
    "        with open(dosya_yolu, 'r', encoding='utf-8') as dosya:\n",
    "            for satir in dosya:\n",
    "                ek = satir.strip()\n",
    "\n",
    "              \n",
    "                if input_str.startswith(ek):\n",
    "                   \n",
    "                    input_str = input_str[len(ek):].strip()\n",
    "                    sonuclar.append((f\"-{ek} Eki\", f\"({ek_adi})\"))\n",
    "                    break  \n",
    "\n",
    "    \n",
    "    if input_str:\n",
    "        sonuclar.append((\"BULUNAMADI\", f\"({input_str})\"))\n",
    "\n",
    "    return sonuclar\n",
    "\n",
    "\n",
    "def is_verb_or_noun(word, dictionary):\n",
    "    if word in dictionary:\n",
    "        return \"Verb\"\n",
    "    else:\n",
    "        return \"Noun\"\n",
    "\n",
    "    \n",
    "def yorekikaldirma_check(input_str, target_str):\n",
    "    index = input_str.find(target_str)\n",
    "    if index != -1:\n",
    "        return input_str[:index]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def daralmaDonustur(input_str):\n",
    "    last_char = input_str[-1]\n",
    "\n",
    "    if last_char in [\"i\", \"ü\"]:\n",
    "        return input_str[:-1] + \"e\"\n",
    "    elif last_char in [\"ı\", \"u\"]:\n",
    "        return input_str[:-1] + \"a\"\n",
    "    else:\n",
    "        return input_str\n",
    "\n",
    "    \n",
    "def islemYap(word_to_process):\n",
    "    \n",
    "    endings_path = 'C:/endings.txt'\n",
    "    dictionary_path = 'C:/KelimeSozluk.txt'\n",
    "\n",
    "    endings = load_endings(endings_path)\n",
    "    raw_dictionary = load_dataset(dictionary_path)\n",
    "    \n",
    "    sonuclar=\"\"\n",
    "    \n",
    "    word_to_process_check = is_verb_or_noun(word_to_process,dictionary)\n",
    "    if word_to_process_check == \"Verb\":\n",
    "        print(f\"Kelimenin kökü: {word_to_process}\")\n",
    "        check=\"1\"\n",
    "    else:\n",
    "        longest_suffix = find_longest_suffix(word_to_process, endings)\n",
    "        print(longest_suffix, \" uzun suffix\")\n",
    "\n",
    "        if longest_suffix:\n",
    "            processed_word = word_to_process[:-len(longest_suffix)]\n",
    "        else:\n",
    "            processed_word = word_to_process\n",
    "\n",
    "        print(processed_word, \" işlenmiş kelime bu\")\n",
    "        \n",
    "        processed_word_check = is_verb_or_noun(processed_word,dictionary)\n",
    "        \n",
    "        if(processed_word_check == \"Verb\"):\n",
    "            print(f\"Kelimenin kökü: {processed_word}\")\n",
    "            sonuclar = parcala_ve_kontrol_et(longest_suffix)\n",
    "        else:           \n",
    "            transformed_root = transform_root(processed_word)\n",
    "            print(transformed_root, \" dönüştürülmüş kelime bu\")\n",
    "\n",
    "            word_type = is_verb_or_noun(transformed_root, dictionary)\n",
    "\n",
    "            if word_type == \"Verb\":\n",
    "                root_word = find_root(transformed_root, dictionary)\n",
    "                print(f\"Kelimenin kökü: {root_word}\")\n",
    "                sonuclar = parcala_ve_kontrol_et(longest_suffix)\n",
    "            \n",
    "    if(sonuclar):\n",
    "        for ek, dosya_adi in sonuclar:\n",
    "            print(f\"{ek} {dosya_adi}\")\n",
    "\n",
    "def main():\n",
    "    \n",
    "\n",
    "    check=0\n",
    "    \n",
    "    dictionary = remove_mek_mak_from_dictionary(raw_dictionary)\n",
    "\n",
    "    word_to_process = input(\"Lütfen işlem yapılacak kelimeyi girin: \")\n",
    "    \n",
    "    islemYap(word_to_process)\n",
    "    \n",
    "    if(islemYap):\n",
    "        print(\"kovboy\")\n",
    "    else:\n",
    "        yorsuzkelime = yorekikaldirma_check(word_to_process,\"yor\")\n",
    "        if(yorsuzkelime):\n",
    "            yeniKelime=daralmaDonustur(yorsuzkelime)\n",
    "                \n",
    "            \n",
    "        if(check == 0):\n",
    "            print(f\"{word_to_process} kelimesi bir isimdir\")\n",
    "            \n",
    "                \n",
    "                    \n",
    "                    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47e30bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfdd256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa08f708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def donustur(input_str):\n",
    "    last_char = input_str[-1]\n",
    "\n",
    "    if last_char in [\"i\", \"ü\"]:\n",
    "        return input_str[:-1] + \"e\"\n",
    "    elif last_char in [\"ı\", \"u\"]:\n",
    "        return input_str[:-1] + \"a\"\n",
    "    else:\n",
    "        return input_str\n",
    "\n",
    "# Kullanım örneği:\n",
    "user_input = input(\"Bir kelime veya cümle girin: \")\n",
    "result = donustur(user_input)\n",
    "print(\"Dönüştürülmüş hali:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8fa57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def daralma_check(input_str, target_str):\n",
    "    index = input_str.find(target_str)\n",
    "    if index != -1:\n",
    "        return input_str[:index]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def donustur(input_str):\n",
    "    last_char = input_str[-1]\n",
    "\n",
    "    if last_char in [\"i\", \"ü\"]:\n",
    "        return input_str[:-1] + \"e\"\n",
    "    elif last_char in [\"ı\", \"u\"]:\n",
    "        return input_str[:-1] + \"a\"\n",
    "    else:\n",
    "        return input_str\n",
    "    \n",
    "input_str = input(\"kelime giriniz.\")\n",
    "target_str = \"yor\"\n",
    "\n",
    "result = daralma_check(input_str, target_str)\n",
    "\n",
    "if result is not None:\n",
    "    sonuc=donustur(result)\n",
    "    print(sonuc)\n",
    "else:\n",
    "    print(\"Hedef string bulunamadı.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f34911b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7e2286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parcala_ve_kontrol_et(input_str):\n",
    "    \n",
    "    ek_dosyalar = [\n",
    "        (\"C:/nlp/fiil/yeterlilikEki.txt\", \"Yeterlilik Fiili Eki\"),\n",
    "        (\"C:/nlp/fiil/yeterlilikOlumsuzEki.txt\", \"Yeterlilik Fiilinin Olumsuzluk Eki\"),\n",
    "        (\"C:/nlp/fiil/olumsuzlukEki.txt\", \"Olumsuzluk Eki\"),\n",
    "        (\"C:/nlp/fiil/daralmaEki.txt\", \"Daralma Eki\"),\n",
    "        (\"C:/nlp/fiil/bilinenGecmisZaman.txt\", \"Bilinen Geçmiş Zaman\"),\n",
    "        (\"C:/nlp/fiil/ogrenilenGecmisZaman.txt\", \"Öğrenilen Geçmiş Zaman\"),\n",
    "        (\"C:/nlp/fiil/simdikiZaman.txt\", \"Şimdiki Zaman\"),\n",
    "        (\"C:/nlp/fiil/gelecekZaman.txt\", \"Gelecek Zaman\"),\n",
    "        (\"C:/nlp/fiil/genisZaman.txt\", \"Geniş Zaman\"),\n",
    "        (\"C:/nlp/fiil/gereklilikEki.txt\", \"Gereklilik Eki\"),\n",
    "        (\"C:/nlp/fiil/dilekSart.txt\", \"Dilek Şart\"),\n",
    "        (\"C:/nlp/fiil/istekKipi.txt\", \"İstek Kipi\"),\n",
    "        (\"C:/nlp/fiil/kaynastirmaEki.txt\", \"Kaynaştırma Eki\"),\n",
    "        (\"C:/nlp/fiil/hikayeGecmisZaman.txt\", \"Hikaye Geçmiş Zaman Eki\"),\n",
    "        (\"C:/nlp/fiil/rivayetZamanEki.txt\", \"Rivayet Zaman Eki\"),\n",
    "        (\"C:/nlp/fiil/sartZaman.txt\", \"Şart Zaman Eki\"),\n",
    "        (\"C:/nlp/fiil/sahisEkleriBirlesik.txt\", \"Şahıs Eki Zaman Eki\"),\n",
    "        (\"C:/nlp/fiil/hikayeGecmisZaman.txt\", \"Hikaye Geçmiş Zaman Eki\"),\n",
    "        (\"C:/nlp/fiil/rivayetZamanEki.txt\", \"Rivayet Zaman Eki\"),\n",
    "        (\"C:/nlp/fiil/sartZaman.txt\", \"Şart Zaman Eki\"),\n",
    "        (\"C:/nlp/fiil/fiildenFiil.txt\", \"Fiilden Fiil Yapım Eki\"),\n",
    "    ]\n",
    "\n",
    "    sonuclar = []\n",
    "\n",
    "    while input_str:\n",
    "        bulundu = False\n",
    "        en_uzun_ek = \"\"\n",
    "        ek_adi = \"\"\n",
    "\n",
    "        for dosya_yolu, ad in ek_dosyalar:\n",
    "            with open(dosya_yolu, 'r', encoding='utf-8') as dosya:\n",
    "                for satir in dosya:\n",
    "                    ek = satir.strip()\n",
    "                    if input_str.endswith(ek) and len(ek) > len(en_uzun_ek):\n",
    "                        en_uzun_ek = ek\n",
    "                        ek_adi = ad\n",
    "                        bulundu = True\n",
    "\n",
    "        if not bulundu:\n",
    "            for dosya_yolu, ad in ek_dosyalar:\n",
    "                with open(dosya_yolu, 'r', encoding='utf-8') as dosya:\n",
    "                    for satir in dosya:\n",
    "                        ek = satir.strip()\n",
    "                        if input_str.startswith(ek) and len(ek) > len(en_uzun_ek):\n",
    "                            en_uzun_ek = ek\n",
    "                            ek_adi = ad\n",
    "                            bulundu = True\n",
    "\n",
    "        if bulundu:\n",
    "            if input_str.startswith(en_uzun_ek):\n",
    "                input_str = input_str[len(en_uzun_ek):].strip()\n",
    "            elif input_str.endswith(en_uzun_ek):\n",
    "                input_str = input_str[:-len(en_uzun_ek)].strip()\n",
    "            sonuclar.append((f\"-{en_uzun_ek} Eki\", f\"({ek_adi})\"))\n",
    "        else:\n",
    "            sonuclar.append((\"BULUNAMADI\", f\"({input_str})\"))\n",
    "            break\n",
    "\n",
    "    return sonuclar\n",
    "\n",
    "sonuclar = parcala_ve_kontrol_et(input(\"ek giriniz: \"))\n",
    "\n",
    "for ek, dosya_adi in sonuclar:\n",
    "    print(f\"{ek} {dosya_adi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc6d0f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae75e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parcala_ve_kontrol_et(input_str):\n",
    "    \n",
    "    ek_dosyalar = [\n",
    "        (\"C:/nlp/isim/belirtmeDurumHalEki.txt\", \"Belirtme Durum Hal Eki\"),\n",
    "        (\"C:/nlp/isim/yonelmeDurumuEki.txt\", \"Yönelme Durumu Eki\"),\n",
    "        (\"C:/nlp/isim/bulunmaDurumuEki.txt\", \"Bulunma Durumu Eki\"),\n",
    "        (\"C:/nlp/isim/ayrilmaDurumuEki.txt\", \"Ayrılma Durumu Eki\"),\n",
    "        (\"C:/nlp/isim/ilgiEki.txt\", \"İlgi Eki\"),\n",
    "        (\"C:/nlp/isim/iyelikEki.txt\", \"İyelik Eki\"),\n",
    "        (\"C:/nlp/isim/esitlikEkleri.txt\", \"Eşitlik Ekleri\"),\n",
    "        (\"C:/nlp/isim/cogulEki.txt\", \"Çoğul Eki\"),\n",
    "        (\"C:/nlp/isim/kaynastirmaEki.txt\", \"Kaynaştırma Eki\"),\n",
    "        (\"C:/nlp/isim/ekFiilinGorulenGecmisZamani.txt\", \"Ek Fiilin Görülen Geçmiş Zamanı\"),\n",
    "        (\"C:/nlp/isim/ekFiilinOgrenilenGecmisZamani.txt\", \"Ek Fiilin Öğrenilen Geçmiş Zamanı\"),\n",
    "        (\"C:/nlp/isim/ekFiilinGenisZamani.txt\", \"Ek Fiilin Geniş Zamanı\"),\n",
    "        (\"C:/nlp/isim/ekFiilinSarti.txt\", \"Ek Fiilin Şartı\"),\n",
    "        (\"C:/nlp/isim/ekFiilinOlumsuzu.txt\", \"Ek Fiilin Olumsuzu\"),\n",
    "        (\"C:/nlp/isim/vasitaEki.txt\", \"Vasıta Eki\"),\n",
    "    ]\n",
    "\n",
    "    ozel_durumlar = {\n",
    "        #\"ımız\": [\"ı\", \"mız\"],\n",
    "        #\"sinler\":[\"s\",\"i\",\"n\",\"ler\"],\n",
    "    }\n",
    "\n",
    "    sonuclar = []\n",
    "\n",
    "    while input_str:\n",
    "        bulundu = False\n",
    "        ozelBulundu=False\n",
    "        en_uzun_ek = \"\"\n",
    "        ek_adi = \"\"\n",
    "\n",
    "        \n",
    "        for ozel_ek, ayrilma_sekli in ozel_durumlar.items():\n",
    "            if(input_str.startswith(ozel_ek)):\n",
    "                for ayrilma in ayrilma_sekli:\n",
    "                    for dosya_yolu, ad in ek_dosyalar:\n",
    "                        with open(dosya_yolu, 'r', encoding='utf-8') as dosya:\n",
    "                            for satir in dosya:\n",
    "                                ek = satir.strip()\n",
    "                                if input_str.startswith(ayrilma) and input_str.startswith(ek) and ayrilma==ek:\n",
    "                                        en_uzun_ek = ek\n",
    "                                        ek_adi = f\"({ad})\"\n",
    "                                        ozelBulundu = True\n",
    "                                        if ozelBulundu:\n",
    "                                            input_str = input_str[len(en_uzun_ek):].strip()\n",
    "                                            sonuclar.append((f\"-{en_uzun_ek} Eki\", ek_adi))\n",
    "                                            \n",
    "                                            \n",
    "\n",
    "        if not ozelBulundu:\n",
    "            for dosya_yolu, ad in ek_dosyalar:\n",
    "                with open(dosya_yolu, 'r', encoding='utf-8') as dosya:\n",
    "                    for satir in dosya:\n",
    "                        ek = satir.strip()\n",
    "                        if input_str.startswith(ek) and len(ek) > len(en_uzun_ek):\n",
    "                            en_uzun_ek = ek\n",
    "                            ek_adi = f\"({ad})\"\n",
    "                            bulundu = True\n",
    "\n",
    "            if bulundu:\n",
    "                input_str = input_str[len(en_uzun_ek):].strip()\n",
    "                sonuclar.append((f\"-{en_uzun_ek} Eki\", ek_adi))\n",
    "            else:\n",
    "                sonuclar.append((\"BULUNAMADI\", f\"({input_str})\"))\n",
    "                break\n",
    "\n",
    "    return sonuclar\n",
    "\n",
    "sonuclar = parcala_ve_kontrol_et(input(\"ek giriniz: \"))\n",
    "\n",
    "for ek, dosya_adi in sonuclar:\n",
    "    print(f\"{ek} {dosya_adi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99abad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parcala_ve_kontrol_et(input_str):\n",
    "\n",
    "    ek_dosyalar = {\n",
    "        ## noun suffixes\n",
    "        \"Pl\": [\"lar\", \"ler\"],\n",
    "        ## case suffixes\n",
    "        \"Dat\": [\"ya\",\"ye\",\"e\",\"a\",\"na\",\"ne\"],\n",
    "        \"Loc\": [\"da\", \"de\", \"ta\", \"te\",\"nde\",\"nda\"],\n",
    "        \"Abl\": [\"dan\", \"den\", \"tan\", \"ten\",\"ndan\",\"nden\"],\n",
    "        \"Gen\": [\"nın\", \"nin\",\"nun\",\"nün\",\"ın\",\"in\",\"un\", \"ün\",\"yın\",\"yin\",\"yun\",\"yün\",\"ım\",\"im\",\"um\",\"üm\",\"m\"],\n",
    "        \"Acc\": [\"yı\", \"yi\", \"yu\", \"yü\", \"ı\", \"i\", \"u\",\"ü\",\"nı\",\"ni\",\"nu\",\"nü\"],\n",
    "        \"Inst\": [\"yla\", \"yle\",\"la\",\"le\"],\n",
    "        \"P1sg\": [\"ım\",\"im\",\"um\",\"üm\",\"m\",\"yum\"],\n",
    "        \"P2sg\": [\"ın\",\"in\",\"un\",\"ün\",\"n\",\"yun\"],\n",
    "        \"P3sg\": [\"sı\", \"si\", \"su\", \"sü\",\"ı\",\"i\",\"u\",\"ü\",\"yu\"],\n",
    "        \"P1pl\": [\"ımız\", \"imiz\", \"umuz\", \"ümüz\",\"mız\",\"miz\",\"muz\",\"müz\",\"yumuz\"],\n",
    "        \"P2pl\": [\"ınız\",\"iniz\",\"unuz\",\"ünüz\",\"nız\",\"niz\",\"nuz\",\"nüz\",\"yunuz\"],\n",
    "        \"P3pl\": [\"ı\", \"i\",\"u\",\"ü\",\"ları\",\"leri\"],\n",
    "        \"Dim\": [\"cık\", \"cik\",\"cuk\",\"cük\",\"çık\",\"çik\",\"çuk\",\"çük\",\"cığ\",\"ciğ\",\"cuğ\",\"cüğ\",\"çığ\",\"çiğ\",\"çuğ\",\"çüğ\"],\n",
    "        \"Dim2\": [\"cağız\", \"ceğiz\"],\n",
    "        \"With\":[\"lı\",\"li\",\"lu\",\"lü\"],\n",
    "        \"Without\": [\"sız\", \"siz\",\"suz\",\"süz\"],\n",
    "        \"Since\":[\"dır\",\"dir\",\"dur\",\"dür\",\"tır\",\"tir\",\"tur\",\"tür\"],\n",
    "        \"Related\": [\"sal\", \"sel\"],\n",
    "        \"Ness\":[\"lık\",\"lik\",\"luk\",\"lük\",\"lığ\",\"liğ\",\"luğ\",\"lüğ\"],\n",
    "        \"Acquire\": [\"lan\", \"len\"],\n",
    "        \"Apply\":[\"la\",\"le\"],\n",
    "        \"Resemb\": [\"ımsı\", \"imsi\",\"umsu\",\"ümsü\",\"msı\",\"msi\",\"msu\",\"msü\"],\n",
    "        \"Rel\":[\"kı\",\"ki\",\"ku\",\"kü\"],\n",
    "        \"By\": [\"ca\", \"ce\"],\n",
    "        \"Cmp\":[\"ca\",\"ce\"],\n",
    "        \"Agt\":[\"cı\",\"ci\",\"cu\",\"cü\",\"yıcı\",\"yici\",\"yucu\",\"yücü\"],\n",
    "        ## Adjective Suffixes\n",
    "        \"Become\":[\"laş\",\"leş\"],\n",
    "        \"Ly\":[\"ca\",\"ce\"],\n",
    "        \"Quite\":[\"ca\",\"ce\"],\n",
    "        ## number-person agreement suffixes\n",
    "        \"A1sg\": [\"yım\", \"yim\",\"yum\",\"yüm\",\"m\",\"ım\",\"im\",\"um\",\"üm\"],\n",
    "        \"A2sg\":[\"sın\",\"sin\",\"sun\",\"sün\",\"n\",\"ın\",\"in\",\"un\",\"ün\"],\n",
    "        \"A3sg\":[\"\"],\n",
    "        \"A1pl\": [\"yız\",\"yiz\",\"k\",\"yuz\",\"yüz\",\"ız\",\"iz\",\"uz\",\"üz\"],\n",
    "        \"A2pl\":[\"sınız\",\"siniz\",\"sunuz\",\"sünüz\",\"nız\",\"niz\",\"nuz\",\"nüz\",\"ınız\",\"iniz\",\"unuz\",\"ünüz\"],\n",
    "        \"A3pl\": [\"lar\",\"ler\"],\n",
    "        ### Numeral suffixes\n",
    "        \"Grouping\": [\"şar\",\"şer\",\"ız\",\"iz\",\"ar\",\"er\"],\n",
    "        \"Ordinal\":[\"ıncı\",\"inci\",\"uncu\",\"üncü\",\"ncı\",\"nci\",\"ncu\",\"ncü\"],\n",
    "        \"Division\":[\"da\",\"de\",\"ta\",\"te\"],\n",
    "        ### isim kontrolü\n",
    "        \"Noun\":[\"CL_ISIM\",\"CL_OI\"],\n",
    "        \n",
    "    }\n",
    "\n",
    "    ozel_durumlar = {\n",
    "        #\"ımız\": [\"ı\", \"mız\"],\n",
    "        #\"sinler\":[\"s\",\"i\",\"n\",\"ler\"],\n",
    "    }\n",
    "\n",
    "    sonuclar = []\n",
    "\n",
    "    while input_str:\n",
    "        bulundu = False\n",
    "        ozelBulundu = False\n",
    "        en_uzun_ek = \"\"\n",
    "        ek_adi = \"\"\n",
    "\n",
    "        for ozel_ek, ayrilma_sekli in ozel_durumlar.items():\n",
    "            if input_str.startswith(ozel_ek):\n",
    "                for ayrilma in ayrilma_sekli:\n",
    "                    for ad, ekler in ek_dosyalar.items():\n",
    "                        if ayrilma in ekler and input_str.startswith(ayrilma):\n",
    "                            en_uzun_ek = ayrilma\n",
    "                            ek_adi = f\"({ad})\"\n",
    "                            ozelBulundu = True\n",
    "                            if ozelBulundu:\n",
    "                                input_str = input_str[len(en_uzun_ek):].strip()\n",
    "                                sonuclar.append((f\"-{en_uzun_ek} Eki\", ek_adi))\n",
    "\n",
    "        if not ozelBulundu:\n",
    "            for ad, ekler in ek_dosyalar.items():\n",
    "                for ek in ekler:\n",
    "                    if input_str.startswith(ek) and len(ek) > len(en_uzun_ek):\n",
    "                        en_uzun_ek = ek\n",
    "                        ek_adi = f\"({ad})\"\n",
    "                        bulundu = True\n",
    "\n",
    "            if bulundu:\n",
    "                input_str = input_str[len(en_uzun_ek):].strip()\n",
    "                sonuclar.append((f\"-{en_uzun_ek} Eki\", ek_adi))\n",
    "            else:\n",
    "                sonuclar.append((\"BULUNAMADI\", f\"({input_str})\"))\n",
    "                break\n",
    "\n",
    "    return sonuclar\n",
    "\n",
    "sonuclar = parcala_ve_kontrol_et(input(\"ek giriniz: \"))\n",
    "\n",
    "for ek, dosya_adi in sonuclar:\n",
    "    print(f\"{ek} {dosya_adi}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884fa914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_classify(file_path, input_word):\n",
    "    data = {}\n",
    "\n",
    "    # Load dataset\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            if parts:\n",
    "                word = parts[0]\n",
    "                types = parts[1:]\n",
    "                data[word] = types\n",
    "\n",
    "    # Classify word\n",
    "    if input_word in data:\n",
    "        word_types = data[input_word]\n",
    "        types = [\"CL_ISIM\", \"CL_OI\", \"IS_OA\", \"IS_ADJ\", \"IS_SD\"]  # Örnek olarak kelime sınıfları\n",
    "\n",
    "        for word_type in word_types:\n",
    "            if word_type in types:\n",
    "                print(f\"{input_word} ({word_type})\")\n",
    "    else:\n",
    "        print(f\"{input_word} kelimesi veri setinde bulunamadı.\")\n",
    "\n",
    "# Kullanım örneği\n",
    "dataset_path = \"C:/dataset.txt\"\n",
    "input_word = input(\"Kelimeyi girin: \")\n",
    "load_and_classify(dataset_path, input_word)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9a99b079",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3a863f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_and_process_suffixes(user_input, dataset_path):\n",
    "    def find_suffixes(word, suffixes):\n",
    "        found_suffixes = [suffix for suffix in suffixes if word.endswith(suffix)]\n",
    "        return found_suffixes\n",
    "\n",
    "    def process_dataset(base_word, suffixes):\n",
    "        matches = []\n",
    "        with open(dataset_path, 'r', encoding='utf-8') as dataset_file:\n",
    "            for line in dataset_file:\n",
    "                parts = line.strip().split()\n",
    "                \n",
    "                if len(parts[0]) > 2:\n",
    "                    current_word, word_type = parts[0], parts[1]\n",
    "                    \n",
    "                    if base_word == current_word and not suffixes:\n",
    "                        matches.append(f\"{current_word} ({word_type})\")\n",
    "                        \n",
    "                    for suffix in suffixes:\n",
    "                        if base_word == current_word:\n",
    "                            matches.append(f\"{current_word} ({word_type}) -{suffix}\")\n",
    "                        else:\n",
    "                            if base_word.endswith('b'):\n",
    "                                base_word = base_word[:-1] + 'p'\n",
    "                            elif base_word.endswith('c'):\n",
    "                                base_word = base_word[:-1] + 'ç'\n",
    "                            elif base_word.endswith('d'):\n",
    "                                base_word = base_word[:-1] + 't'\n",
    "                            elif base_word.endswith('ğ'):\n",
    "                                base_word = base_word[:-1] + 'k'\n",
    "                            if base_word == current_word:\n",
    "                                matches.append(f\"{current_word} ({word_type}) -{suffix}\")\n",
    "                            \n",
    "        return matches\n",
    "\n",
    "    with open('C:/endings.txt', 'r', encoding='utf-8') as endings_file:\n",
    "        suffixes = [line.strip() for line in endings_file]\n",
    "    \n",
    "    eksizEslesme=process_dataset(user_input,[])\n",
    "\n",
    "    if(eksizEslesme):\n",
    "        for eslesmeler in eksizEslesme:\n",
    "            print(eslesmeler)\n",
    "        return\n",
    "    \n",
    "    all_suffixes = find_suffixes(user_input, suffixes)\n",
    "    \n",
    "    if all_suffixes:\n",
    "        # En uzundan başlayarak ekleri deniyoruz.\n",
    "        for suffix in sorted(all_suffixes, key=len, reverse=True):\n",
    "            remaining_word = user_input[:-len(suffix)]\n",
    "            matches = process_dataset(remaining_word, [suffix])\n",
    "\n",
    "            if matches:\n",
    "                for match in matches:\n",
    "                    print(match)\n",
    "                return  # exit the function if a match is found\n",
    "    print(f\"{user_input} kelimesi için eşleşme bulunamadı.\")\n",
    "\n",
    "# Example usage:\n",
    "user_input = input(\"Kelimeyi girin: \")\n",
    "find_and_process_suffixes(user_input, 'C:/dataset.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271e42d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93694750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parcala_ve_kontrol_et(input_str):\n",
    "\n",
    "    ek_dosyalar = {\n",
    "        ## noun suffixes\n",
    "        \"Pl\": [\"lar\", \"ler\"],\n",
    "        ## case suffixes\n",
    "        \"Dat\": [\"ya\",\"ye\",\"e\",\"a\",\"na\",\"ne\"],\n",
    "        \"Loc\": [\"da\", \"de\", \"ta\", \"te\",\"nde\",\"nda\"],\n",
    "        \"Abl\": [\"dan\", \"den\", \"tan\", \"ten\",\"ndan\",\"nden\"],\n",
    "        \"Gen\": [\"nın\", \"nin\",\"nun\",\"nün\",\"ın\",\"in\",\"un\", \"ün\",\"yın\",\"yin\",\"yun\",\"yün\",\"ım\",\"im\",\"um\",\"üm\",\"m\"],\n",
    "        \"Acc\": [\"yı\", \"yi\", \"yu\", \"yü\", \"ı\", \"i\", \"u\",\"ü\",\"nı\",\"ni\",\"nu\",\"nü\"],\n",
    "        \"Inst\": [\"yla\", \"yle\",\"la\",\"le\"],\n",
    "        \"P1sg\": [\"ım\",\"im\",\"um\",\"üm\",\"m\",\"yum\"],\n",
    "        \"P2sg\": [\"ın\",\"in\",\"un\",\"ün\",\"n\",\"yun\"],\n",
    "        \"P3sg\": [\"sı\", \"si\", \"su\", \"sü\",\"ı\",\"i\",\"u\",\"ü\",\"yu\"],\n",
    "        \"P1pl\": [\"ımız\", \"imiz\", \"umuz\", \"ümüz\",\"mız\",\"miz\",\"muz\",\"müz\",\"yumuz\"],\n",
    "        \"P2pl\": [\"ınız\",\"iniz\",\"unuz\",\"ünüz\",\"nız\",\"niz\",\"nuz\",\"nüz\",\"yunuz\"],\n",
    "        \"P3pl\": [\"ı\", \"i\",\"u\",\"ü\",\"ları\",\"leri\"],\n",
    "        \"Dim\": [\"cık\", \"cik\",\"cuk\",\"cük\",\"çık\",\"çik\",\"çuk\",\"çük\",\"cığ\",\"ciğ\",\"cuğ\",\"cüğ\",\"çığ\",\"çiğ\",\"çuğ\",\"çüğ\"],\n",
    "        \"Dim2\": [\"cağız\", \"ceğiz\"],\n",
    "        \"With\":[\"lı\",\"li\",\"lu\",\"lü\"],\n",
    "        \"Without\": [\"sız\", \"siz\",\"suz\",\"süz\"],\n",
    "        \"Since\":[\"dır\",\"dir\",\"dur\",\"dür\",\"tır\",\"tir\",\"tur\",\"tür\"],\n",
    "        \"Related\": [\"sal\", \"sel\"],\n",
    "        \"Ness\":[\"lık\",\"lik\",\"luk\",\"lük\",\"lığ\",\"liğ\",\"luğ\",\"lüğ\"],\n",
    "        \"Acquire\": [\"lan\", \"len\"],\n",
    "        \"Apply\":[\"la\",\"le\"],\n",
    "        \"Resemb\": [\"ımsı\", \"imsi\",\"umsu\",\"ümsü\",\"msı\",\"msi\",\"msu\",\"msü\"],\n",
    "        \"Rel\":[\"kı\",\"ki\",\"ku\",\"kü\"],\n",
    "        \"By\": [\"ca\", \"ce\"],\n",
    "        \"Cmp\":[\"ca\",\"ce\"],\n",
    "        \"Agt\":[\"cı\",\"ci\",\"cu\",\"cü\",\"yıcı\",\"yici\",\"yucu\",\"yücü\"],\n",
    "        ## Adjective Suffixes\n",
    "        \"Become\":[\"laş\",\"leş\"],\n",
    "        \"Ly\":[\"ca\",\"ce\"],\n",
    "        \"Quite\":[\"ca\",\"ce\"],\n",
    "        ## number-person agreement suffixes\n",
    "        \"A1sg\": [\"yım\", \"yim\",\"yum\",\"yüm\",\"m\",\"ım\",\"im\",\"um\",\"üm\"],\n",
    "        \"A2sg\":[\"sın\",\"sin\",\"sun\",\"sün\",\"n\",\"ın\",\"in\",\"un\",\"ün\"],\n",
    "        \"A3sg\":[\"\"],\n",
    "        \"A1pl\": [\"yız\",\"yiz\",\"k\",\"yuz\",\"yüz\",\"ız\",\"iz\",\"uz\",\"üz\"],\n",
    "        \"A2pl\":[\"sınız\",\"siniz\",\"sunuz\",\"sünüz\",\"nız\",\"niz\",\"nuz\",\"nüz\",\"ınız\",\"iniz\",\"unuz\",\"ünüz\"],\n",
    "        \"A3pl\": [\"lar\",\"ler\"],\n",
    "        ### Numeral suffixes\n",
    "        \"Grouping\": [\"şar\",\"şer\",\"ız\",\"iz\",\"ar\",\"er\"],\n",
    "        \"Ordinal\":[\"ıncı\",\"inci\",\"uncu\",\"üncü\",\"ncı\",\"nci\",\"ncu\",\"ncü\"],\n",
    "        \"Division\":[\"da\",\"de\",\"ta\",\"te\"],\n",
    "        ### isim kontrolü\n",
    "        \"Noun\":[\"CL_ISIM\",\"CL_OI\"],\n",
    "        \n",
    "    }\n",
    "\n",
    "    ozel_durumlar = {\n",
    "        #\"ımız\": [\"ı\", \"mız\"],\n",
    "        #\"sinler\":[\"s\",\"i\",\"n\",\"ler\"],\n",
    "    }\n",
    "\n",
    "    sonuclar = []\n",
    "\n",
    "    while input_str:\n",
    "        bulundu = False\n",
    "        ozelBulundu = False\n",
    "        en_uzun_ek = \"\"\n",
    "        ek_adi = \"\"\n",
    "\n",
    "        for ozel_ek, ayrilma_sekli in ozel_durumlar.items():\n",
    "            if input_str.startswith(ozel_ek):\n",
    "                for ayrilma in ayrilma_sekli:\n",
    "                    for ad, ekler in ek_dosyalar.items():\n",
    "                        if ayrilma in ekler and input_str.startswith(ayrilma):\n",
    "                            en_uzun_ek = ayrilma\n",
    "                            ek_adi = f\"({ad})\"\n",
    "                            ozelBulundu = True\n",
    "                            if ozelBulundu:\n",
    "                                input_str = input_str[len(en_uzun_ek):].strip()\n",
    "                                sonuclar.append((f\"-{en_uzun_ek} Eki\", ek_adi))\n",
    "\n",
    "        if not ozelBulundu:\n",
    "            for ad, ekler in ek_dosyalar.items():\n",
    "                for ek in ekler:\n",
    "                    if input_str.startswith(ek) and len(ek) > len(en_uzun_ek):\n",
    "                        en_uzun_ek = ek\n",
    "                        ek_adi = f\"({ad})\"\n",
    "                        bulundu = True\n",
    "\n",
    "            if bulundu:\n",
    "                input_str = input_str[len(en_uzun_ek):].strip()\n",
    "                sonuclar.append((f\"-{en_uzun_ek} Eki\", ek_adi))\n",
    "            else:\n",
    "                sonuclar.append((\"BULUNAMADI\", f\"({input_str})\"))\n",
    "                break\n",
    "\n",
    "    return sonuclar\n",
    "\n",
    "sonuclar = parcala_ve_kontrol_et(input(\"ek giriniz: \"))\n",
    "\n",
    "for ek, dosya_adi in sonuclar:\n",
    "    print(f\"{ek} {dosya_adi}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bbb961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbdf586",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def sentence_splitter(text):\n",
    "    sentence_endings = r\"[.!?]\"\n",
    "    sentences = re.split(sentence_endings, text)\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    return sentences\n",
    "\n",
    "def tokenizer(text):\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "def parcala_ve_kontrol_et(input_str):\n",
    "    ek_dosyalar = {\n",
    "        \n",
    "        \"Opt\":[\"yalım\",\"yelim\",\"alım\",\"elim\"],\n",
    "        \"Opt+A2sg\":[\"yasın\",\"yesin\",\"asın\",\"esin\"],\n",
    "        \"Opt+A2pl\":[\"yasınız\",\"yesiniz\",\"asınız\",\"esiniz\"],\n",
    "        \"Opt+A1sg\":[\"yayım\",\"yeyim\",\"ayım\",\"eyim\"],\n",
    "        ## noun suffixes\n",
    "        \"Pl\": [\"lar\", \"ler\"],\n",
    "        ## case suffixes\n",
    "        \"Dat\": [\"ya\",\"ye\",\"e\",\"a\",\"na\",\"ne\"],\n",
    "        \"Loc\": [\"da\", \"de\", \"ta\", \"te\",\"nde\",\"nda\"],\n",
    "        \"Abl\": [\"dan\", \"den\", \"tan\", \"ten\",\"ndan\",\"nden\"],\n",
    "        \"Gen\": [\"nın\", \"nin\",\"nun\",\"nün\",\"ın\",\"in\",\"un\", \"ün\",\"yın\",\"yin\",\"yun\",\"yün\",\"ım\",\"im\",\"um\",\"üm\",\"m\"],\n",
    "        \"Acc\": [\"yı\", \"yi\", \"yu\", \"yü\", \"ı\", \"i\", \"u\",\"ü\",\"nı\",\"ni\",\"nu\",\"nü\"],\n",
    "        \"Inst\": [\"yla\", \"yle\",\"la\",\"le\"],\n",
    "        \"P1sg\": [\"ım\",\"im\",\"um\",\"üm\",\"m\",\"yum\"],\n",
    "        \"P2sg\": [\"ın\",\"in\",\"un\",\"ün\",\"n\",\"yun\"],\n",
    "        \"P3sg\": [\"sı\", \"si\", \"su\", \"sü\",\"ı\",\"i\",\"u\",\"ü\",\"yu\"],\n",
    "        \"P1pl\": [\"ımız\", \"imiz\", \"umuz\", \"ümüz\",\"mız\",\"miz\",\"muz\",\"müz\",\"yumuz\"],\n",
    "        \"P2pl\": [\"ınız\",\"iniz\",\"unuz\",\"ünüz\",\"nız\",\"niz\",\"nuz\",\"nüz\",\"yunuz\"],\n",
    "        \"P3pl\": [\"ı\", \"i\",\"u\",\"ü\",\"ları\",\"leri\"],\n",
    "        \"Dim\": [\"cık\", \"cik\",\"cuk\",\"cük\",\"çık\",\"çik\",\"çuk\",\"çük\",\"cığ\",\"ciğ\",\"cuğ\",\"cüğ\",\"çığ\",\"çiğ\",\"çuğ\",\"çüğ\"],\n",
    "        \"Dim2\": [\"cağız\", \"ceğiz\"],\n",
    "        \"With\":[\"lı\",\"li\",\"lu\",\"lü\"],\n",
    "        \"Without\": [\"sız\", \"siz\",\"suz\",\"süz\"],\n",
    "        \"Since\":[\"dır\",\"dir\",\"dur\",\"dür\",\"tır\",\"tir\",\"tur\",\"tür\"],\n",
    "        \"Related\": [\"sal\", \"sel\"],\n",
    "        \"Ness\":[\"lık\",\"lik\",\"luk\",\"lük\",\"lığ\",\"liğ\",\"luğ\",\"lüğ\"],\n",
    "        \"Acquire\": [\"lan\", \"len\"],\n",
    "        \"Apply\":[\"la\",\"le\"],\n",
    "        \"Resemb\": [\"ımsı\", \"imsi\",\"umsu\",\"ümsü\",\"msı\",\"msi\",\"msu\",\"msü\"],\n",
    "        \"Rel\":[\"kı\",\"ki\",\"ku\",\"kü\"],\n",
    "        \"By\": [\"ca\", \"ce\"],\n",
    "        \"Cmp\":[\"ca\",\"ce\"],\n",
    "        \"Agt\":[\"cı\",\"ci\",\"cu\",\"cü\",\"yıcı\",\"yici\",\"yucu\",\"yücü\"],\n",
    "        ## Adjective Suffixes\n",
    "        \"Become\":[\"laş\",\"leş\"],\n",
    "        \"Ly\":[\"ca\",\"ce\"],\n",
    "        \"Quite\":[\"ca\",\"ce\"],\n",
    "        ## number-person agreement suffixes\n",
    "        \"A1sg\": [\"yım\", \"yim\",\"yum\",\"yüm\",\"m\",\"ım\",\"im\",\"um\",\"üm\"],\n",
    "        \"A2sg\":[\"sın\",\"sin\",\"sun\",\"sün\",\"n\",\"ın\",\"in\",\"un\",\"ün\"],\n",
    "        \"A3sg\":[\"\"],\n",
    "        \"A1pl\": [\"yız\",\"yiz\",\"k\",\"yuz\",\"yüz\",\"ız\",\"iz\",\"uz\",\"üz\"],\n",
    "        \"A2pl\":[\"sınız\",\"siniz\",\"sunuz\",\"sünüz\",\"nız\",\"niz\",\"nuz\",\"nüz\",\"ınız\",\"iniz\",\"unuz\",\"ünüz\"],\n",
    "        \"A3pl\": [\"lar\",\"ler\"],\n",
    "        ### Numeral suffixes\n",
    "        \"Grouping\": [\"şar\",\"şer\",\"ız\",\"iz\",\"ar\",\"er\"],\n",
    "        \"Ordinal\":[\"ıncı\",\"inci\",\"uncu\",\"üncü\",\"ncı\",\"nci\",\"ncu\",\"ncü\"],\n",
    "        \"Division\":[\"da\",\"de\",\"ta\",\"te\"],\n",
    "        ### verbal suffixes\n",
    "        \"Aor\":[\"ır,ir,ur,ür,ar,er,z\"],\n",
    "        \"Prog\":[\"ıyor\",\"iyor\",\"uyor\",\"üyor\"],\n",
    "        \"Prog2\":[\"makta\",\"mekte\"],\n",
    "        \"Fut\":[\"yacak\",\"yecek\",\"yacağ\",\"yeceğ\",\"acak\",\"ecek\",\"acağ\",\"eceğ\"],\n",
    "        \"Past\":[\"dı\",\"di\",\"du\",\"dü\",\"tı\",\"ti\",\"tu\",\"tü\"],\n",
    "        \"Evid\":[\"mış\",\"miş\",\"muş\",\"müş\"],\n",
    "        \"Neg\":[\"ma,me,m\"],\n",
    "        \"Cond\":[\"se\",\"sa\"],\n",
    "        \"Necess\":[\"meli\",\"malı\"],\n",
    "        \"Opt\":[\"ye\",\"ya\",\"e\",\"a\"],\n",
    "        \"Imp\":[\"sın\",\"sin\",\"sun\",\"sün\",\"yın\",\"yin\",\"yun\",\"yün\",\"ın\",\"in\",\"un\",\"ün\",\"sınlar\",\"sinler\",\"sünler\",\"sunlar\",\"yınız\",\"yiniz\",\"yunuz\",\"yünüz\",\"ınız\",\"iniz\",\"unuz\",\"ünüz\"],\n",
    "        ### copular suffixes\n",
    "        \"PastCop\":[\"ydı\",\"ydi\",\"ydu\",\"ydü\",\"dı\",\"di\",\"du\",\"dü\"],\n",
    "        \"EvidCop\":[\"ymış\",\"ymiş\",\"ymuş\",\"ymüş\",\"mış\",\"miş\",\"muş\",\"müş\"],\n",
    "        \"CondCop\":[\"ysa\",\"yse\",\"sa\",\"se\"],\n",
    "        \"While\":[\"yken\",\"ken\"],\n",
    "        \"Cop\":[\"dır\",\"dir\",\"dur\",\"dür\",\"tır\",\"tir\",\"tur\",\"tür\"],\n",
    "        ### Derivational Suffixes\n",
    "        \"Pass\":[\"ıl\",\"il\",\"ul\",\"ül\",\"ın\",\"in\",\"un\",\"ün\",\"n\",\"nıl\",\"nil\",\"nul\",\"nül\"],\n",
    "        \"Caus\":[\"dır\",\"dir\",\"dur\",\"dür\",\"tır\",\"tir\",\"tur\",\"tür\",\"t\"],\n",
    "        \"Recip\":[\"yış\",\"yiş\",\"yuş\",\"yüş\",\"ış\",\"iş\",\"uş\",\"üş\",\"ş\"],\n",
    "        \"Reflex\":[\"ın\",\"in\",\"un\",\"ün\"],\n",
    "        \"Abil\":[\"yabıl\",\"yabil\",\"yabul\",\"yabül\",\"yebil\",\"yebul\",\"yebül\",\"abıl\",\"ebil\",\"abul\",\"abül\",\"ebıl\",\"ebul\",\"ebül\"],\n",
    "        \"NegAbil\":[\"yama\",\"yeme\",\"ama\",\"eme\"],\n",
    "        \"Cont\":[\"yadur\",\"yedur\",\"adur\",\"edur\"],\n",
    "        \"EverSince\":[\"yagel\",\"yegel\",\"agel\",\"egel\"],\n",
    "        \"Cont2\":[\"yagör\",\"yegör\",\"agör\",\"egör\"],\n",
    "        \"Almost\":[\"yayaz\",\"yeyez\",\"yayez\",\"yeyaz\",\"ayaz\",\"eyez\",\"ayez\",\"eyaz\"],\n",
    "        \"Hastily\":[\"yıver\",\"yiver\",\"ıver\",\"iver\"],\n",
    "        \"Stay\":[\"yakal\",\"yekal\",\"akal\",\"ekal\"],\n",
    "        \"Inf\":[\"mak\",\"mek\"],\n",
    "        \"Inf2\":[\"ma\",\"me\"],\n",
    "        \"Inf3\":[\"yış\",\"yiş\",\"yuş\",\"yüş\",\"ış\",\"iş\",\"uş\",\"üş\"],\n",
    "        \"FutPart\":[\"yacak\",\"yecek\",\"yacağ\",\"yeceğ\",\"acak\",\"ecek\",\"acağ\",\"eceğ\"],\n",
    "        \"EvidPart\":[\"mış\",\"miş\",\"muş\",\"müş\"],\n",
    "        \"PastPart\":[\"dık\",\"dik\",\"duk\",\"dük\",\"tık\",\"tik\",\"tuk\",\"tük\",\"dığ\",\"diğ\",\"duğ\",\"düğ\",\"tığ\",\"tiğ\",\"tuğ\",\"tüğ\"],\n",
    "        \"PresPart\":[\"yan\",\"yen\",\"an\",\"en\"],\n",
    "        \"AorPart\":[\"ır\",\"ir\",\"ur\",\"ür\",\"ar\",\"er\",\"r\"],\n",
    "        \"AfterDoing\":[\"yıp\",\"yip\",\"yup\",\"yüp\",\"ıp\",\"ip\",\"up\",\"üp\"],\n",
    "        \"Agt\":[\"yıcı\",\"yici\",\"yucu\",\"yücü\",\"ıcı\",\"ici\",\"ucu\",\"ücü\"],\n",
    "        \"FeelLike\":[\"yası\",\"yesi\",\"ası\",\"esi\"],\n",
    "        \"SinceDoing\":[\"yalı\",\"yeli\",\"alı\",\"eli\"],\n",
    "        \"AsLongAs\":[\"dıkca\",\"dikce\",\"dıkça\",\"dikçe\",\"tıkca\",\"tikce\",\"tıkça\",\"tikçe\"],\n",
    "        \"JustAfter\":[\"yınca\",\"yince\",\"ınca\",\"ince\"],\n",
    "        \"ByDoing\":[\"yarak\",\"yerek\",\"arak\",\"erek\"],\n",
    "        \"WithoutDoing\":[\"madan\",\"meden\",\"adan\",\"eden\"],\n",
    "        \"WithoutDoing2\":[\"maksızın\",\"meksizin\",\"aksızın\",\"eksizin\"],\n",
    "        \"RatherThanInsteadOfDoing\":[\"maktansa\",\"mektense\",\"aktansa\",\"ektense\"],\n",
    "        \"UnableToDo\":[\"yamadan\",\"yemeden\",\"amadan\",\"emeden\"],\n",
    "        \"ActOf\":[\"maca\",\"mece\"],\n",
    "        \"NotState\":[\"mazlık\",\"mezlik\",\"mazlığ\",\"mezliğ\"],\n",
    "        \"AsIf\":[\"casına\",\"cesine\",\"çasına\",\"çesine\"],\n",
    "        \"Adamantly\":[\"yasıya\",\"yesiye\",\"asıya\",\"esiye\"],\n",
    "\n",
    "    }\n",
    "    ozel_durumlar = {\n",
    "        #\"ımız\": [\"ı\", \"mız\"],\n",
    "        #\"sinler\":[\"s\",\"i\",\"n\",\"ler\"],\n",
    "        #\"leri\":[\"ler\",\"i\"],\n",
    "        #\"sının\":[\"sı\",\"nın\"],\n",
    "        #\"elim\":[\"e\",\"lim\"],\n",
    "    }\n",
    "\n",
    "    sonuclar = []\n",
    "    en_uzun_ekler=[]\n",
    "\n",
    "    while input_str:\n",
    "        bulundu = False\n",
    "        ozelBulundu = False\n",
    "        en_uzun_ek = \"\"\n",
    "        ek_adi_list = []  \n",
    "\n",
    "        for ozel_ek, ayrilma_sekli in ozel_durumlar.items():\n",
    "            if input_str.startswith(ozel_ek):\n",
    "                for ayrilma in ayrilma_sekli:\n",
    "                    for ad, ekler in ek_dosyalar.items():\n",
    "                        if ayrilma in ekler and input_str.startswith(ayrilma):\n",
    "                            en_uzun_ek = ayrilma\n",
    "                            ek_adi = f\"({ad})\"\n",
    "                            ozelBulundu = True\n",
    "                            if ozelBulundu:\n",
    "                                input_str = input_str[len(en_uzun_ek):].strip()\n",
    "                                ek_adi_list.append(ek_adi)\n",
    "\n",
    "        if not ozelBulundu:\n",
    "            for ad, ekler in ek_dosyalar.items():\n",
    "                for ek in ekler:\n",
    "                    if input_str.startswith(ek) and len(ek) >= len(en_uzun_ek):\n",
    "                        en_uzun_ek = ek\n",
    "                        ek_adi = f\"({ad})\"\n",
    "                        bulundu = True\n",
    "                        \n",
    "                \n",
    "                \n",
    "            if bulundu:\n",
    "                input_str = input_str[len(en_uzun_ek):].strip()\n",
    "                for ad,ekler in ek_dosyalar.items():\n",
    "\n",
    "                    if en_uzun_ek in ekler:\n",
    "                        #print(en_uzun_ek,ekler,\"yakup\")\n",
    "                        ek_adi_list.append(f\"({ad})\")\n",
    "                    \n",
    "                #ek_adi_list.append(ek_adi)\n",
    "            else:\n",
    "                ek_adi_list.append(f\"({input_str})\")\n",
    "\n",
    "        if ek_adi_list:\n",
    "            sonuclar.append((f\"-{en_uzun_ek} \", ek_adi_list))\n",
    "\n",
    "    return sonuclar\n",
    "\n",
    "\n",
    "def find_and_process_suffixes_v2(base_word, dataset_path):\n",
    "    def find_suffixes(word, suffixes):\n",
    "        found_suffixes = [suffix for suffix in suffixes if word.endswith(suffix)]\n",
    "        return found_suffixes\n",
    "\n",
    "    def process_dataset(base_word, suffixes):\n",
    "        matches = []\n",
    "        suffix_categories = []  \n",
    "\n",
    "        with open(dataset_path, 'r', encoding='utf-8') as dataset_file:\n",
    "            for line in dataset_file:\n",
    "                parts = line.strip().split()\n",
    "\n",
    "                if len(parts[0]) >= 2:\n",
    "                    current_word, word_type = parts[0], parts[1:]\n",
    "                    \n",
    "                    if base_word == current_word and not suffixes:\n",
    "                        matches.append(current_word)\n",
    "                        suffix_categories.append(f\"({word_type})\")\n",
    "                        \n",
    "                        \n",
    "                    for suffix in suffixes:\n",
    "                        if base_word == current_word:\n",
    "                            matches.append(current_word)\n",
    "                            suffix_categories.append(f\"({word_type})\")\n",
    "\n",
    "                            sonuclar = parcala_ve_kontrol_et(suffix)\n",
    "                            for ek, dosya_adi in sonuclar:\n",
    "        \n",
    "                                matches.append(ek)\n",
    "                                suffix_categories.append(f\"({dosya_adi})\")\n",
    "                        else:\n",
    "                            if base_word.endswith('b'):\n",
    "                                base_word = base_word[:-1] + 'p'\n",
    "                            elif base_word.endswith('c'):\n",
    "                                base_word = base_word[:-1] + 'ç'\n",
    "                            elif base_word.endswith('d'):\n",
    "                                base_word = base_word[:-1] + 't'\n",
    "                            elif base_word.endswith('ğ'):\n",
    "                                base_word = base_word[:-1] + 'k'\n",
    "                            if base_word == current_word:\n",
    "                                matches.append(current_word)\n",
    "                                suffix_categories.append(f\"({word_type})\")\n",
    "\n",
    "                                sonuclar = parcala_ve_kontrol_et(suffix)\n",
    "                                for ek, dosya_adi in sonuclar:\n",
    "                                    matches.append(ek)\n",
    "                                    suffix_categories.append(f\"({dosya_adi})\")\n",
    "\n",
    "        return matches, suffix_categories\n",
    "\n",
    "    with open('C:/endings.txt', 'r', encoding='utf-8') as endings_file:\n",
    "        suffixes = [line.strip() for line in endings_file]\n",
    "\n",
    "    eksizEslesme, suffix_categories = process_dataset(base_word, [])\n",
    "    \n",
    "\n",
    "    if eksizEslesme:\n",
    "        print(''.join(eksizEslesme), ''.join(suffix_categories))\n",
    "        return\n",
    "\n",
    "    all_suffixes = find_suffixes(base_word, suffixes)\n",
    "   \n",
    "    \n",
    "    if all_suffixes:\n",
    "        result_suffixes = []\n",
    "        for suffix in sorted(all_suffixes, key=len, reverse=True):\n",
    "            remaining_word = base_word[:-len(suffix)]\n",
    "            matches, suffix_categories = process_dataset(remaining_word, [suffix])\n",
    "            \n",
    "\n",
    "            if matches:\n",
    "                result_suffixes.extend(matches)\n",
    "                result_suffixes.extend(suffix_categories)\n",
    "                \n",
    "        \n",
    "        print(''.join(result_suffixes))\n",
    "\n",
    "    else:\n",
    "        print(f\"{base_word} kelimesi için eşleşme bulunamadı.\")\n",
    "\n",
    "\n",
    "\n",
    "text = input(\"Türkçe metin giriniz:\")\n",
    "text = text.lower()\n",
    "\n",
    "sentences = sentence_splitter(text)\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokens = tokenizer(sentence)\n",
    "    processed_tokens = []\n",
    "\n",
    "    for token in tokens:\n",
    "        find_and_process_suffixes_v2(token, 'C:/dataset.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7389bea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def sentence_splitter(text):\n",
    "    sentence_endings = r\"[.!?]\"\n",
    "    sentences = re.split(sentence_endings, text)\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    return sentences\n",
    "\n",
    "def tokenizer(text):\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "def parcala_ve_kontrol_et(input_str):\n",
    "    ek_dosyalar = {\n",
    "        ## noun suffixes\n",
    "        \"Pl\": [\"lar\", \"ler\"],\n",
    "        ## case suffixes\n",
    "        \"Dat\": [\"ya\",\"ye\",\"e\",\"a\",\"na\",\"ne\"],\n",
    "        \"Loc\": [\"da\", \"de\", \"ta\", \"te\",\"nde\",\"nda\"],\n",
    "        \"Abl\": [\"dan\", \"den\", \"tan\", \"ten\",\"ndan\",\"nden\"],\n",
    "        \"Gen\": [\"nın\", \"nin\",\"nun\",\"nün\",\"ın\",\"in\",\"un\", \"ün\",\"yın\",\"yin\",\"yun\",\"yün\",\"ım\",\"im\",\"um\",\"üm\",\"m\"],\n",
    "        \"Acc\": [\"yı\", \"yi\", \"yu\", \"yü\", \"ı\", \"i\", \"u\",\"ü\",\"nı\",\"ni\",\"nu\",\"nü\"],\n",
    "        \"Inst\": [\"yla\", \"yle\",\"la\",\"le\"],\n",
    "        \"P1sg\": [\"ım\",\"im\",\"um\",\"üm\",\"m\",\"yum\"],\n",
    "        \"P2sg\": [\"ın\",\"in\",\"un\",\"ün\",\"n\",\"yun\"],\n",
    "        \"P3sg\": [\"sı\", \"si\", \"su\", \"sü\",\"ı\",\"i\",\"u\",\"ü\",\"yu\"],\n",
    "        \"P1pl\": [\"ımız\", \"imiz\", \"umuz\", \"ümüz\",\"mız\",\"miz\",\"muz\",\"müz\",\"yumuz\"],\n",
    "        \"P2pl\": [\"ınız\",\"iniz\",\"unuz\",\"ünüz\",\"nız\",\"niz\",\"nuz\",\"nüz\",\"yunuz\"],\n",
    "        \"P3pl\": [\"ı\", \"i\",\"u\",\"ü\",\"ları\",\"leri\"],\n",
    "        \"Dim\": [\"cık\", \"cik\",\"cuk\",\"cük\",\"çık\",\"çik\",\"çuk\",\"çük\",\"cığ\",\"ciğ\",\"cuğ\",\"cüğ\",\"çığ\",\"çiğ\",\"çuğ\",\"çüğ\"],\n",
    "        \"Dim2\": [\"cağız\", \"ceğiz\"],\n",
    "        \"With\":[\"lı\",\"li\",\"lu\",\"lü\"],\n",
    "        \"Without\": [\"sız\", \"siz\",\"suz\",\"süz\"],\n",
    "        \"Since\":[\"dır\",\"dir\",\"dur\",\"dür\",\"tır\",\"tir\",\"tur\",\"tür\"],\n",
    "        \"Related\": [\"sal\", \"sel\"],\n",
    "        \"Ness\":[\"lık\",\"lik\",\"luk\",\"lük\",\"lığ\",\"liğ\",\"luğ\",\"lüğ\"],\n",
    "        \"Acquire\": [\"lan\", \"len\"],\n",
    "        \"Apply\":[\"la\",\"le\"],\n",
    "        \"Resemb\": [\"ımsı\", \"imsi\",\"umsu\",\"ümsü\",\"msı\",\"msi\",\"msu\",\"msü\"],\n",
    "        \"Rel\":[\"kı\",\"ki\",\"ku\",\"kü\"],\n",
    "        \"By\": [\"ca\", \"ce\"],\n",
    "        \"Cmp\":[\"ca\",\"ce\"],\n",
    "        \"Agt\":[\"cı\",\"ci\",\"cu\",\"cü\",\"yıcı\",\"yici\",\"yucu\",\"yücü\"],\n",
    "        ## Adjective Suffixes\n",
    "        \"Become\":[\"laş\",\"leş\"],\n",
    "        \"Ly\":[\"ca\",\"ce\"],\n",
    "        \"Quite\":[\"ca\",\"ce\"],\n",
    "        ## number-person agreement suffixes\n",
    "        \"A1sg\": [\"yım\", \"yim\",\"yum\",\"yüm\",\"m\",\"ım\",\"im\",\"um\",\"üm\"],\n",
    "        \"A2sg\":[\"sın\",\"sin\",\"sun\",\"sün\",\"n\",\"ın\",\"in\",\"un\",\"ün\"],\n",
    "        \"A3sg\":[\"\"],\n",
    "        \"A1pl\": [\"yız\",\"yiz\",\"k\",\"yuz\",\"yüz\",\"ız\",\"iz\",\"uz\",\"üz\"],\n",
    "        \"A2pl\":[\"sınız\",\"siniz\",\"sunuz\",\"sünüz\",\"nız\",\"niz\",\"nuz\",\"nüz\",\"ınız\",\"iniz\",\"unuz\",\"ünüz\"],\n",
    "        \"A3pl\": [\"lar\",\"ler\"],\n",
    "        ### Numeral suffixes\n",
    "        \"Grouping\": [\"şar\",\"şer\",\"ız\",\"iz\",\"ar\",\"er\"],\n",
    "        \"Ordinal\":[\"ıncı\",\"inci\",\"uncu\",\"üncü\",\"ncı\",\"nci\",\"ncu\",\"ncü\"],\n",
    "        \"Division\":[\"da\",\"de\",\"ta\",\"te\"],\n",
    "        ### verbal suffixes\n",
    "        \"Aor\":[\"ır,ir,ur,ür,ar,er,z\"],\n",
    "        \"Prog\":[\"ıyor\",\"iyor\",\"uyor\",\"üyor\"],\n",
    "        \"Prog2\":[\"makta\",\"mekte\"],\n",
    "        \"Fut\":[\"yacak\",\"yecek\",\"yacağ\",\"yeceğ\",\"acak\",\"ecek\",\"acağ\",\"eceğ\"],\n",
    "        \"Past\":[\"dı\",\"di\",\"du\",\"dü\",\"tı\",\"ti\",\"tu\",\"tü\"],\n",
    "        \"Evid\":[\"mış\",\"miş\",\"muş\",\"müş\"],\n",
    "        \"Neg\":[\"ma,me,m\"],\n",
    "        \"Cond\":[\"se\",\"sa\"],\n",
    "        \"Necess\":[\"meli\",\"malı\"],\n",
    "        \"Opt\":[\"ye\",\"ya\",\"e\",\"a\"],\n",
    "        \"Imp\":[\"sın\",\"sin\",\"sun\",\"sün\",\"yın\",\"yin\",\"yun\",\"yün\",\"ın\",\"in\",\"un\",\"ün\",\"sınlar\",\"sinler\",\"sünler\",\"sunlar\",\"yınız\",\"yiniz\",\"yunuz\",\"yünüz\",\"ınız\",\"iniz\",\"unuz\",\"ünüz\"],\n",
    "        ### copular suffixes\n",
    "        \"PastCop\":[\"ydı\",\"ydi\",\"ydu\",\"ydü\",\"dı\",\"di\",\"du\",\"dü\"],\n",
    "        \"EvidCop\":[\"ymış\",\"ymiş\",\"ymuş\",\"ymüş\",\"mış\",\"miş\",\"muş\",\"müş\"],\n",
    "        \"CondCop\":[\"ysa\",\"yse\",\"sa\",\"se\"],\n",
    "        \"While\":[\"yken\",\"ken\"],\n",
    "        \"Cop\":[\"dır\",\"dir\",\"dur\",\"dür\",\"tır\",\"tir\",\"tur\",\"tür\"],\n",
    "        ### Derivational Suffixes\n",
    "        \"Pass\":[\"ıl\",\"il\",\"ul\",\"ül\",\"ın\",\"in\",\"un\",\"ün\",\"n\",\"nıl\",\"nil\",\"nul\",\"nül\"],\n",
    "        \"Caus\":[\"dır\",\"dir\",\"dur\",\"dür\",\"tır\",\"tir\",\"tur\",\"tür\",\"t\"],\n",
    "        \"Recip\":[\"yış\",\"yiş\",\"yuş\",\"yüş\",\"ış\",\"iş\",\"uş\",\"üş\",\"ş\"],\n",
    "        \"Reflex\":[\"ın\",\"in\",\"un\",\"ün\"],\n",
    "        \"Abil\":[\"yabıl\",\"yabil\",\"yabul\",\"yabül\",\"yebil\",\"yebul\",\"yebül\",\"abıl\",\"ebil\",\"abul\",\"abül\",\"ebıl\",\"ebul\",\"ebül\"],\n",
    "        \"NegAbil\":[\"yama\",\"yeme\",\"ama\",\"eme\"],\n",
    "        \"Cont\":[\"yadur\",\"yedur\",\"adur\",\"edur\"],\n",
    "        \"EverSince\":[\"yagel\",\"yegel\",\"agel\",\"egel\"],\n",
    "        \"Cont2\":[\"yagör\",\"yegör\",\"agör\",\"egör\"],\n",
    "        \"Almost\":[\"yayaz\",\"yeyez\",\"yayez\",\"yeyaz\",\"ayaz\",\"eyez\",\"ayez\",\"eyaz\"],\n",
    "        \"Hastily\":[\"yıver\",\"yiver\",\"ıver\",\"iver\"],\n",
    "        \"Stay\":[\"yakal\",\"yekal\",\"akal\",\"ekal\"],\n",
    "        \"Inf\":[\"mak\",\"mek\"],\n",
    "        \"Inf2\":[\"ma\",\"me\"],\n",
    "        \"Inf3\":[\"yış\",\"yiş\",\"yuş\",\"yüş\",\"ış\",\"iş\",\"uş\",\"üş\"],\n",
    "        \"FutPart\":[\"yacak\",\"yecek\",\"yacağ\",\"yeceğ\",\"acak\",\"ecek\",\"acağ\",\"eceğ\"],\n",
    "        \"EvidPart\":[\"mış\",\"miş\",\"muş\",\"müş\"],\n",
    "        \"PastPart\":[\"dık\",\"dik\",\"duk\",\"dük\",\"tık\",\"tik\",\"tuk\",\"tük\",\"dığ\",\"diğ\",\"duğ\",\"düğ\",\"tığ\",\"tiğ\",\"tuğ\",\"tüğ\"],\n",
    "        \"PresPart\":[\"yan\",\"yen\",\"an\",\"en\"],\n",
    "        \"AorPart\":[\"ır\",\"ir\",\"ur\",\"ür\",\"ar\",\"er\",\"r\"],\n",
    "        \"AfterDoing\":[\"yıp\",\"yip\",\"yup\",\"yüp\",\"ıp\",\"ip\",\"up\",\"üp\"],\n",
    "        \"Agt\":[\"yıcı\",\"yici\",\"yucu\",\"yücü\",\"ıcı\",\"ici\",\"ucu\",\"ücü\"],\n",
    "        \"FeelLike\":[\"yası\",\"yesi\",\"ası\",\"esi\"],\n",
    "        \"SinceDoing\":[\"yalı\",\"yeli\",\"alı\",\"eli\"],\n",
    "        \"AsLongAs\":[\"dıkca\",\"dikce\",\"dıkça\",\"dikçe\",\"tıkca\",\"tikce\",\"tıkça\",\"tikçe\"],\n",
    "        \"JustAfter\":[\"yınca\",\"yince\",\"ınca\",\"ince\"],\n",
    "        \"ByDoing\":[\"yarak\",\"yerek\",\"arak\",\"erek\"],\n",
    "        \"WithoutDoing\":[\"madan\",\"meden\",\"adan\",\"eden\"],\n",
    "        \"WithoutDoing2\":[\"maksızın\",\"meksizin\",\"aksızın\",\"eksizin\"],\n",
    "        \"RatherThanInsteadOfDoing\":[\"maktansa\",\"mektense\",\"aktansa\",\"ektense\"],\n",
    "        \"UnableToDo\":[\"yamadan\",\"yemeden\",\"amadan\",\"emeden\"],\n",
    "        \"ActOf\":[\"maca\",\"mece\"],\n",
    "        \"NotState\":[\"mazlık\",\"mezlik\",\"mazlığ\",\"mezliğ\"],\n",
    "        \"AsIf\":[\"casına\",\"cesine\",\"çasına\",\"çesine\"],\n",
    "        \"Adamantly\":[\"yasıya\",\"yesiye\",\"asıya\",\"esiye\"],\n",
    "        \n",
    "    }\n",
    "    ozel_durumlar = {\n",
    "        #\"ımız\": [\"ı\", \"mız\"],\n",
    "        #\"sinler\":[\"s\",\"i\",\"n\",\"ler\"],\n",
    "        #\"leri\":[\"ler\",\"i\"],\n",
    "        #\"sının\":[\"sı\",\"nın\"],\n",
    "        \"elim\":[\"e\",\"lim\"],\n",
    "    }\n",
    "\n",
    "    sonuclar = []\n",
    "    en_uzun_ekler=[]\n",
    "\n",
    "    while input_str:\n",
    "        bulundu = False\n",
    "        ozelBulundu = False\n",
    "        en_uzun_ek = \"\"\n",
    "        ek_adi_list = []  \n",
    "\n",
    "        for ozel_ek, ayrilma_sekli in ozel_durumlar.items():\n",
    "            if input_str.startswith(ozel_ek):\n",
    "                for ayrilma in ayrilma_sekli:\n",
    "                    for ad, ekler in ek_dosyalar.items():\n",
    "                        if ayrilma in ekler and input_str.startswith(ayrilma):\n",
    "                            en_uzun_ek = ayrilma\n",
    "                            ek_adi = f\"({ad})\"\n",
    "                            ozelBulundu = True\n",
    "                            if ozelBulundu:\n",
    "                                input_str = input_str[len(en_uzun_ek):].strip()\n",
    "                                ek_adi_list.append(ek_adi)\n",
    "\n",
    "        if not ozelBulundu:\n",
    "            for ad, ekler in ek_dosyalar.items():\n",
    "                for ek in ekler:\n",
    "                    if input_str.startswith(ek) and len(ek) >= len(en_uzun_ek):\n",
    "                        en_uzun_ek = ek\n",
    "                        ek_adi = f\"({ad})\"\n",
    "                        bulundu = True\n",
    "                        \n",
    "                \n",
    "                \n",
    "            if bulundu:\n",
    "                input_str = input_str[len(en_uzun_ek):].strip()\n",
    "                for ad,ekler in ek_dosyalar.items():\n",
    "\n",
    "                    if en_uzun_ek in ekler:\n",
    "                        #print(en_uzun_ek,ekler,\"yakup\")\n",
    "                        ek_adi_list.append(f\"({ad})\")\n",
    "                    \n",
    "                #ek_adi_list.append(ek_adi)\n",
    "            else:\n",
    "                ek_adi_list.append(f\"({input_str})\")\n",
    "\n",
    "        if ek_adi_list:\n",
    "            sonuclar.append((f\"-{en_uzun_ek} \", ek_adi_list))\n",
    "\n",
    "    return sonuclar\n",
    "\n",
    "\n",
    "def find_and_process_suffixes_v2(base_word, dataset_path):\n",
    "    def find_suffixes(word, suffixes):\n",
    "        found_suffixes = [suffix for suffix in suffixes if word.endswith(suffix)]\n",
    "        return found_suffixes\n",
    "\n",
    "    def process_dataset(base_word, suffixes):\n",
    "        matches = []\n",
    "        suffix_categories = []  \n",
    "\n",
    "        with open(dataset_path, 'r', encoding='utf-8') as dataset_file:\n",
    "            for line in dataset_file:\n",
    "                parts = line.strip().split()\n",
    "\n",
    "                if len(parts[0]) >= 2:\n",
    "                    current_word, word_type = parts[0], parts[1:]\n",
    "                    \n",
    "                    if base_word == current_word and not suffixes:\n",
    "                        matches.append(current_word)\n",
    "                        suffix_categories.append(f\"({word_type})\")\n",
    "                        \n",
    "                        \n",
    "                    for suffix in suffixes:\n",
    "                        if base_word == current_word:\n",
    "                            matches.append(current_word)\n",
    "                            suffix_categories.append(f\"({word_type})\")\n",
    "\n",
    "                            sonuclar = parcala_ve_kontrol_et(suffix)\n",
    "                            for ek, dosya_adi in sonuclar:\n",
    "        \n",
    "                                matches.append(ek)\n",
    "                                suffix_categories.append(f\"({dosya_adi})\")\n",
    "                        else:\n",
    "                            if base_word.endswith('b'):\n",
    "                                base_word = base_word[:-1] + 'p'\n",
    "                            elif base_word.endswith('c'):\n",
    "                                base_word = base_word[:-1] + 'ç'\n",
    "                            elif base_word.endswith('d'):\n",
    "                                base_word = base_word[:-1] + 't'\n",
    "                            elif base_word.endswith('ğ'):\n",
    "                                base_word = base_word[:-1] + 'k'\n",
    "                            if base_word == current_word:\n",
    "                                matches.append(current_word)\n",
    "                                suffix_categories.append(f\"({word_type})\")\n",
    "\n",
    "                                sonuclar = parcala_ve_kontrol_et(suffix)\n",
    "                                for ek, dosya_adi in sonuclar:\n",
    "                                    matches.append(ek)\n",
    "                                    suffix_categories.append(f\"({dosya_adi})\")\n",
    "\n",
    "        return matches, suffix_categories\n",
    "\n",
    "    with open('C:/endings.txt', 'r', encoding='utf-8') as endings_file:\n",
    "        suffixes = [line.strip() for line in endings_file]\n",
    "\n",
    "    eksizEslesme, suffix_categories = process_dataset(base_word, [])\n",
    "    \n",
    "\n",
    "    if eksizEslesme:\n",
    "        print(''.join(eksizEslesme), ''.join(suffix_categories))\n",
    "        return\n",
    "\n",
    "    all_suffixes = find_suffixes(base_word, suffixes)\n",
    "   \n",
    "    \n",
    "    if all_suffixes:\n",
    "        result_suffixes = []\n",
    "        for suffix in sorted(all_suffixes, key=len, reverse=True):\n",
    "            remaining_word = base_word[:-len(suffix)]\n",
    "            matches, suffix_categories = process_dataset(remaining_word, [suffix])\n",
    "            \n",
    "\n",
    "            if matches:\n",
    "                result_suffixes.extend(matches)\n",
    "                result_suffixes.extend(suffix_categories)\n",
    "                \n",
    "        \n",
    "        print(''.join(result_suffixes))\n",
    "\n",
    "    else:\n",
    "        print(f\"{base_word} kelimesi için eşleşme bulunamadı.\")\n",
    "\n",
    "\n",
    "\n",
    "text = input(\"Türkçe metin giriniz:\")\n",
    "text = text.lower()\n",
    "\n",
    "sentences = sentence_splitter(text)\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokens = tokenizer(sentence)\n",
    "    processed_tokens = []\n",
    "\n",
    "    for token in tokens:\n",
    "        find_and_process_suffixes_v2(token, 'C:/dataset.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecb8270",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Türkçe metin giriniz:alicanın ve lokmanın bir de yakubun nargilesi ve telefonları ve bilgisayarları var\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9948ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def sentence_splitter(text):\n",
    "    sentence_endings = r\"[.!?]\"\n",
    "    sentences = re.split(sentence_endings, text)\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    return sentences\n",
    "\n",
    "def tokenizer(text):\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "def parcala_ve_kontrol_et(input_str):\n",
    "    ek_dosyalar = {\n",
    "        ## noun suffixes\n",
    "        \"Pl\": [\"lar\", \"ler\"],\n",
    "        ## case suffixes\n",
    "        \"Dat\": [\"ya\",\"ye\",\"e\",\"a\",\"na\",\"ne\"],\n",
    "        \"Loc\": [\"da\", \"de\", \"ta\", \"te\",\"nde\",\"nda\"],\n",
    "        \"Abl\": [\"dan\", \"den\", \"tan\", \"ten\",\"ndan\",\"nden\"],\n",
    "        \"Gen\": [\"nın\", \"nin\",\"nun\",\"nün\",\"ın\",\"in\",\"un\", \"ün\",\"yın\",\"yin\",\"yun\",\"yün\",\"ım\",\"im\",\"um\",\"üm\",\"m\"],\n",
    "        \"Acc\": [\"yı\", \"yi\", \"yu\", \"yü\", \"ı\", \"i\", \"u\",\"ü\",\"nı\",\"ni\",\"nu\",\"nü\"],\n",
    "        \"Inst\": [\"yla\", \"yle\",\"la\",\"le\"],\n",
    "        \"P1sg\": [\"ım\",\"im\",\"um\",\"üm\",\"m\",\"yum\"],\n",
    "        \"P2sg\": [\"ın\",\"in\",\"un\",\"ün\",\"n\",\"yun\"],\n",
    "        \"P3sg\": [\"sı\", \"si\", \"su\", \"sü\",\"ı\",\"i\",\"u\",\"ü\",\"yu\"],\n",
    "        \"P1pl\": [\"ımız\", \"imiz\", \"umuz\", \"ümüz\",\"mız\",\"miz\",\"muz\",\"müz\",\"yumuz\"],\n",
    "        \"P2pl\": [\"ınız\",\"iniz\",\"unuz\",\"ünüz\",\"nız\",\"niz\",\"nuz\",\"nüz\",\"yunuz\"],\n",
    "        \"P3pl\": [\"ı\", \"i\",\"u\",\"ü\",\"ları\",\"leri\"],\n",
    "        \"Dim\": [\"cık\", \"cik\",\"cuk\",\"cük\",\"çık\",\"çik\",\"çuk\",\"çük\",\"cığ\",\"ciğ\",\"cuğ\",\"cüğ\",\"çığ\",\"çiğ\",\"çuğ\",\"çüğ\"],\n",
    "        \"Dim2\": [\"cağız\", \"ceğiz\"],\n",
    "        \"With\":[\"lı\",\"li\",\"lu\",\"lü\"],\n",
    "        \"Without\": [\"sız\", \"siz\",\"suz\",\"süz\"],\n",
    "        \"Since\":[\"dır\",\"dir\",\"dur\",\"dür\",\"tır\",\"tir\",\"tur\",\"tür\"],\n",
    "        \"Related\": [\"sal\", \"sel\"],\n",
    "        \"Ness\":[\"lık\",\"lik\",\"luk\",\"lük\",\"lığ\",\"liğ\",\"luğ\",\"lüğ\"],\n",
    "        \"Acquire\": [\"lan\", \"len\"],\n",
    "        \"Apply\":[\"la\",\"le\"],\n",
    "        \"Resemb\": [\"ımsı\", \"imsi\",\"umsu\",\"ümsü\",\"msı\",\"msi\",\"msu\",\"msü\"],\n",
    "        \"Rel\":[\"kı\",\"ki\",\"ku\",\"kü\"],\n",
    "        \"By\": [\"ca\", \"ce\"],\n",
    "        \"Cmp\":[\"ca\",\"ce\"],\n",
    "        \"Agt\":[\"cı\",\"ci\",\"cu\",\"cü\",\"yıcı\",\"yici\",\"yucu\",\"yücü\"],\n",
    "        ## Adjective Suffixes\n",
    "        \"Become\":[\"laş\",\"leş\"],\n",
    "        \"Ly\":[\"ca\",\"ce\"],\n",
    "        \"Quite\":[\"ca\",\"ce\"],\n",
    "        ## number-person agreement suffixes\n",
    "        \"A1sg\": [\"yım\", \"yim\",\"yum\",\"yüm\",\"m\",\"ım\",\"im\",\"um\",\"üm\"],\n",
    "        \"A2sg\":[\"sın\",\"sin\",\"sun\",\"sün\",\"n\",\"ın\",\"in\",\"un\",\"ün\"],\n",
    "        \"A3sg\":[\"\"],\n",
    "        \"A1pl\": [\"yız\",\"yiz\",\"k\",\"yuz\",\"yüz\",\"ız\",\"iz\",\"uz\",\"üz\"],\n",
    "        \"A2pl\":[\"sınız\",\"siniz\",\"sunuz\",\"sünüz\",\"nız\",\"niz\",\"nuz\",\"nüz\",\"ınız\",\"iniz\",\"unuz\",\"ünüz\"],\n",
    "        \"A3pl\": [\"lar\",\"ler\"],\n",
    "        ### Numeral suffixes\n",
    "        \"Grouping\": [\"şar\",\"şer\",\"ız\",\"iz\",\"ar\",\"er\"],\n",
    "        \"Ordinal\":[\"ıncı\",\"inci\",\"uncu\",\"üncü\",\"ncı\",\"nci\",\"ncu\",\"ncü\"],\n",
    "        \"Division\":[\"da\",\"de\",\"ta\",\"te\"],\n",
    "        ### verbal suffixes\n",
    "        \"Aor\":[\"ır,ir,ur,ür,ar,er,z\"],\n",
    "        \"Prog\":[\"ıyor\",\"iyor\",\"uyor\",\"üyor\"],\n",
    "        \"Prog2\":[\"makta\",\"mekte\"],\n",
    "        \"Fut\":[\"yacak\",\"yecek\",\"yacağ\",\"yeceğ\",\"acak\",\"ecek\",\"acağ\",\"eceğ\"],\n",
    "        \"Past\":[\"dı\",\"di\",\"du\",\"dü\",\"tı\",\"ti\",\"tu\",\"tü\"],\n",
    "        \"Evid\":[\"mış\",\"miş\",\"muş\",\"müş\"],\n",
    "        \"Neg\":[\"ma,me,m\"],\n",
    "        \"Cond\":[\"se\",\"sa\"],\n",
    "        \"Necess\":[\"meli\",\"malı\"],\n",
    "        \"Opt\":[\"ye\",\"ya\",\"e\",\"a\"],\n",
    "        \"Imp\":[\"sın\",\"sin\",\"sun\",\"sün\",\"yın\",\"yin\",\"yun\",\"yün\",\"ın\",\"in\",\"un\",\"ün\",\"sınlar\",\"sinler\",\"sünler\",\"sunlar\",\"yınız\",\"yiniz\",\"yunuz\",\"yünüz\",\"ınız\",\"iniz\",\"unuz\",\"ünüz\"],\n",
    "        ### copular suffixes\n",
    "        \"PastCop\":[\"ydı\",\"ydi\",\"ydu\",\"ydü\",\"dı\",\"di\",\"du\",\"dü\"],\n",
    "        \"EvidCop\":[\"ymış\",\"ymiş\",\"ymuş\",\"ymüş\",\"mış\",\"miş\",\"muş\",\"müş\"],\n",
    "        \"CondCop\":[\"ysa\",\"yse\",\"sa\",\"se\"],\n",
    "        \"While\":[\"yken\",\"ken\"],\n",
    "        \"Cop\":[\"dır\",\"dir\",\"dur\",\"dür\",\"tır\",\"tir\",\"tur\",\"tür\"],\n",
    "        ### Derivational Suffixes\n",
    "        \"Pass\":[\"ıl\",\"il\",\"ul\",\"ül\",\"ın\",\"in\",\"un\",\"ün\",\"n\",\"nıl\",\"nil\",\"nul\",\"nül\"],\n",
    "        \"Caus\":[\"dır\",\"dir\",\"dur\",\"dür\",\"tır\",\"tir\",\"tur\",\"tür\",\"t\"],\n",
    "        \"Recip\":[\"yış\",\"yiş\",\"yuş\",\"yüş\",\"ış\",\"iş\",\"uş\",\"üş\",\"ş\"],\n",
    "        \"Reflex\":[\"ın\",\"in\",\"un\",\"ün\"],\n",
    "        \"Abil\":[\"yabıl\",\"yabil\",\"yabul\",\"yabül\",\"yebil\",\"yebul\",\"yebül\",\"abıl\",\"ebil\",\"abul\",\"abül\",\"ebıl\",\"ebul\",\"ebül\"],\n",
    "        \"NegAbil\":[\"yama\",\"yeme\",\"ama\",\"eme\"],\n",
    "        \"Cont\":[\"yadur\",\"yedur\",\"adur\",\"edur\"],\n",
    "        \"EverSince\":[\"yagel\",\"yegel\",\"agel\",\"egel\"],\n",
    "        \"Cont2\":[\"yagör\",\"yegör\",\"agör\",\"egör\"],\n",
    "        \"Almost\":[\"yayaz\",\"yeyez\",\"yayez\",\"yeyaz\",\"ayaz\",\"eyez\",\"ayez\",\"eyaz\"],\n",
    "        \"Hastily\":[\"yıver\",\"yiver\",\"ıver\",\"iver\"],\n",
    "        \"Stay\":[\"yakal\",\"yekal\",\"akal\",\"ekal\"],\n",
    "        \"Inf\":[\"mak\",\"mek\"],\n",
    "        \"Inf2\":[\"ma\",\"me\"],\n",
    "        \"Inf3\":[\"yış\",\"yiş\",\"yuş\",\"yüş\",\"ış\",\"iş\",\"uş\",\"üş\"],\n",
    "        \"FutPart\":[\"yacak\",\"yecek\",\"yacağ\",\"yeceğ\",\"acak\",\"ecek\",\"acağ\",\"eceğ\"],\n",
    "        \"EvidPart\":[\"mış\",\"miş\",\"muş\",\"müş\"],\n",
    "        \"PastPart\":[\"dık\",\"dik\",\"duk\",\"dük\",\"tık\",\"tik\",\"tuk\",\"tük\",\"dığ\",\"diğ\",\"duğ\",\"düğ\",\"tığ\",\"tiğ\",\"tuğ\",\"tüğ\"],\n",
    "        \"PresPart\":[\"yan\",\"yen\",\"an\",\"en\"],\n",
    "        \"AorPart\":[\"ır\",\"ir\",\"ur\",\"ür\",\"ar\",\"er\",\"r\"],\n",
    "        \"AfterDoing\":[\"yıp\",\"yip\",\"yup\",\"yüp\",\"ıp\",\"ip\",\"up\",\"üp\"],\n",
    "        \"Agt\":[\"yıcı\",\"yici\",\"yucu\",\"yücü\",\"ıcı\",\"ici\",\"ucu\",\"ücü\"],\n",
    "        \"FeelLike\":[\"yası\",\"yesi\",\"ası\",\"esi\"],\n",
    "        \"SinceDoing\":[\"yalı\",\"yeli\",\"alı\",\"eli\"],\n",
    "        \"AsLongAs\":[\"dıkca\",\"dikce\",\"dıkça\",\"dikçe\",\"tıkca\",\"tikce\",\"tıkça\",\"tikçe\"],\n",
    "        \"JustAfter\":[\"yınca\",\"yince\",\"ınca\",\"ince\"],\n",
    "        \"ByDoing\":[\"yarak\",\"yerek\",\"arak\",\"erek\"],\n",
    "        \"WithoutDoing\":[\"madan\",\"meden\",\"adan\",\"eden\"],\n",
    "        \"WithoutDoing2\":[\"maksızın\",\"meksizin\",\"aksızın\",\"eksizin\"],\n",
    "        \"RatherThanInsteadOfDoing\":[\"maktansa\",\"mektense\",\"aktansa\",\"ektense\"],\n",
    "        \"UnableToDo\":[\"yamadan\",\"yemeden\",\"amadan\",\"emeden\"],\n",
    "        \"ActOf\":[\"maca\",\"mece\"],\n",
    "        \"NotState\":[\"mazlık\",\"mezlik\",\"mazlığ\",\"mezliğ\"],\n",
    "        \"AsIf\":[\"casına\",\"cesine\",\"çasına\",\"çesine\"],\n",
    "        \"Adamantly\":[\"yasıya\",\"yesiye\",\"asıya\",\"esiye\"],\n",
    "        \n",
    "    }\n",
    "    ozel_durumlar = {\n",
    "        #\"ımız\": [\"ı\", \"mız\"],\n",
    "        #\"sinler\":[\"s\",\"i\",\"n\",\"ler\"],\n",
    "        #\"leri\":[\"ler\",\"i\"],\n",
    "        \"sının\":[\"sı\",\"nın\"],\n",
    "        \n",
    "    }\n",
    "\n",
    "    sonuclar = []\n",
    "\n",
    "    while input_str:\n",
    "        bulundu = False\n",
    "        ozelBulundu = False\n",
    "        en_uzun_ek = \"\"\n",
    "        ek_adi = \"\"\n",
    "\n",
    "        for ozel_ek, ayrilma_sekli in ozel_durumlar.items():\n",
    "            if input_str.startswith(ozel_ek):\n",
    "                for ayrilma in ayrilma_sekli:\n",
    "                    for ad, ekler in ek_dosyalar.items():\n",
    "                        if ayrilma in ekler and input_str.startswith(ayrilma):\n",
    "                            en_uzun_ek = ayrilma\n",
    "                            ek_adi = f\"({ad})\"\n",
    "                            ozelBulundu = True\n",
    "                            if ozelBulundu:\n",
    "                                input_str = input_str[len(en_uzun_ek):].strip()\n",
    "                                sonuclar.append((f\"-{en_uzun_ek}\", ek_adi))\n",
    "\n",
    "        if not ozelBulundu:\n",
    "            for ad, ekler in ek_dosyalar.items():\n",
    "                for ek in ekler:\n",
    "                    if input_str.startswith(ek) and len(ek) > len(en_uzun_ek):\n",
    "                        en_uzun_ek = ek\n",
    "                        ek_adi = f\"({ad})\"\n",
    "                        bulundu = True\n",
    "\n",
    "            if bulundu:\n",
    "                input_str = input_str[len(en_uzun_ek):].strip()\n",
    "                sonuclar.append((f\"-{en_uzun_ek} \", ek_adi))\n",
    "            else:\n",
    "                sonuclar.append((\"BULUNAMADI\", f\"({input_str})\"))\n",
    "                break\n",
    "\n",
    "    return sonuclar\n",
    "\n",
    "\n",
    "def find_and_process_suffixes_v2(base_word, dataset_path):\n",
    "    def find_suffixes(word, suffixes):\n",
    "        found_suffixes = [suffix for suffix in suffixes if word.endswith(suffix)]\n",
    "        return found_suffixes\n",
    "\n",
    "    def process_dataset(base_word, suffixes):\n",
    "        matches = []\n",
    "        suffix_categories = []  \n",
    "\n",
    "        with open(dataset_path, 'r', encoding='utf-8') as dataset_file:\n",
    "            for line in dataset_file:\n",
    "                parts = line.strip().split()\n",
    "\n",
    "                if len(parts[0]) >= 2:\n",
    "                    current_word, word_type = parts[0], parts[1:]\n",
    "                    \n",
    "                    if base_word == current_word and not suffixes:\n",
    "                        matches.append(current_word)\n",
    "                        suffix_categories.append(f\"({word_type})\")\n",
    "                        \n",
    "                        \n",
    "                    for suffix in suffixes:\n",
    "                        if base_word == current_word:\n",
    "                            matches.append(current_word)\n",
    "                            suffix_categories.append(f\"({word_type})\")\n",
    "\n",
    "                            sonuclar = parcala_ve_kontrol_et(suffix)\n",
    "                            for ek, dosya_adi in sonuclar:\n",
    "                                matches.append(ek)\n",
    "                                suffix_categories.append(f\"({dosya_adi})\")\n",
    "                        else:\n",
    "                            if base_word.endswith('b'):\n",
    "                                base_word = base_word[:-1] + 'p'\n",
    "                            elif base_word.endswith('c'):\n",
    "                                base_word = base_word[:-1] + 'ç'\n",
    "                            elif base_word.endswith('d'):\n",
    "                                base_word = base_word[:-1] + 't'\n",
    "                            elif base_word.endswith('ğ'):\n",
    "                                base_word = base_word[:-1] + 'k'\n",
    "                            if base_word == current_word:\n",
    "                                matches.append(current_word)\n",
    "                                suffix_categories.append(f\"({word_type})\")\n",
    "\n",
    "                                sonuclar = parcala_ve_kontrol_et(suffix)\n",
    "                                for ek, dosya_adi in sonuclar:\n",
    "                                    matches.append(ek)\n",
    "                                    suffix_categories.append(f\"({dosya_adi})\")\n",
    "\n",
    "        return matches, suffix_categories\n",
    "\n",
    "    with open('C:/endings.txt', 'r', encoding='utf-8') as endings_file:\n",
    "        suffixes = [line.strip() for line in endings_file]\n",
    "\n",
    "    eksizEslesme, suffix_categories = process_dataset(base_word, [])\n",
    "\n",
    "    if eksizEslesme:\n",
    "        print(''.join(eksizEslesme), ''.join(suffix_categories))\n",
    "        return\n",
    "\n",
    "    all_suffixes = find_suffixes(base_word, suffixes)\n",
    "    \n",
    "    if all_suffixes:\n",
    "        result_suffixes = []\n",
    "        for suffix in sorted(all_suffixes, key=len, reverse=True):\n",
    "            remaining_word = base_word[:-len(suffix)]\n",
    "            matches, suffix_categories = process_dataset(remaining_word, [suffix])\n",
    "\n",
    "            if matches:\n",
    "                result_suffixes.extend(matches)\n",
    "                result_suffixes.extend(suffix_categories)\n",
    "\n",
    "        print(''.join(result_suffixes))\n",
    "\n",
    "    else:\n",
    "        print(f\"{base_word} kelimesi için eşleşme bulunamadı.\")\n",
    "\n",
    "\n",
    "\n",
    "text = input(\"Türkçe metin giriniz:\")\n",
    "text = text.lower()\n",
    "\n",
    "sentences = sentence_splitter(text)\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokens = tokenizer(sentence)\n",
    "    processed_tokens = []\n",
    "\n",
    "    for token in tokens:\n",
    "        find_and_process_suffixes_v2(token, 'C:/dataset.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be8dc89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b6fe76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c011a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1c5337",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, render_template, request\n",
    "import re\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "\n",
    "\n",
    "def sentence_splitter(text):\n",
    "    sentence_endings = r\"[.!?]\"\n",
    "    sentences = re.split(sentence_endings, text)\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    return sentences\n",
    "\n",
    "def tokenizer(text):\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "def parcala_ve_kontrol_et(input_str):\n",
    "    ek_dosyalar = {\n",
    "        ## noun suffixes\n",
    "        \"Pl\": [\"lar\", \"ler\"],\n",
    "        ## case suffixes\n",
    "        \"Dat\": [\"ya\",\"ye\",\"e\",\"a\",\"na\",\"ne\"],\n",
    "        \"Loc\": [\"da\", \"de\", \"ta\", \"te\",\"nde\",\"nda\"],\n",
    "        \"Abl\": [\"dan\", \"den\", \"tan\", \"ten\",\"ndan\",\"nden\"],\n",
    "        \"Gen\": [\"nın\", \"nin\",\"nun\",\"nün\",\"ın\",\"in\",\"un\", \"ün\",\"yın\",\"yin\",\"yun\",\"yün\",\"ım\",\"im\",\"um\",\"üm\",\"m\"],\n",
    "        \"Acc\": [\"yı\", \"yi\", \"yu\", \"yü\", \"ı\", \"i\", \"u\",\"ü\",\"nı\",\"ni\",\"nu\",\"nü\"],\n",
    "        \"Inst\": [\"yla\", \"yle\",\"la\",\"le\"],\n",
    "        \"P1sg\": [\"ım\",\"im\",\"um\",\"üm\",\"m\",\"yum\"],\n",
    "        \"P2sg\": [\"ın\",\"in\",\"un\",\"ün\",\"n\",\"yun\"],\n",
    "        \"P3sg\": [\"sı\", \"si\", \"su\", \"sü\",\"ı\",\"i\",\"u\",\"ü\",\"yu\"],\n",
    "        \"P1pl\": [\"ımız\", \"imiz\", \"umuz\", \"ümüz\",\"mız\",\"miz\",\"muz\",\"müz\",\"yumuz\"],\n",
    "        \"P2pl\": [\"ınız\",\"iniz\",\"unuz\",\"ünüz\",\"nız\",\"niz\",\"nuz\",\"nüz\",\"yunuz\"],\n",
    "        \"P3pl\": [\"ı\", \"i\",\"u\",\"ü\",\"ları\",\"leri\"],\n",
    "        \"Dim\": [\"cık\", \"cik\",\"cuk\",\"cük\",\"çık\",\"çik\",\"çuk\",\"çük\",\"cığ\",\"ciğ\",\"cuğ\",\"cüğ\",\"çığ\",\"çiğ\",\"çuğ\",\"çüğ\"],\n",
    "        \"Dim2\": [\"cağız\", \"ceğiz\"],\n",
    "        \"With\":[\"lı\",\"li\",\"lu\",\"lü\"],\n",
    "        \"Without\": [\"sız\", \"siz\",\"suz\",\"süz\"],\n",
    "        \"Since\":[\"dır\",\"dir\",\"dur\",\"dür\",\"tır\",\"tir\",\"tur\",\"tür\"],\n",
    "        \"Related\": [\"sal\", \"sel\"],\n",
    "        \"Ness\":[\"lık\",\"lik\",\"luk\",\"lük\",\"lığ\",\"liğ\",\"luğ\",\"lüğ\"],\n",
    "        \"Acquire\": [\"lan\", \"len\"],\n",
    "        \"Apply\":[\"la\",\"le\"],\n",
    "        \"Resemb\": [\"ımsı\", \"imsi\",\"umsu\",\"ümsü\",\"msı\",\"msi\",\"msu\",\"msü\"],\n",
    "        \"Rel\":[\"kı\",\"ki\",\"ku\",\"kü\"],\n",
    "        \"By\": [\"ca\", \"ce\"],\n",
    "        \"Cmp\":[\"ca\",\"ce\"],\n",
    "        \"Agt\":[\"cı\",\"ci\",\"cu\",\"cü\",\"yıcı\",\"yici\",\"yucu\",\"yücü\"],\n",
    "        ## Adjective Suffixes\n",
    "        \"Become\":[\"laş\",\"leş\"],\n",
    "        \"Ly\":[\"ca\",\"ce\"],\n",
    "        \"Quite\":[\"ca\",\"ce\"],\n",
    "        ## number-person agreement suffixes\n",
    "        \"A1sg\": [\"yım\", \"yim\",\"yum\",\"yüm\",\"m\",\"ım\",\"im\",\"um\",\"üm\"],\n",
    "        \"A2sg\":[\"sın\",\"sin\",\"sun\",\"sün\",\"n\",\"ın\",\"in\",\"un\",\"ün\"],\n",
    "        \"A3sg\":[\"\"],\n",
    "        \"A1pl\": [\"yız\",\"yiz\",\"k\",\"yuz\",\"yüz\",\"ız\",\"iz\",\"uz\",\"üz\"],\n",
    "        \"A2pl\":[\"sınız\",\"siniz\",\"sunuz\",\"sünüz\",\"nız\",\"niz\",\"nuz\",\"nüz\",\"ınız\",\"iniz\",\"unuz\",\"ünüz\"],\n",
    "        \"A3pl\": [\"lar\",\"ler\"],\n",
    "        ### Numeral suffixes\n",
    "        \"Grouping\": [\"şar\",\"şer\",\"ız\",\"iz\",\"ar\",\"er\"],\n",
    "        \"Ordinal\":[\"ıncı\",\"inci\",\"uncu\",\"üncü\",\"ncı\",\"nci\",\"ncu\",\"ncü\"],\n",
    "        \"Division\":[\"da\",\"de\",\"ta\",\"te\"],\n",
    "        ### verbal suffixes\n",
    "        \"Aor\":[\"ır,ir,ur,ür,ar,er,z\"],\n",
    "        \"Prog\":[\"ıyor\",\"iyor\",\"uyor\",\"üyor\"],\n",
    "        \"Prog2\":[\"makta\",\"mekte\"],\n",
    "        \"Fut\":[\"yacak\",\"yecek\",\"yacağ\",\"yeceğ\",\"acak\",\"ecek\",\"acağ\",\"eceğ\"],\n",
    "        \"Past\":[\"dı\",\"di\",\"du\",\"dü\",\"tı\",\"ti\",\"tu\",\"tü\"],\n",
    "        \"Evid\":[\"mış\",\"miş\",\"muş\",\"müş\"],\n",
    "        \"Neg\":[\"ma,me,m\"],\n",
    "        \"Cond\":[\"se\",\"sa\"],\n",
    "        \"Necess\":[\"meli\",\"malı\"],\n",
    "        \"Opt\":[\"ye\",\"ya\",\"e\",\"a\"],\n",
    "        \"Imp\":[\"sın\",\"sin\",\"sun\",\"sün\",\"yın\",\"yin\",\"yun\",\"yün\",\"ın\",\"in\",\"un\",\"ün\",\"sınlar\",\"sinler\",\"sünler\",\"sunlar\",\"yınız\",\"yiniz\",\"yunuz\",\"yünüz\",\"ınız\",\"iniz\",\"unuz\",\"ünüz\"],\n",
    "        ### copular suffixes\n",
    "        \"PastCop\":[\"ydı\",\"ydi\",\"ydu\",\"ydü\",\"dı\",\"di\",\"du\",\"dü\"],\n",
    "        \"EvidCop\":[\"ymış\",\"ymiş\",\"ymuş\",\"ymüş\",\"mış\",\"miş\",\"muş\",\"müş\"],\n",
    "        \"CondCop\":[\"ysa\",\"yse\",\"sa\",\"se\"],\n",
    "        \"While\":[\"yken\",\"ken\"],\n",
    "        \"Cop\":[\"dır\",\"dir\",\"dur\",\"dür\",\"tır\",\"tir\",\"tur\",\"tür\"],\n",
    "        ### Derivational Suffixes\n",
    "        \"Pass\":[\"ıl\",\"il\",\"ul\",\"ül\",\"ın\",\"in\",\"un\",\"ün\",\"n\",\"nıl\",\"nil\",\"nul\",\"nül\"],\n",
    "        \"Caus\":[\"dır\",\"dir\",\"dur\",\"dür\",\"tır\",\"tir\",\"tur\",\"tür\",\"t\"],\n",
    "        \"Recip\":[\"yış\",\"yiş\",\"yuş\",\"yüş\",\"ış\",\"iş\",\"uş\",\"üş\",\"ş\"],\n",
    "        \"Reflex\":[\"ın\",\"in\",\"un\",\"ün\"],\n",
    "        \"Abil\":[\"yabıl\",\"yabil\",\"yabul\",\"yabül\",\"yebil\",\"yebul\",\"yebül\",\"abıl\",\"ebil\",\"abul\",\"abül\",\"ebıl\",\"ebul\",\"ebül\"],\n",
    "        \"NegAbil\":[\"yama\",\"yeme\",\"ama\",\"eme\"],\n",
    "        \"Cont\":[\"yadur\",\"yedur\",\"adur\",\"edur\"],\n",
    "        \"EverSince\":[\"yagel\",\"yegel\",\"agel\",\"egel\"],\n",
    "        \"Cont2\":[\"yagör\",\"yegör\",\"agör\",\"egör\"],\n",
    "        \"Almost\":[\"yayaz\",\"yeyez\",\"yayez\",\"yeyaz\",\"ayaz\",\"eyez\",\"ayez\",\"eyaz\"],\n",
    "        \"Hastily\":[\"yıver\",\"yiver\",\"ıver\",\"iver\"],\n",
    "        \"Stay\":[\"yakal\",\"yekal\",\"akal\",\"ekal\"],\n",
    "        \"Inf\":[\"mak\",\"mek\"],\n",
    "        \"Inf2\":[\"ma\",\"me\"],\n",
    "        \"Inf3\":[\"yış\",\"yiş\",\"yuş\",\"yüş\",\"ış\",\"iş\",\"uş\",\"üş\"],\n",
    "        \"FutPart\":[\"yacak\",\"yecek\",\"yacağ\",\"yeceğ\",\"acak\",\"ecek\",\"acağ\",\"eceğ\"],\n",
    "        \"EvidPart\":[\"mış\",\"miş\",\"muş\",\"müş\"],\n",
    "        \"PastPart\":[\"dık\",\"dik\",\"duk\",\"dük\",\"tık\",\"tik\",\"tuk\",\"tük\",\"dığ\",\"diğ\",\"duğ\",\"düğ\",\"tığ\",\"tiğ\",\"tuğ\",\"tüğ\"],\n",
    "        \"PresPart\":[\"yan\",\"yen\",\"an\",\"en\"],\n",
    "        \"AorPart\":[\"ır\",\"ir\",\"ur\",\"ür\",\"ar\",\"er\",\"r\"],\n",
    "        \"AfterDoing\":[\"yıp\",\"yip\",\"yup\",\"yüp\",\"ıp\",\"ip\",\"up\",\"üp\"],\n",
    "        \"Agt\":[\"yıcı\",\"yici\",\"yucu\",\"yücü\",\"ıcı\",\"ici\",\"ucu\",\"ücü\"],\n",
    "        \"FeelLike\":[\"yası\",\"yesi\",\"ası\",\"esi\"],\n",
    "        \"SinceDoing\":[\"yalı\",\"yeli\",\"alı\",\"eli\"],\n",
    "        \"AsLongAs\":[\"dıkca\",\"dikce\",\"dıkça\",\"dikçe\",\"tıkca\",\"tikce\",\"tıkça\",\"tikçe\"],\n",
    "        \"JustAfter\":[\"yınca\",\"yince\",\"ınca\",\"ince\"],\n",
    "        \"ByDoing\":[\"yarak\",\"yerek\",\"arak\",\"erek\"],\n",
    "        \"WithoutDoing\":[\"madan\",\"meden\",\"adan\",\"eden\"],\n",
    "        \"WithoutDoing2\":[\"maksızın\",\"meksizin\",\"aksızın\",\"eksizin\"],\n",
    "        \"RatherThanInsteadOfDoing\":[\"maktansa\",\"mektense\",\"aktansa\",\"ektense\"],\n",
    "        \"UnableToDo\":[\"yamadan\",\"yemeden\",\"amadan\",\"emeden\"],\n",
    "        \"ActOf\":[\"maca\",\"mece\"],\n",
    "        \"NotState\":[\"mazlık\",\"mezlik\",\"mazlığ\",\"mezliğ\"],\n",
    "        \"AsIf\":[\"casına\",\"cesine\",\"çasına\",\"çesine\"],\n",
    "        \"Adamantly\":[\"yasıya\",\"yesiye\",\"asıya\",\"esiye\"],\n",
    "        \n",
    "    }\n",
    "    ozel_durumlar = {\n",
    "        #\"ımız\": [\"ı\", \"mız\"],\n",
    "        #\"sinler\":[\"s\",\"i\",\"n\",\"ler\"],\n",
    "        #\"leri\":[\"ler\",\"i\"],\n",
    "    }\n",
    "\n",
    "    sonuclar = []\n",
    "\n",
    "    while input_str:\n",
    "        bulundu = False\n",
    "        ozelBulundu = False\n",
    "        en_uzun_ek = \"\"\n",
    "        ek_adi = \"\"\n",
    "\n",
    "        for ozel_ek, ayrilma_sekli in ozel_durumlar.items():\n",
    "            if input_str.startswith(ozel_ek):\n",
    "                for ayrilma in ayrilma_sekli:\n",
    "                    for ad, ekler in ek_dosyalar.items():\n",
    "                        if ayrilma in ekler and input_str.startswith(ayrilma):\n",
    "                            en_uzun_ek = ayrilma\n",
    "                            ek_adi = f\"({ad})\"\n",
    "                            ozelBulundu = True\n",
    "                            if ozelBulundu:\n",
    "                                input_str = input_str[len(en_uzun_ek):].strip()\n",
    "                                sonuclar.append((f\"-{en_uzun_ek} Eki\", ek_adi))\n",
    "\n",
    "        if not ozelBulundu:\n",
    "            for ad, ekler in ek_dosyalar.items():\n",
    "                for ek in ekler:\n",
    "                    if input_str.startswith(ek) and len(ek) > len(en_uzun_ek):\n",
    "                        en_uzun_ek = ek\n",
    "                        ek_adi = f\"({ad})\"\n",
    "                        bulundu = True\n",
    "\n",
    "            if bulundu:\n",
    "                input_str = input_str[len(en_uzun_ek):].strip()\n",
    "                sonuclar.append((f\"-{en_uzun_ek} \", ek_adi))\n",
    "            else:\n",
    "                sonuclar.append((\"BULUNAMADI\", f\"({input_str})\"))\n",
    "                break\n",
    "\n",
    "    return sonuclar\n",
    "\n",
    "\n",
    "def find_and_process_suffixes_v2(base_word, dataset_path):\n",
    "    def find_suffixes(word, suffixes):\n",
    "        found_suffixes = [suffix for suffix in suffixes if word.endswith(suffix)]\n",
    "        return found_suffixes\n",
    "\n",
    "    def process_dataset(base_word, suffixes):\n",
    "        matches = []\n",
    "        suffix_categories = []  \n",
    "\n",
    "        with open(dataset_path, 'r', encoding='utf-8') as dataset_file:\n",
    "            for line in dataset_file:\n",
    "                parts = line.strip().split()\n",
    "\n",
    "                if len(parts[0]) >= 2:\n",
    "                    current_word, word_type = parts[0], parts[1]\n",
    "\n",
    "                    if base_word == current_word and not suffixes:\n",
    "                        matches.append(current_word)\n",
    "                        suffix_categories.append(f\"({word_type})\")\n",
    "\n",
    "                    for suffix in suffixes:\n",
    "                        if base_word == current_word:\n",
    "                            matches.append(current_word)\n",
    "                            suffix_categories.append(f\"({word_type})\")\n",
    "\n",
    "                            sonuclar = parcala_ve_kontrol_et(suffix)\n",
    "                            for ek, dosya_adi in sonuclar:\n",
    "                                matches.append(ek)\n",
    "                                suffix_categories.append(f\"({dosya_adi})\")\n",
    "                        else:\n",
    "                            if base_word.endswith('b'):\n",
    "                                base_word = base_word[:-1] + 'p'\n",
    "                            elif base_word.endswith('c'):\n",
    "                                base_word = base_word[:-1] + 'ç'\n",
    "                            elif base_word.endswith('d'):\n",
    "                                base_word = base_word[:-1] + 't'\n",
    "                            elif base_word.endswith('ğ'):\n",
    "                                base_word = base_word[:-1] + 'k'\n",
    "                            if base_word == current_word:\n",
    "                                matches.append(current_word)\n",
    "                                suffix_categories.append(f\"({word_type})\")\n",
    "\n",
    "                                sonuclar = parcala_ve_kontrol_et(suffix)\n",
    "                                for ek, dosya_adi in sonuclar:\n",
    "                                    matches.append(ek)\n",
    "                                    suffix_categories.append(f\"({dosya_adi})\")\n",
    "\n",
    "        return matches, suffix_categories\n",
    "\n",
    "    with open('C:/endings.txt', 'r', encoding='utf-8') as endings_file:\n",
    "        suffixes = [line.strip() for line in endings_file]\n",
    "\n",
    "    eksizEslesme, suffix_categories = process_dataset(base_word, [])\n",
    "\n",
    "    if eksizEslesme:\n",
    "        return ''.join(eksizEslesme) + ''.join(suffix_categories)\n",
    "\n",
    "    all_suffixes = find_suffixes(base_word, suffixes)\n",
    "\n",
    "    if all_suffixes:\n",
    "        result_suffixes = []\n",
    "        for suffix in sorted(all_suffixes, key=len, reverse=True):\n",
    "            remaining_word = base_word[:-len(suffix)]\n",
    "            matches, suffix_categories = process_dataset(remaining_word, [suffix])\n",
    "\n",
    "            if matches:\n",
    "                result_suffixes.extend(matches)\n",
    "                result_suffixes.extend(suffix_categories)\n",
    "\n",
    "        return ''.join(result_suffixes)\n",
    "\n",
    "    else:\n",
    "        return f\"{base_word} kelimesi için eşleşme bulunamadı.\"\n",
    "\n",
    "\n",
    "\n",
    "@app.route(\"/\", methods=[\"GET\", \"POST\"])\n",
    "def index():\n",
    "    results = []\n",
    "\n",
    "    if request.method == \"POST\":\n",
    "        user_input = request.form.get(\"user_input\")\n",
    "        user_input = user_input.lower()\n",
    "\n",
    "        sentences = sentence_splitter(user_input)\n",
    "\n",
    "        for sentence in sentences:\n",
    "            tokens = tokenizer(sentence)\n",
    "            processed_tokens = []\n",
    "\n",
    "            for token in tokens:\n",
    "                result = find_and_process_suffixes_v2(token, 'C:/dataset.txt')\n",
    "                processed_tokens.append(result)\n",
    "\n",
    "            results.append(processed_tokens)\n",
    "\n",
    "    return render_template(\"index.html\", results=results)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True, use_reloader=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7f78af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc2485d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92ca593",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bc721c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, render_template, request\n",
    "import re\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "\n",
    "\n",
    "def sentence_splitter(text):\n",
    "    sentence_endings = r\"[.!?]\"\n",
    "    sentences = re.split(sentence_endings, text)\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    return sentences\n",
    "\n",
    "def tokenizer(text):\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "def parcala_ve_kontrol_et(input_str):\n",
    "    ek_dosyalar = {\n",
    "        \n",
    "        \"Opt\":[\"yalım\",\"yelim\",\"alım\",\"elim\"],\n",
    "        \"Opt+A2sg\":[\"yasın\",\"yesin\",\"asın\",\"esin\"],\n",
    "        \"Opt+A2pl\":[\"yasınız\",\"yesiniz\",\"asınız\",\"esiniz\"],\n",
    "        \"Opt+A1sg\":[\"yayım\",\"yeyim\",\"ayım\",\"eyim\"],\n",
    "        ## noun suffixes\n",
    "        \"Pl\": [\"lar\", \"ler\"],\n",
    "        ## case suffixes\n",
    "        \"Dat\": [\"ya\",\"ye\",\"e\",\"a\",\"na\",\"ne\"],\n",
    "        \"Loc\": [\"da\", \"de\", \"ta\", \"te\",\"nde\",\"nda\"],\n",
    "        \"Abl\": [\"dan\", \"den\", \"tan\", \"ten\",\"ndan\",\"nden\"],\n",
    "        \"Gen\": [\"nın\", \"nin\",\"nun\",\"nün\",\"ın\",\"in\",\"un\", \"ün\",\"yın\",\"yin\",\"yun\",\"yün\",\"ım\",\"im\",\"um\",\"üm\",\"m\"],\n",
    "        \"Acc\": [\"yı\", \"yi\", \"yu\", \"yü\", \"ı\", \"i\", \"u\",\"ü\",\"nı\",\"ni\",\"nu\",\"nü\"],\n",
    "        \"Inst\": [\"yla\", \"yle\",\"la\",\"le\"],\n",
    "        \"P1sg\": [\"ım\",\"im\",\"um\",\"üm\",\"m\",\"yum\"],\n",
    "        \"P2sg\": [\"ın\",\"in\",\"un\",\"ün\",\"n\",\"yun\"],\n",
    "        \"P3sg\": [\"sı\", \"si\", \"su\", \"sü\",\"ı\",\"i\",\"u\",\"ü\",\"yu\"],\n",
    "        \"P1pl\": [\"ımız\", \"imiz\", \"umuz\", \"ümüz\",\"mız\",\"miz\",\"muz\",\"müz\",\"yumuz\"],\n",
    "        \"P2pl\": [\"ınız\",\"iniz\",\"unuz\",\"ünüz\",\"nız\",\"niz\",\"nuz\",\"nüz\",\"yunuz\"],\n",
    "        \"P3pl\": [\"ı\", \"i\",\"u\",\"ü\",\"ları\",\"leri\"],\n",
    "        \"Dim\": [\"cık\", \"cik\",\"cuk\",\"cük\",\"çık\",\"çik\",\"çuk\",\"çük\",\"cığ\",\"ciğ\",\"cuğ\",\"cüğ\",\"çığ\",\"çiğ\",\"çuğ\",\"çüğ\"],\n",
    "        \"Dim2\": [\"cağız\", \"ceğiz\"],\n",
    "        \"With\":[\"lı\",\"li\",\"lu\",\"lü\"],\n",
    "        \"Without\": [\"sız\", \"siz\",\"suz\",\"süz\"],\n",
    "        \"Since\":[\"dır\",\"dir\",\"dur\",\"dür\",\"tır\",\"tir\",\"tur\",\"tür\"],\n",
    "        \"Related\": [\"sal\", \"sel\"],\n",
    "        \"Ness\":[\"lık\",\"lik\",\"luk\",\"lük\",\"lığ\",\"liğ\",\"luğ\",\"lüğ\"],\n",
    "        \"Acquire\": [\"lan\", \"len\"],\n",
    "        \"Apply\":[\"la\",\"le\"],\n",
    "        \"Resemb\": [\"ımsı\", \"imsi\",\"umsu\",\"ümsü\",\"msı\",\"msi\",\"msu\",\"msü\"],\n",
    "        \"Rel\":[\"kı\",\"ki\",\"ku\",\"kü\"],\n",
    "        \"By\": [\"ca\", \"ce\"],\n",
    "        \"Cmp\":[\"ca\",\"ce\"],\n",
    "        \"Agt\":[\"cı\",\"ci\",\"cu\",\"cü\",\"yıcı\",\"yici\",\"yucu\",\"yücü\"],\n",
    "        ## Adjective Suffixes\n",
    "        \"Become\":[\"laş\",\"leş\"],\n",
    "        \"Ly\":[\"ca\",\"ce\"],\n",
    "        \"Quite\":[\"ca\",\"ce\"],\n",
    "        ## number-person agreement suffixes\n",
    "        \"A1sg\": [\"yım\", \"yim\",\"yum\",\"yüm\",\"m\",\"ım\",\"im\",\"um\",\"üm\"],\n",
    "        \"A2sg\":[\"sın\",\"sin\",\"sun\",\"sün\",\"n\",\"ın\",\"in\",\"un\",\"ün\"],\n",
    "        \"A3sg\":[\"\"],\n",
    "        \"A1pl\": [\"yız\",\"yiz\",\"k\",\"yuz\",\"yüz\",\"ız\",\"iz\",\"uz\",\"üz\"],\n",
    "        \"A2pl\":[\"sınız\",\"siniz\",\"sunuz\",\"sünüz\",\"nız\",\"niz\",\"nuz\",\"nüz\",\"ınız\",\"iniz\",\"unuz\",\"ünüz\"],\n",
    "        \"A3pl\": [\"lar\",\"ler\"],\n",
    "        ### Numeral suffixes\n",
    "        \"Grouping\": [\"şar\",\"şer\",\"ız\",\"iz\",\"ar\",\"er\"],\n",
    "        \"Ordinal\":[\"ıncı\",\"inci\",\"uncu\",\"üncü\",\"ncı\",\"nci\",\"ncu\",\"ncü\"],\n",
    "        \"Division\":[\"da\",\"de\",\"ta\",\"te\"],\n",
    "        ### verbal suffixes\n",
    "        \"Aor\":[\"ır,ir,ur,ür,ar,er,z\"],\n",
    "        \"Prog\":[\"ıyor\",\"iyor\",\"uyor\",\"üyor\"],\n",
    "        \"Prog2\":[\"makta\",\"mekte\"],\n",
    "        \"Fut\":[\"yacak\",\"yecek\",\"yacağ\",\"yeceğ\",\"acak\",\"ecek\",\"acağ\",\"eceğ\"],\n",
    "        \"Past\":[\"dı\",\"di\",\"du\",\"dü\",\"tı\",\"ti\",\"tu\",\"tü\"],\n",
    "        \"Evid\":[\"mış\",\"miş\",\"muş\",\"müş\"],\n",
    "        \"Neg\":[\"ma,me,m\"],\n",
    "        \"Cond\":[\"se\",\"sa\"],\n",
    "        \"Necess\":[\"meli\",\"malı\"],\n",
    "        \"Opt\":[\"ye\",\"ya\",\"e\",\"a\"],\n",
    "        \"Imp\":[\"sın\",\"sin\",\"sun\",\"sün\",\"yın\",\"yin\",\"yun\",\"yün\",\"ın\",\"in\",\"un\",\"ün\",\"sınlar\",\"sinler\",\"sünler\",\"sunlar\",\"yınız\",\"yiniz\",\"yunuz\",\"yünüz\",\"ınız\",\"iniz\",\"unuz\",\"ünüz\"],\n",
    "        ### copular suffixes\n",
    "        \"PastCop\":[\"ydı\",\"ydi\",\"ydu\",\"ydü\",\"dı\",\"di\",\"du\",\"dü\"],\n",
    "        \"EvidCop\":[\"ymış\",\"ymiş\",\"ymuş\",\"ymüş\",\"mış\",\"miş\",\"muş\",\"müş\"],\n",
    "        \"CondCop\":[\"ysa\",\"yse\",\"sa\",\"se\"],\n",
    "        \"While\":[\"yken\",\"ken\"],\n",
    "        \"Cop\":[\"dır\",\"dir\",\"dur\",\"dür\",\"tır\",\"tir\",\"tur\",\"tür\"],\n",
    "        ### Derivational Suffixes\n",
    "        \"Pass\":[\"ıl\",\"il\",\"ul\",\"ül\",\"ın\",\"in\",\"un\",\"ün\",\"n\",\"nıl\",\"nil\",\"nul\",\"nül\"],\n",
    "        \"Caus\":[\"dır\",\"dir\",\"dur\",\"dür\",\"tır\",\"tir\",\"tur\",\"tür\",\"t\"],\n",
    "        \"Recip\":[\"yış\",\"yiş\",\"yuş\",\"yüş\",\"ış\",\"iş\",\"uş\",\"üş\",\"ş\"],\n",
    "        \"Reflex\":[\"ın\",\"in\",\"un\",\"ün\"],\n",
    "        \"Abil\":[\"yabıl\",\"yabil\",\"yabul\",\"yabül\",\"yebil\",\"yebul\",\"yebül\",\"abıl\",\"ebil\",\"abul\",\"abül\",\"ebıl\",\"ebul\",\"ebül\"],\n",
    "        \"NegAbil\":[\"yama\",\"yeme\",\"ama\",\"eme\"],\n",
    "        \"Cont\":[\"yadur\",\"yedur\",\"adur\",\"edur\"],\n",
    "        \"EverSince\":[\"yagel\",\"yegel\",\"agel\",\"egel\"],\n",
    "        \"Cont2\":[\"yagör\",\"yegör\",\"agör\",\"egör\"],\n",
    "        \"Almost\":[\"yayaz\",\"yeyez\",\"yayez\",\"yeyaz\",\"ayaz\",\"eyez\",\"ayez\",\"eyaz\"],\n",
    "        \"Hastily\":[\"yıver\",\"yiver\",\"ıver\",\"iver\"],\n",
    "        \"Stay\":[\"yakal\",\"yekal\",\"akal\",\"ekal\"],\n",
    "        \"Inf\":[\"mak\",\"mek\"],\n",
    "        \"Inf2\":[\"ma\",\"me\"],\n",
    "        \"Inf3\":[\"yış\",\"yiş\",\"yuş\",\"yüş\",\"ış\",\"iş\",\"uş\",\"üş\"],\n",
    "        \"FutPart\":[\"yacak\",\"yecek\",\"yacağ\",\"yeceğ\",\"acak\",\"ecek\",\"acağ\",\"eceğ\"],\n",
    "        \"EvidPart\":[\"mış\",\"miş\",\"muş\",\"müş\"],\n",
    "        \"PastPart\":[\"dık\",\"dik\",\"duk\",\"dük\",\"tık\",\"tik\",\"tuk\",\"tük\",\"dığ\",\"diğ\",\"duğ\",\"düğ\",\"tığ\",\"tiğ\",\"tuğ\",\"tüğ\"],\n",
    "        \"PresPart\":[\"yan\",\"yen\",\"an\",\"en\"],\n",
    "        \"AorPart\":[\"ır\",\"ir\",\"ur\",\"ür\",\"ar\",\"er\",\"r\"],\n",
    "        \"AfterDoing\":[\"yıp\",\"yip\",\"yup\",\"yüp\",\"ıp\",\"ip\",\"up\",\"üp\"],\n",
    "        \"Agt\":[\"yıcı\",\"yici\",\"yucu\",\"yücü\",\"ıcı\",\"ici\",\"ucu\",\"ücü\"],\n",
    "        \"FeelLike\":[\"yası\",\"yesi\",\"ası\",\"esi\"],\n",
    "        \"SinceDoing\":[\"yalı\",\"yeli\",\"alı\",\"eli\"],\n",
    "        \"AsLongAs\":[\"dıkca\",\"dikce\",\"dıkça\",\"dikçe\",\"tıkca\",\"tikce\",\"tıkça\",\"tikçe\"],\n",
    "        \"JustAfter\":[\"yınca\",\"yince\",\"ınca\",\"ince\"],\n",
    "        \"ByDoing\":[\"yarak\",\"yerek\",\"arak\",\"erek\"],\n",
    "        \"WithoutDoing\":[\"madan\",\"meden\",\"adan\",\"eden\"],\n",
    "        \"WithoutDoing2\":[\"maksızın\",\"meksizin\",\"aksızın\",\"eksizin\"],\n",
    "        \"RatherThanInsteadOfDoing\":[\"maktansa\",\"mektense\",\"aktansa\",\"ektense\"],\n",
    "        \"UnableToDo\":[\"yamadan\",\"yemeden\",\"amadan\",\"emeden\"],\n",
    "        \"ActOf\":[\"maca\",\"mece\"],\n",
    "        \"NotState\":[\"mazlık\",\"mezlik\",\"mazlığ\",\"mezliğ\"],\n",
    "        \"AsIf\":[\"casına\",\"cesine\",\"çasına\",\"çesine\"],\n",
    "        \"Adamantly\":[\"yasıya\",\"yesiye\",\"asıya\",\"esiye\"],\n",
    "\n",
    "    }\n",
    "    ozel_durumlar = {\n",
    "        #\"ımız\": [\"ı\", \"mız\"],\n",
    "        #\"sinler\":[\"s\",\"i\",\"n\",\"ler\"],\n",
    "        #\"leri\":[\"ler\",\"i\"],\n",
    "        #\"sının\":[\"sı\",\"nın\"],\n",
    "        #\"elim\":[\"e\",\"lim\"],\n",
    "    }\n",
    "\n",
    "    sonuclar = []\n",
    "    en_uzun_ekler=[]\n",
    "\n",
    "    while input_str:\n",
    "        bulundu = False\n",
    "        ozelBulundu = False\n",
    "        en_uzun_ek = \"\"\n",
    "        ek_adi_list = []  \n",
    "\n",
    "        for ozel_ek, ayrilma_sekli in ozel_durumlar.items():\n",
    "            if input_str.startswith(ozel_ek):\n",
    "                for ayrilma in ayrilma_sekli:\n",
    "                    for ad, ekler in ek_dosyalar.items():\n",
    "                        if ayrilma in ekler and input_str.startswith(ayrilma):\n",
    "                            en_uzun_ek = ayrilma\n",
    "                            ek_adi = f\"({ad})\"\n",
    "                            ozelBulundu = True\n",
    "                            if ozelBulundu:\n",
    "                                input_str = input_str[len(en_uzun_ek):].strip()\n",
    "                                ek_adi_list.append(ek_adi)\n",
    "\n",
    "        if not ozelBulundu:\n",
    "            for ad, ekler in ek_dosyalar.items():\n",
    "                for ek in ekler:\n",
    "                    if input_str.startswith(ek) and len(ek) >= len(en_uzun_ek):\n",
    "                        en_uzun_ek = ek\n",
    "                        ek_adi = f\"({ad})\"\n",
    "                        bulundu = True\n",
    "                        \n",
    "                \n",
    "                \n",
    "            if bulundu:\n",
    "                input_str = input_str[len(en_uzun_ek):].strip()\n",
    "                for ad,ekler in ek_dosyalar.items():\n",
    "\n",
    "                    if en_uzun_ek in ekler:\n",
    "                        #print(en_uzun_ek,ekler,\"yakup\")\n",
    "                        ek_adi_list.append(f\"({ad})\")\n",
    "                    \n",
    "                #ek_adi_list.append(ek_adi)\n",
    "            else:\n",
    "                ek_adi_list.append(f\"({input_str})\")\n",
    "\n",
    "        if ek_adi_list:\n",
    "            sonuclar.append((f\"-{en_uzun_ek} \", ek_adi_list))\n",
    "\n",
    "    return sonuclar\n",
    "\n",
    "\n",
    "def find_and_process_suffixes_v2(base_word, dataset_path):\n",
    "    def find_suffixes(word, suffixes):\n",
    "        found_suffixes = [suffix for suffix in suffixes if word.endswith(suffix)]\n",
    "        return found_suffixes\n",
    "\n",
    "    def process_dataset(base_word, suffixes):\n",
    "        matches = []\n",
    "        suffix_categories = []  \n",
    "\n",
    "        with open(dataset_path, 'r', encoding='utf-8') as dataset_file:\n",
    "            for line in dataset_file:\n",
    "                parts = line.strip().split()\n",
    "\n",
    "                if len(parts[0]) >= 2:\n",
    "                    current_word, word_type = parts[0], parts[1:]\n",
    "                    \n",
    "                    if base_word == current_word and not suffixes:\n",
    "                        matches.append(current_word)\n",
    "                        suffix_categories.append(f\"({word_type})\")\n",
    "                        \n",
    "                        \n",
    "                    for suffix in suffixes:\n",
    "                        if base_word == current_word:\n",
    "                            matches.append(current_word)\n",
    "                            suffix_categories.append(f\"({word_type})\")\n",
    "\n",
    "                            sonuclar = parcala_ve_kontrol_et(suffix)\n",
    "                            for ek, dosya_adi in sonuclar:\n",
    "        \n",
    "                                matches.append(ek)\n",
    "                                suffix_categories.append(f\"({dosya_adi})\")\n",
    "                        else:\n",
    "                            if base_word.endswith('b'):\n",
    "                                base_word = base_word[:-1] + 'p'\n",
    "                            elif base_word.endswith('c'):\n",
    "                                base_word = base_word[:-1] + 'ç'\n",
    "                            elif base_word.endswith('d'):\n",
    "                                base_word = base_word[:-1] + 't'\n",
    "                            elif base_word.endswith('ğ'):\n",
    "                                base_word = base_word[:-1] + 'k'\n",
    "                            if base_word == current_word:\n",
    "                                matches.append(current_word)\n",
    "                                suffix_categories.append(f\"({word_type})\")\n",
    "\n",
    "                                sonuclar = parcala_ve_kontrol_et(suffix)\n",
    "                                for ek, dosya_adi in sonuclar:\n",
    "                                    matches.append(ek)\n",
    "                                    suffix_categories.append(f\"({dosya_adi})\")\n",
    "\n",
    "        return matches, suffix_categories\n",
    "\n",
    "    with open('C:/endings.txt', 'r', encoding='utf-8') as endings_file:\n",
    "        suffixes = [line.strip() for line in endings_file]\n",
    "\n",
    "    eksizEslesme, suffix_categories = process_dataset(base_word, [])\n",
    "    \n",
    "\n",
    "    all_suffixes = find_suffixes(base_word, suffixes)\n",
    "\n",
    "    if eksizEslesme:\n",
    "        return ''.join(eksizEslesme), ''.join(suffix_categories)\n",
    "    elif all_suffixes:\n",
    "        result_suffixes = []\n",
    "        for suffix in sorted(all_suffixes, key=len, reverse=True):\n",
    "            remaining_word = base_word[:-len(suffix)]\n",
    "            matches, suffix_categories = process_dataset(remaining_word, [suffix])\n",
    "\n",
    "            if matches:\n",
    "                result_suffixes.extend(matches)\n",
    "                result_suffixes.extend(suffix_categories)\n",
    "\n",
    "        return ''.join(result_suffixes)\n",
    "    else:\n",
    "        return f\"{base_word} kelimesi için eşleşme bulunamadı.\"\n",
    "\n",
    "        \n",
    "@app.route(\"/\", methods=[\"GET\", \"POST\"])\n",
    "def index():\n",
    "    results = []\n",
    "\n",
    "    if request.method == \"POST\":\n",
    "        user_input = request.form.get(\"user_input\")\n",
    "        user_input = user_input.lower()\n",
    "\n",
    "        sentences = sentence_splitter(user_input)\n",
    "\n",
    "        for sentence in sentences:\n",
    "            tokens = tokenizer(sentence)\n",
    "            processed_tokens = []\n",
    "\n",
    "            for token in tokens:\n",
    "                result = find_and_process_suffixes_v2(token, 'C:/dataset.txt')\n",
    "                processed_tokens.append(result)\n",
    "\n",
    "            results.append(processed_tokens)\n",
    "    return render_template(\"index.html\", results=results)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True, use_reloader=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36e2d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def en_uzun_ek_bul(dosya_yolu, kontrol_dosya_yolu, kullanici_input):\n",
    "    kontrol_sozlugu = {}\n",
    "    try:\n",
    "        with open(kontrol_dosya_yolu, 'r', encoding='utf-8') as kontrol_dosya:\n",
    "            for satir in kontrol_dosya:\n",
    "                satir = satir.strip()\n",
    "                if satir:\n",
    "                    kelime, kontrol_bilgisi = satir.split(maxsplit=1)\n",
    "                    kontrol_sozlugu[kelime] = kontrol_bilgisi\n",
    "    except ValueError:\n",
    "        print(\"Kontrol dosyası hatalı format içeriyor.\")\n",
    "\n",
    "    with open(dosya_yolu, 'r', encoding='utf-8') as dosya:\n",
    "        satirlar = [line.strip() for line in dosya.readlines()]\n",
    "\n",
    "    ekler = []\n",
    "    ekTaglari = []\n",
    "    \n",
    "    for satir in satirlar:\n",
    "        bolunmus_satir = satir.split('\\t')\n",
    "        if len(bolunmus_satir) == 2:\n",
    "            ekler.append(bolunmus_satir[0])\n",
    "            ekTaglari.append(bolunmus_satir[1])\n",
    "\n",
    "    en_uzun_ek = \"\"\n",
    "    for ek in ekler:\n",
    "        if kullanici_input.endswith(ek) and len(ek) > len(en_uzun_ek):\n",
    "            en_uzun_ek = ek\n",
    "\n",
    "    if en_uzun_ek:\n",
    "        indeks = ekler.index(en_uzun_ek)\n",
    "        ekTaglariUzun = ekTaglari[indeks].split('^DB')\n",
    "        ekTaglariUzun = [tag.strip() for tag in ekTaglariUzun if tag]\n",
    "        \n",
    "        kok = kullanici_input[:-len(en_uzun_ek)]\n",
    "\n",
    "        if kok in kontrol_sozlugu:\n",
    "            kontrol_bilgisi = kontrol_sozlugu[kok]\n",
    "            print(f\"{kok}+{en_uzun_ek} ( {'^DB'.join(ekTaglariUzun)} )\")\n",
    "        else:\n",
    "            print(f\"Uygun kök bulunamadı: {kok}\")\n",
    "    else:\n",
    "        print(\"Uygun ek bulunamadı.\")\n",
    "\n",
    "\n",
    "dosya_yolu = 'C:/tagliEkler.txt'\n",
    "kontrol_dosya_yolu = 'C:/dataset.txt'\n",
    "kullanici_input = input(\"Bir kelime girin: \")\n",
    "en_uzun_ek_bul(dosya_yolu, kontrol_dosya_yolu, kullanici_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aa02e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def sentence_splitter(text):\n",
    "    sentence_endings = r\"[.!?]\"\n",
    "    sentences = re.split(sentence_endings, text)\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    return sentences\n",
    "\n",
    "def tokenizer(text):\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "def parcala_ve_kontrol_et(input_str):\n",
    "    ek_dosyalar = {\n",
    "        \n",
    "        \"Opt\":[\"yalım\",\"yelim\",\"alım\",\"elim\"],\n",
    "        \"Opt+A2sg\":[\"yasın\",\"yesin\",\"asın\",\"esin\"],\n",
    "        \"Opt+A2pl\":[\"yasınız\",\"yesiniz\",\"asınız\",\"esiniz\"],\n",
    "        \"Opt+A1sg\":[\"yayım\",\"yeyim\",\"ayım\",\"eyim\"],\n",
    "        ## noun suffixes\n",
    "        \"Pl\": [\"lar\", \"ler\"],\n",
    "        ## case suffixes\n",
    "        \"Dat\": [\"ya\",\"ye\",\"e\",\"a\",\"na\",\"ne\"],\n",
    "        \"Loc\": [\"da\", \"de\", \"ta\", \"te\",\"nde\",\"nda\"],\n",
    "        \"Abl\": [\"dan\", \"den\", \"tan\", \"ten\",\"ndan\",\"nden\"],\n",
    "        \"Gen\": [\"nın\", \"nin\",\"nun\",\"nün\",\"ın\",\"in\",\"un\", \"ün\",\"yın\",\"yin\",\"yun\",\"yün\",\"ım\",\"im\",\"um\",\"üm\",\"m\"],\n",
    "        \"Acc\": [\"yı\", \"yi\", \"yu\", \"yü\", \"ı\", \"i\", \"u\",\"ü\",\"nı\",\"ni\",\"nu\",\"nü\"],\n",
    "        \"Inst\": [\"yla\", \"yle\",\"la\",\"le\"],\n",
    "        \"P1sg\": [\"ım\",\"im\",\"um\",\"üm\",\"m\",\"yum\"],\n",
    "        \"P2sg\": [\"ın\",\"in\",\"un\",\"ün\",\"n\",\"yun\"],\n",
    "        \"P3sg\": [\"sı\", \"si\", \"su\", \"sü\",\"ı\",\"i\",\"u\",\"ü\",\"yu\"],\n",
    "        \"P1pl\": [\"ımız\", \"imiz\", \"umuz\", \"ümüz\",\"mız\",\"miz\",\"muz\",\"müz\",\"yumuz\"],\n",
    "        \"P2pl\": [\"ınız\",\"iniz\",\"unuz\",\"ünüz\",\"nız\",\"niz\",\"nuz\",\"nüz\",\"yunuz\"],\n",
    "        \"P3pl\": [\"ı\", \"i\",\"u\",\"ü\",\"ları\",\"leri\"],\n",
    "        \"Dim\": [\"cık\", \"cik\",\"cuk\",\"cük\",\"çık\",\"çik\",\"çuk\",\"çük\",\"cığ\",\"ciğ\",\"cuğ\",\"cüğ\",\"çığ\",\"çiğ\",\"çuğ\",\"çüğ\"],\n",
    "        \"Dim2\": [\"cağız\", \"ceğiz\"],\n",
    "        \"With\":[\"lı\",\"li\",\"lu\",\"lü\"],\n",
    "        \"Without\": [\"sız\", \"siz\",\"suz\",\"süz\"],\n",
    "        \"Since\":[\"dır\",\"dir\",\"dur\",\"dür\",\"tır\",\"tir\",\"tur\",\"tür\"],\n",
    "        \"Related\": [\"sal\", \"sel\"],\n",
    "        \"Ness\":[\"lık\",\"lik\",\"luk\",\"lük\",\"lığ\",\"liğ\",\"luğ\",\"lüğ\"],\n",
    "        \"Acquire\": [\"lan\", \"len\"],\n",
    "        \"Apply\":[\"la\",\"le\"],\n",
    "        \"Resemb\": [\"ımsı\", \"imsi\",\"umsu\",\"ümsü\",\"msı\",\"msi\",\"msu\",\"msü\"],\n",
    "        \"Rel\":[\"kı\",\"ki\",\"ku\",\"kü\"],\n",
    "        \"By\": [\"ca\", \"ce\"],\n",
    "        \"Cmp\":[\"ca\",\"ce\"],\n",
    "        \"Agt\":[\"cı\",\"ci\",\"cu\",\"cü\",\"yıcı\",\"yici\",\"yucu\",\"yücü\"],\n",
    "        ## Adjective Suffixes\n",
    "        \"Become\":[\"laş\",\"leş\"],\n",
    "        \"Ly\":[\"ca\",\"ce\"],\n",
    "        \"Quite\":[\"ca\",\"ce\"],\n",
    "        ## number-person agreement suffixes\n",
    "        \"A1sg\": [\"yım\", \"yim\",\"yum\",\"yüm\",\"m\",\"ım\",\"im\",\"um\",\"üm\"],\n",
    "        \"A2sg\":[\"sın\",\"sin\",\"sun\",\"sün\",\"n\",\"ın\",\"in\",\"un\",\"ün\"],\n",
    "        \"A3sg\":[\"\"],\n",
    "        \"A1pl\": [\"yız\",\"yiz\",\"k\",\"yuz\",\"yüz\",\"ız\",\"iz\",\"uz\",\"üz\"],\n",
    "        \"A2pl\":[\"sınız\",\"siniz\",\"sunuz\",\"sünüz\",\"nız\",\"niz\",\"nuz\",\"nüz\",\"ınız\",\"iniz\",\"unuz\",\"ünüz\"],\n",
    "        \"A3pl\": [\"lar\",\"ler\"],\n",
    "        ### Numeral suffixes\n",
    "        \"Grouping\": [\"şar\",\"şer\",\"ız\",\"iz\",\"ar\",\"er\"],\n",
    "        \"Ordinal\":[\"ıncı\",\"inci\",\"uncu\",\"üncü\",\"ncı\",\"nci\",\"ncu\",\"ncü\"],\n",
    "        \"Division\":[\"da\",\"de\",\"ta\",\"te\"],\n",
    "        ### verbal suffixes\n",
    "        \"Aor\":[\"ır,ir,ur,ür,ar,er,z\"],\n",
    "        \"Prog\":[\"ıyor\",\"iyor\",\"uyor\",\"üyor\"],\n",
    "        \"Prog2\":[\"makta\",\"mekte\"],\n",
    "        \"Fut\":[\"yacak\",\"yecek\",\"yacağ\",\"yeceğ\",\"acak\",\"ecek\",\"acağ\",\"eceğ\"],\n",
    "        \"Past\":[\"dı\",\"di\",\"du\",\"dü\",\"tı\",\"ti\",\"tu\",\"tü\"],\n",
    "        \"Evid\":[\"mış\",\"miş\",\"muş\",\"müş\"],\n",
    "        \"Neg\":[\"ma,me,m\"],\n",
    "        \"Cond\":[\"se\",\"sa\"],\n",
    "        \"Necess\":[\"meli\",\"malı\"],\n",
    "        \"Opt\":[\"ye\",\"ya\",\"e\",\"a\"],\n",
    "        \"Imp\":[\"sın\",\"sin\",\"sun\",\"sün\",\"yın\",\"yin\",\"yun\",\"yün\",\"ın\",\"in\",\"un\",\"ün\",\"sınlar\",\"sinler\",\"sünler\",\"sunlar\",\"yınız\",\"yiniz\",\"yunuz\",\"yünüz\",\"ınız\",\"iniz\",\"unuz\",\"ünüz\"],\n",
    "        ### copular suffixes\n",
    "        \"PastCop\":[\"ydı\",\"ydi\",\"ydu\",\"ydü\",\"dı\",\"di\",\"du\",\"dü\"],\n",
    "        \"EvidCop\":[\"ymış\",\"ymiş\",\"ymuş\",\"ymüş\",\"mış\",\"miş\",\"muş\",\"müş\"],\n",
    "        \"CondCop\":[\"ysa\",\"yse\",\"sa\",\"se\"],\n",
    "        \"While\":[\"yken\",\"ken\"],\n",
    "        \"Cop\":[\"dır\",\"dir\",\"dur\",\"dür\",\"tır\",\"tir\",\"tur\",\"tür\"],\n",
    "        ### Derivational Suffixes\n",
    "        \"Pass\":[\"ıl\",\"il\",\"ul\",\"ül\",\"ın\",\"in\",\"un\",\"ün\",\"n\",\"nıl\",\"nil\",\"nul\",\"nül\"],\n",
    "        \"Caus\":[\"dır\",\"dir\",\"dur\",\"dür\",\"tır\",\"tir\",\"tur\",\"tür\",\"t\"],\n",
    "        \"Recip\":[\"yış\",\"yiş\",\"yuş\",\"yüş\",\"ış\",\"iş\",\"uş\",\"üş\",\"ş\"],\n",
    "        \"Reflex\":[\"ın\",\"in\",\"un\",\"ün\"],\n",
    "        \"Abil\":[\"yabıl\",\"yabil\",\"yabul\",\"yabül\",\"yebil\",\"yebul\",\"yebül\",\"abıl\",\"ebil\",\"abul\",\"abül\",\"ebıl\",\"ebul\",\"ebül\"],\n",
    "        \"NegAbil\":[\"yama\",\"yeme\",\"ama\",\"eme\"],\n",
    "        \"Cont\":[\"yadur\",\"yedur\",\"adur\",\"edur\"],\n",
    "        \"EverSince\":[\"yagel\",\"yegel\",\"agel\",\"egel\"],\n",
    "        \"Cont2\":[\"yagör\",\"yegör\",\"agör\",\"egör\"],\n",
    "        \"Almost\":[\"yayaz\",\"yeyez\",\"yayez\",\"yeyaz\",\"ayaz\",\"eyez\",\"ayez\",\"eyaz\"],\n",
    "        \"Hastily\":[\"yıver\",\"yiver\",\"ıver\",\"iver\"],\n",
    "        \"Stay\":[\"yakal\",\"yekal\",\"akal\",\"ekal\"],\n",
    "        \"Inf\":[\"mak\",\"mek\"],\n",
    "        \"Inf2\":[\"ma\",\"me\"],\n",
    "        \"Inf3\":[\"yış\",\"yiş\",\"yuş\",\"yüş\",\"ış\",\"iş\",\"uş\",\"üş\"],\n",
    "        \"FutPart\":[\"yacak\",\"yecek\",\"yacağ\",\"yeceğ\",\"acak\",\"ecek\",\"acağ\",\"eceğ\"],\n",
    "        \"EvidPart\":[\"mış\",\"miş\",\"muş\",\"müş\"],\n",
    "        \"PastPart\":[\"dık\",\"dik\",\"duk\",\"dük\",\"tık\",\"tik\",\"tuk\",\"tük\",\"dığ\",\"diğ\",\"duğ\",\"düğ\",\"tığ\",\"tiğ\",\"tuğ\",\"tüğ\"],\n",
    "        \"PresPart\":[\"yan\",\"yen\",\"an\",\"en\"],\n",
    "        \"AorPart\":[\"ır\",\"ir\",\"ur\",\"ür\",\"ar\",\"er\",\"r\"],\n",
    "        \"AfterDoing\":[\"yıp\",\"yip\",\"yup\",\"yüp\",\"ıp\",\"ip\",\"up\",\"üp\"],\n",
    "        \"Agt\":[\"yıcı\",\"yici\",\"yucu\",\"yücü\",\"ıcı\",\"ici\",\"ucu\",\"ücü\"],\n",
    "        \"FeelLike\":[\"yası\",\"yesi\",\"ası\",\"esi\"],\n",
    "        \"SinceDoing\":[\"yalı\",\"yeli\",\"alı\",\"eli\"],\n",
    "        \"AsLongAs\":[\"dıkca\",\"dikce\",\"dıkça\",\"dikçe\",\"tıkca\",\"tikce\",\"tıkça\",\"tikçe\"],\n",
    "        \"JustAfter\":[\"yınca\",\"yince\",\"ınca\",\"ince\"],\n",
    "        \"ByDoing\":[\"yarak\",\"yerek\",\"arak\",\"erek\"],\n",
    "        \"WithoutDoing\":[\"madan\",\"meden\",\"adan\",\"eden\"],\n",
    "        \"WithoutDoing2\":[\"maksızın\",\"meksizin\",\"aksızın\",\"eksizin\"],\n",
    "        \"RatherThanInsteadOfDoing\":[\"maktansa\",\"mektense\",\"aktansa\",\"ektense\"],\n",
    "        \"UnableToDo\":[\"yamadan\",\"yemeden\",\"amadan\",\"emeden\"],\n",
    "        \"ActOf\":[\"maca\",\"mece\"],\n",
    "        \"NotState\":[\"mazlık\",\"mezlik\",\"mazlığ\",\"mezliğ\"],\n",
    "        \"AsIf\":[\"casına\",\"cesine\",\"çasına\",\"çesine\"],\n",
    "        \"Adamantly\":[\"yasıya\",\"yesiye\",\"asıya\",\"esiye\"],\n",
    "\n",
    "    }\n",
    "    ozel_durumlar = {\n",
    "        #\"ımız\": [\"ı\", \"mız\"],\n",
    "        #\"sinler\":[\"s\",\"i\",\"n\",\"ler\"],\n",
    "        #\"leri\":[\"ler\",\"i\"],\n",
    "        #\"sının\":[\"sı\",\"nın\"],\n",
    "        #\"elim\":[\"e\",\"lim\"],\n",
    "    }\n",
    "\n",
    "    sonuclar = []\n",
    "    en_uzun_ekler=[]\n",
    "\n",
    "    while input_str:\n",
    "        bulundu = False\n",
    "        ozelBulundu = False\n",
    "        en_uzun_ek = \"\"\n",
    "        ek_adi_list = []  \n",
    "\n",
    "        for ozel_ek, ayrilma_sekli in ozel_durumlar.items():\n",
    "            if input_str.startswith(ozel_ek):\n",
    "                for ayrilma in ayrilma_sekli:\n",
    "                    for ad, ekler in ek_dosyalar.items():\n",
    "                        if ayrilma in ekler and input_str.startswith(ayrilma):\n",
    "                            en_uzun_ek = ayrilma\n",
    "                            ek_adi = f\"({ad})\"\n",
    "                            ozelBulundu = True\n",
    "                            if ozelBulundu:\n",
    "                                input_str = input_str[len(en_uzun_ek):].strip()\n",
    "                                ek_adi_list.append(ek_adi)\n",
    "\n",
    "        if not ozelBulundu:\n",
    "            for ad, ekler in ek_dosyalar.items():\n",
    "                for ek in ekler:\n",
    "                    if input_str.startswith(ek) and len(ek) >= len(en_uzun_ek):\n",
    "                        en_uzun_ek = ek\n",
    "                        ek_adi = f\"({ad})\"\n",
    "                        bulundu = True\n",
    "                        \n",
    "                \n",
    "                \n",
    "            if bulundu:\n",
    "                input_str = input_str[len(en_uzun_ek):].strip()\n",
    "                for ad,ekler in ek_dosyalar.items():\n",
    "\n",
    "                    if en_uzun_ek in ekler:\n",
    "                        #print(en_uzun_ek,ekler,\"yakup\")\n",
    "                        ek_adi_list.append(f\"({ad})\")\n",
    "                    \n",
    "                #ek_adi_list.append(ek_adi)\n",
    "            else:\n",
    "                ek_adi_list.append(f\"({input_str})\")\n",
    "\n",
    "        if ek_adi_list:\n",
    "            sonuclar.append((f\"-{en_uzun_ek} \", ek_adi_list))\n",
    "\n",
    "    return sonuclar\n",
    "\n",
    "\n",
    "def find_and_process_suffixes_v2(base_word, dataset_path):\n",
    "    def find_suffixes(word, suffixes):\n",
    "        found_suffixes = [suffix for suffix in suffixes if word.endswith(suffix)]\n",
    "        return found_suffixes\n",
    "\n",
    "    def process_dataset(base_word, suffixes):\n",
    "        matches = []\n",
    "        suffix_categories = []  \n",
    "\n",
    "        with open(dataset_path, 'r', encoding='utf-8') as dataset_file:\n",
    "            for line in dataset_file:\n",
    "                parts = line.strip().split()\n",
    "\n",
    "                if len(parts[0]) >= 2:\n",
    "                    current_word, word_type = parts[0], parts[1:]\n",
    "                    \n",
    "                    if base_word == current_word and not suffixes:\n",
    "                        matches.append(current_word)\n",
    "                        suffix_categories.append(f\"({word_type})\")\n",
    "                        \n",
    "                        \n",
    "                    for suffix in suffixes:\n",
    "                        if base_word == current_word:\n",
    "                            matches.append(current_word)\n",
    "                            suffix_categories.append(f\"({word_type})\")\n",
    "\n",
    "                            sonuclar = parcala_ve_kontrol_et(suffix)\n",
    "                            for ek, dosya_adi in sonuclar:\n",
    "        \n",
    "                                matches.append(ek)\n",
    "                                suffix_categories.append(f\"({dosya_adi})\")\n",
    "                        else:\n",
    "                            if base_word.endswith('b'):\n",
    "                                base_word = base_word[:-1] + 'p'\n",
    "                            elif base_word.endswith('c'):\n",
    "                                base_word = base_word[:-1] + 'ç'\n",
    "                            elif base_word.endswith('d'):\n",
    "                                base_word = base_word[:-1] + 't'\n",
    "                            elif base_word.endswith('ğ'):\n",
    "                                base_word = base_word[:-1] + 'k'\n",
    "                            if base_word == current_word:\n",
    "                                matches.append(current_word)\n",
    "                                suffix_categories.append(f\"({word_type})\")\n",
    "\n",
    "                                sonuclar = parcala_ve_kontrol_et(suffix)\n",
    "                                for ek, dosya_adi in sonuclar:\n",
    "                                    matches.append(ek)\n",
    "                                    suffix_categories.append(f\"({dosya_adi})\")\n",
    "\n",
    "        return matches, suffix_categories\n",
    "\n",
    "    with open('C:/endings.txt', 'r', encoding='utf-8') as endings_file:\n",
    "        suffixes = [line.strip() for line in endings_file]\n",
    "\n",
    "    eksizEslesme, suffix_categories = process_dataset(base_word, [])\n",
    "    \n",
    "\n",
    "    if eksizEslesme:\n",
    "        print(''.join(eksizEslesme), ''.join(suffix_categories))\n",
    "        return\n",
    "\n",
    "    all_suffixes = find_suffixes(base_word, suffixes)\n",
    "   \n",
    "    \n",
    "    if all_suffixes:\n",
    "        result_suffixes = []\n",
    "        for suffix in sorted(all_suffixes, key=len, reverse=True):\n",
    "            remaining_word = base_word[:-len(suffix)]\n",
    "            matches, suffix_categories = process_dataset(remaining_word, [suffix])\n",
    "            \n",
    "\n",
    "            if matches:\n",
    "                result_suffixes.extend(matches)\n",
    "                result_suffixes.extend(suffix_categories)\n",
    "                \n",
    "        \n",
    "        print(''.join(result_suffixes))\n",
    "\n",
    "    else:\n",
    "        print(f\"{base_word} kelimesi için eşleşme bulunamadı.\")\n",
    "\n",
    "\n",
    "\n",
    "text = input(\"Türkçe metin giriniz:\")\n",
    "text = text.lower()\n",
    "\n",
    "sentences = sentence_splitter(text)\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokens = tokenizer(sentence)\n",
    "    processed_tokens = []\n",
    "\n",
    "    for token in tokens:\n",
    "        find_and_process_suffixes_v2(token, 'C:/dataset.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aae45e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, render_template, request\n",
    "import re\n",
    "%run '/FiilEkAyrimi.ipynb'\n",
    "#from FiilEkAyrimi import deger\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "\n",
    "\n",
    "def sentence_splitter(text):\n",
    "    sentence_endings = r\"[.!?]\"\n",
    "    sentences = re.split(sentence_endings, text)\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    return sentences\n",
    "\n",
    "def tokenizer(text):\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "def parcala_ve_kontrol_et(input_str):\n",
    "    ek_dosyalar = {\n",
    "        \n",
    "        \"Opt\":[\"yalım\",\"yelim\",\"alım\",\"elim\"],\n",
    "        \"Opt+A2sg\":[\"yasın\",\"yesin\",\"asın\",\"esin\"],\n",
    "        \"Opt+A2pl\":[\"yasınız\",\"yesiniz\",\"asınız\",\"esiniz\"],\n",
    "        \"Opt+A1sg\":[\"yayım\",\"yeyim\",\"ayım\",\"eyim\"],\n",
    "        ## noun suffixes\n",
    "        \"Pl\": [\"lar\", \"ler\"],\n",
    "        ## case suffixes\n",
    "        \"Dat\": [\"ya\",\"ye\",\"e\",\"a\",\"na\",\"ne\"],\n",
    "        \"Loc\": [\"da\", \"de\", \"ta\", \"te\",\"nde\",\"nda\"],\n",
    "        \"Abl\": [\"dan\", \"den\", \"tan\", \"ten\",\"ndan\",\"nden\"],\n",
    "        \"Gen\": [\"nın\", \"nin\",\"nun\",\"nün\",\"ın\",\"in\",\"un\", \"ün\",\"yın\",\"yin\",\"yun\",\"yün\",\"ım\",\"im\",\"um\",\"üm\",\"m\"],\n",
    "        \"Acc\": [\"yı\", \"yi\", \"yu\", \"yü\", \"ı\", \"i\", \"u\",\"ü\",\"nı\",\"ni\",\"nu\",\"nü\"],\n",
    "        \"Inst\": [\"yla\", \"yle\",\"la\",\"le\"],\n",
    "        \"P1sg\": [\"ım\",\"im\",\"um\",\"üm\",\"m\",\"yum\"],\n",
    "        \"P2sg\": [\"ın\",\"in\",\"un\",\"ün\",\"n\",\"yun\"],\n",
    "        \"P3sg\": [\"sı\", \"si\", \"su\", \"sü\",\"ı\",\"i\",\"u\",\"ü\",\"yu\"],\n",
    "        \"P1pl\": [\"ımız\", \"imiz\", \"umuz\", \"ümüz\",\"mız\",\"miz\",\"muz\",\"müz\",\"yumuz\"],\n",
    "        \"P2pl\": [\"ınız\",\"iniz\",\"unuz\",\"ünüz\",\"nız\",\"niz\",\"nuz\",\"nüz\",\"yunuz\"],\n",
    "        \"P3pl\": [\"ı\", \"i\",\"u\",\"ü\",\"ları\",\"leri\"],\n",
    "        \"Dim\": [\"cık\", \"cik\",\"cuk\",\"cük\",\"çık\",\"çik\",\"çuk\",\"çük\",\"cığ\",\"ciğ\",\"cuğ\",\"cüğ\",\"çığ\",\"çiğ\",\"çuğ\",\"çüğ\"],\n",
    "        \"Dim2\": [\"cağız\", \"ceğiz\"],\n",
    "        \"With\":[\"lı\",\"li\",\"lu\",\"lü\"],\n",
    "        \"Without\": [\"sız\", \"siz\",\"suz\",\"süz\"],\n",
    "        \"Since\":[\"dır\",\"dir\",\"dur\",\"dür\",\"tır\",\"tir\",\"tur\",\"tür\"],\n",
    "        \"Related\": [\"sal\", \"sel\"],\n",
    "        \"Ness\":[\"lık\",\"lik\",\"luk\",\"lük\",\"lığ\",\"liğ\",\"luğ\",\"lüğ\"],\n",
    "        \"Acquire\": [\"lan\", \"len\"],\n",
    "        \"Apply\":[\"la\",\"le\"],\n",
    "        \"Resemb\": [\"ımsı\", \"imsi\",\"umsu\",\"ümsü\",\"msı\",\"msi\",\"msu\",\"msü\"],\n",
    "        \"Rel\":[\"kı\",\"ki\",\"ku\",\"kü\"],\n",
    "        \"By\": [\"ca\", \"ce\"],\n",
    "        \"Cmp\":[\"ca\",\"ce\"],\n",
    "        \"Agt\":[\"cı\",\"ci\",\"cu\",\"cü\",\"yıcı\",\"yici\",\"yucu\",\"yücü\"],\n",
    "        ## Adjective Suffixes\n",
    "        \"Become\":[\"laş\",\"leş\"],\n",
    "        \"Ly\":[\"ca\",\"ce\"],\n",
    "        \"Quite\":[\"ca\",\"ce\"],\n",
    "        ## number-person agreement suffixes\n",
    "        \"A1sg\": [\"yım\", \"yim\",\"yum\",\"yüm\",\"m\",\"ım\",\"im\",\"um\",\"üm\"],\n",
    "        \"A2sg\":[\"sın\",\"sin\",\"sun\",\"sün\",\"n\",\"ın\",\"in\",\"un\",\"ün\"],\n",
    "        \"A3sg\":[\"\"],\n",
    "        \"A1pl\": [\"yız\",\"yiz\",\"k\",\"yuz\",\"yüz\",\"ız\",\"iz\",\"uz\",\"üz\"],\n",
    "        \"A2pl\":[\"sınız\",\"siniz\",\"sunuz\",\"sünüz\",\"nız\",\"niz\",\"nuz\",\"nüz\",\"ınız\",\"iniz\",\"unuz\",\"ünüz\"],\n",
    "        \"A3pl\": [\"lar\",\"ler\"],\n",
    "        ### Numeral suffixes\n",
    "        \"Grouping\": [\"şar\",\"şer\",\"ız\",\"iz\",\"ar\",\"er\"],\n",
    "        \"Ordinal\":[\"ıncı\",\"inci\",\"uncu\",\"üncü\",\"ncı\",\"nci\",\"ncu\",\"ncü\"],\n",
    "        \"Division\":[\"da\",\"de\",\"ta\",\"te\"],\n",
    "        ### verbal suffixes\n",
    "        \"Aor\":[\"ır,ir,ur,ür,ar,er,z\"],\n",
    "        \"Prog\":[\"ıyor\",\"iyor\",\"uyor\",\"üyor\"],\n",
    "        \"Prog2\":[\"makta\",\"mekte\"],\n",
    "        \"Fut\":[\"yacak\",\"yecek\",\"yacağ\",\"yeceğ\",\"acak\",\"ecek\",\"acağ\",\"eceğ\"],\n",
    "        \"Past\":[\"dı\",\"di\",\"du\",\"dü\",\"tı\",\"ti\",\"tu\",\"tü\"],\n",
    "        \"Evid\":[\"mış\",\"miş\",\"muş\",\"müş\"],\n",
    "        \"Neg\":[\"ma,me,m\"],\n",
    "        \"Cond\":[\"se\",\"sa\"],\n",
    "        \"Necess\":[\"meli\",\"malı\"],\n",
    "        \"Opt\":[\"ye\",\"ya\",\"e\",\"a\"],\n",
    "        \"Imp\":[\"sın\",\"sin\",\"sun\",\"sün\",\"yın\",\"yin\",\"yun\",\"yün\",\"ın\",\"in\",\"un\",\"ün\",\"sınlar\",\"sinler\",\"sünler\",\"sunlar\",\"yınız\",\"yiniz\",\"yunuz\",\"yünüz\",\"ınız\",\"iniz\",\"unuz\",\"ünüz\"],\n",
    "        ### copular suffixes\n",
    "        \"PastCop\":[\"ydı\",\"ydi\",\"ydu\",\"ydü\",\"dı\",\"di\",\"du\",\"dü\"],\n",
    "        \"EvidCop\":[\"ymış\",\"ymiş\",\"ymuş\",\"ymüş\",\"mış\",\"miş\",\"muş\",\"müş\"],\n",
    "        \"CondCop\":[\"ysa\",\"yse\",\"sa\",\"se\"],\n",
    "        \"While\":[\"yken\",\"ken\"],\n",
    "        \"Cop\":[\"dır\",\"dir\",\"dur\",\"dür\",\"tır\",\"tir\",\"tur\",\"tür\"],\n",
    "        ### Derivational Suffixes\n",
    "        \"Pass\":[\"ıl\",\"il\",\"ul\",\"ül\",\"ın\",\"in\",\"un\",\"ün\",\"n\",\"nıl\",\"nil\",\"nul\",\"nül\"],\n",
    "        \"Caus\":[\"dır\",\"dir\",\"dur\",\"dür\",\"tır\",\"tir\",\"tur\",\"tür\",\"t\"],\n",
    "        \"Recip\":[\"yış\",\"yiş\",\"yuş\",\"yüş\",\"ış\",\"iş\",\"uş\",\"üş\",\"ş\"],\n",
    "        \"Reflex\":[\"ın\",\"in\",\"un\",\"ün\"],\n",
    "        \"Abil\":[\"yabıl\",\"yabil\",\"yabul\",\"yabül\",\"yebil\",\"yebul\",\"yebül\",\"abıl\",\"ebil\",\"abul\",\"abül\",\"ebıl\",\"ebul\",\"ebül\"],\n",
    "        \"NegAbil\":[\"yama\",\"yeme\",\"ama\",\"eme\"],\n",
    "        \"Cont\":[\"yadur\",\"yedur\",\"adur\",\"edur\"],\n",
    "        \"EverSince\":[\"yagel\",\"yegel\",\"agel\",\"egel\"],\n",
    "        \"Cont2\":[\"yagör\",\"yegör\",\"agör\",\"egör\"],\n",
    "        \"Almost\":[\"yayaz\",\"yeyez\",\"yayez\",\"yeyaz\",\"ayaz\",\"eyez\",\"ayez\",\"eyaz\"],\n",
    "        \"Hastily\":[\"yıver\",\"yiver\",\"ıver\",\"iver\"],\n",
    "        \"Stay\":[\"yakal\",\"yekal\",\"akal\",\"ekal\"],\n",
    "        \"Inf\":[\"mak\",\"mek\"],\n",
    "        \"Inf2\":[\"ma\",\"me\"],\n",
    "        \"Inf3\":[\"yış\",\"yiş\",\"yuş\",\"yüş\",\"ış\",\"iş\",\"uş\",\"üş\"],\n",
    "        \"FutPart\":[\"yacak\",\"yecek\",\"yacağ\",\"yeceğ\",\"acak\",\"ecek\",\"acağ\",\"eceğ\"],\n",
    "        \"EvidPart\":[\"mış\",\"miş\",\"muş\",\"müş\"],\n",
    "        \"PastPart\":[\"dık\",\"dik\",\"duk\",\"dük\",\"tık\",\"tik\",\"tuk\",\"tük\",\"dığ\",\"diğ\",\"duğ\",\"düğ\",\"tığ\",\"tiğ\",\"tuğ\",\"tüğ\"],\n",
    "        \"PresPart\":[\"yan\",\"yen\",\"an\",\"en\"],\n",
    "        \"AorPart\":[\"ır\",\"ir\",\"ur\",\"ür\",\"ar\",\"er\",\"r\"],\n",
    "        \"AfterDoing\":[\"yıp\",\"yip\",\"yup\",\"yüp\",\"ıp\",\"ip\",\"up\",\"üp\"],\n",
    "        \"Agt\":[\"yıcı\",\"yici\",\"yucu\",\"yücü\",\"ıcı\",\"ici\",\"ucu\",\"ücü\"],\n",
    "        \"FeelLike\":[\"yası\",\"yesi\",\"ası\",\"esi\"],\n",
    "        \"SinceDoing\":[\"yalı\",\"yeli\",\"alı\",\"eli\"],\n",
    "        \"AsLongAs\":[\"dıkca\",\"dikce\",\"dıkça\",\"dikçe\",\"tıkca\",\"tikce\",\"tıkça\",\"tikçe\"],\n",
    "        \"JustAfter\":[\"yınca\",\"yince\",\"ınca\",\"ince\"],\n",
    "        \"ByDoing\":[\"yarak\",\"yerek\",\"arak\",\"erek\"],\n",
    "        \"WithoutDoing\":[\"madan\",\"meden\",\"adan\",\"eden\"],\n",
    "        \"WithoutDoing2\":[\"maksızın\",\"meksizin\",\"aksızın\",\"eksizin\"],\n",
    "        \"RatherThanInsteadOfDoing\":[\"maktansa\",\"mektense\",\"aktansa\",\"ektense\"],\n",
    "        \"UnableToDo\":[\"yamadan\",\"yemeden\",\"amadan\",\"emeden\"],\n",
    "        \"ActOf\":[\"maca\",\"mece\"],\n",
    "        \"NotState\":[\"mazlık\",\"mezlik\",\"mazlığ\",\"mezliğ\"],\n",
    "        \"AsIf\":[\"casına\",\"cesine\",\"çasına\",\"çesine\"],\n",
    "        \"Adamantly\":[\"yasıya\",\"yesiye\",\"asıya\",\"esiye\"],\n",
    "\n",
    "    }\n",
    "    ozel_durumlar = {\n",
    "        #\"ımız\": [\"ım\", \"ız\"],\n",
    "        #\"sinler\":[\"s\",\"i\",\"n\",\"ler\"],\n",
    "        #\"leri\":[\"ler\",\"i\"],\n",
    "        #\"sının\":[\"sı\",\"nın\"],\n",
    "        #\"elim\":[\"e\",\"lim\"],\n",
    "\n",
    "    }\n",
    "\n",
    "    sonuclar = []\n",
    "    en_uzun_ekler=[]\n",
    "\n",
    "    while input_str:\n",
    "        bulundu = False\n",
    "        ozelBulundu = False\n",
    "        en_uzun_ek = \"\"\n",
    "        ek_adi_list = []  \n",
    "\n",
    "        for ozel_ek, ayrilma_sekli in ozel_durumlar.items():\n",
    "            if input_str.startswith(ozel_ek):\n",
    "                for ayrilma in ayrilma_sekli:\n",
    "                    for ad, ekler in ek_dosyalar.items():\n",
    "                        if ayrilma in ekler and input_str.startswith(ayrilma):\n",
    "                            en_uzun_ek = ayrilma\n",
    "                            ek_adi = f\"({ad})\"\n",
    "                            ozelBulundu = True\n",
    "                            if ozelBulundu:\n",
    "                                input_str = input_str[len(en_uzun_ek):].strip()\n",
    "                                ek_adi_list.append(ek_adi)\n",
    "\n",
    "        if not ozelBulundu:\n",
    "            for ad, ekler in ek_dosyalar.items():\n",
    "                for ek in ekler:\n",
    "                    if input_str.startswith(ek) and len(ek) >= len(en_uzun_ek):\n",
    "                        en_uzun_ek = ek\n",
    "                        ek_adi = f\"({ad})\"\n",
    "                        bulundu = True\n",
    "                        \n",
    "                \n",
    "                \n",
    "            if bulundu:\n",
    "                input_str = input_str[len(en_uzun_ek):].strip()\n",
    "                for ad,ekler in ek_dosyalar.items():\n",
    "\n",
    "                    if en_uzun_ek in ekler:\n",
    "                        #print(en_uzun_ek,ekler,\"yakup\")\n",
    "                        ek_adi_list.append(f\"({ad})\")\n",
    "                    \n",
    "                #ek_adi_list.append(ek_adi)\n",
    "            else:\n",
    "                ek_adi_list.append(f\"({input_str})\")\n",
    "\n",
    "        if ek_adi_list:\n",
    "            sonuclar.append((f\"-{en_uzun_ek} \", ek_adi_list))\n",
    "\n",
    "    return sonuclar\n",
    "\n",
    "\n",
    "\n",
    "def find_and_process_suffixes_v2(base_word, dataset_path):\n",
    "    def find_suffixes(word, suffixes):\n",
    "        found_suffixes = [suffix for suffix in suffixes if word.endswith(suffix)]\n",
    "        return found_suffixes\n",
    "\n",
    "    def process_dataset(base_word, suffixes):\n",
    "        matches = []\n",
    "        suffix_categories = []  \n",
    "\n",
    "        with open(dataset_path, 'r', encoding='utf-8') as dataset_file:\n",
    "            for line in dataset_file:\n",
    "                parts = line.strip().split()\n",
    "\n",
    "                if len(parts[0]) >= 2:\n",
    "                    current_word, word_type = parts[0], parts[1:]\n",
    "                    \n",
    "                    if base_word == current_word and not suffixes:\n",
    "                        matches.append(current_word)\n",
    "                        suffix_categories.append(f\"({word_type})\")\n",
    "                        \n",
    "                        \n",
    "                    for suffix in suffixes:\n",
    "                        if base_word == current_word:\n",
    "                            matches.append(current_word)\n",
    "                            suffix_categories.append(f\"({word_type})\")\n",
    "\n",
    "                            sonuclar = parcala_ve_kontrol_et(suffix)\n",
    "                            for ek, dosya_adi in sonuclar:\n",
    "        \n",
    "                                matches.append(ek)\n",
    "                                suffix_categories.append(f\"({dosya_adi})\")\n",
    "                        else:\n",
    "                            if base_word.endswith('b'):\n",
    "                                base_word = base_word[:-1] + 'p'\n",
    "                            elif base_word.endswith('c'):\n",
    "                                base_word = base_word[:-1] + 'ç'\n",
    "                            elif base_word.endswith('d'):\n",
    "                                base_word = base_word[:-1] + 't'\n",
    "                            elif base_word.endswith('ğ'):\n",
    "                                base_word = base_word[:-1] + 'k'\n",
    "                            if base_word == current_word:\n",
    "                                matches.append(current_word)\n",
    "                                suffix_categories.append(f\"({word_type})\")\n",
    "\n",
    "                                sonuclar = parcala_ve_kontrol_et(suffix)\n",
    "                                for ek, dosya_adi in sonuclar:\n",
    "                                    matches.append(ek)\n",
    "                                    suffix_categories.append(f\"({dosya_adi})\")\n",
    "\n",
    "        return matches, suffix_categories\n",
    "\n",
    "    with open('C:/endings.txt', 'r', encoding='utf-8') as endings_file:\n",
    "        suffixes = [line.strip() for line in endings_file]\n",
    "\n",
    "    eksizEslesme, suffix_categories = process_dataset(base_word, [])\n",
    "    \n",
    "    if any('CL_FIIL' in category for category in suffix_categories):\n",
    "        #return 'Bu bir fiil oldugu icin burasi lokmandan gelecek'\n",
    "        return deger(base_word)\n",
    "\n",
    "    all_suffixes = find_suffixes(base_word, suffixes)\n",
    "\n",
    "    if eksizEslesme:\n",
    "        return ''.join(eksizEslesme), ''.join(suffix_categories)\n",
    "    elif all_suffixes:\n",
    "        result_suffixes = []\n",
    "        for suffix in sorted(all_suffixes, key=len, reverse=True):\n",
    "            remaining_word = base_word[:-len(suffix)]\n",
    "            matches, suffix_categories = process_dataset(remaining_word, [suffix])\n",
    "            \n",
    "            \n",
    "            if matches:\n",
    "\n",
    "                if any('CL_FIIL' in category for category in suffix_categories):\n",
    "                    #return 'Bu bir fiil oldugu icin burasi lokmandan gelecek'\n",
    "                    return deger(base_word)\n",
    "                result_suffixes.extend(matches)\n",
    "                result_suffixes.extend(suffix_categories)\n",
    "\n",
    "        return ''.join(result_suffixes)\n",
    "    else:\n",
    "        return f\"{base_word} kelimesi için eşleşme bulunamadı.\"\n",
    "        \n",
    "@app.route(\"/\", methods=[\"GET\", \"POST\"])\n",
    "def index():\n",
    "    results = []\n",
    "\n",
    "    if request.method == \"POST\":\n",
    "        user_input = request.form.get(\"user_input\")\n",
    "        user_input = user_input.lower()\n",
    "\n",
    "        sentences = sentence_splitter(user_input)\n",
    "\n",
    "        for sentence in sentences:\n",
    "            tokens = tokenizer(sentence)\n",
    "            processed_tokens = []\n",
    "\n",
    "            for token in tokens:\n",
    "                result = find_and_process_suffixes_v2(token, 'C:/dataset.txt')\n",
    "                processed_tokens.append(result)\n",
    "\n",
    "            results.append(processed_tokens)\n",
    "    return render_template(\"index.html\", results=results)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True, use_reloader=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e03a28d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [13/Jan/2024 17:54:55] \"GET / HTTP/1.1\" 500 -\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\yakup\\anaconda3\\Lib\\site-packages\\flask\\app.py\", line 2548, in __call__\n",
      "    return self.wsgi_app(environ, start_response)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yakup\\anaconda3\\Lib\\site-packages\\flask\\app.py\", line 2528, in wsgi_app\n",
      "    response = self.handle_exception(e)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yakup\\anaconda3\\Lib\\site-packages\\flask\\app.py\", line 2525, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yakup\\anaconda3\\Lib\\site-packages\\flask\\app.py\", line 1822, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yakup\\anaconda3\\Lib\\site-packages\\flask\\app.py\", line 1820, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yakup\\anaconda3\\Lib\\site-packages\\flask\\app.py\", line 1796, in dispatch_request\n",
      "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yakup\\AppData\\Local\\Temp\\ipykernel_14348\\823516225.py\", line 289, in index\n",
      "    return render_template(\"index.html\", results=results)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yakup\\anaconda3\\Lib\\site-packages\\flask\\templating.py\", line 147, in render_template\n",
      "    return _render(app, template, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yakup\\anaconda3\\Lib\\site-packages\\flask\\templating.py\", line 130, in _render\n",
      "    rv = template.render(context)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yakup\\anaconda3\\Lib\\site-packages\\jinja2\\environment.py\", line 1301, in render\n",
      "    self.environment.handle_exception()\n",
      "  File \"C:\\Users\\yakup\\anaconda3\\Lib\\site-packages\\jinja2\\environment.py\", line 936, in handle_exception\n",
      "    raise rewrite_traceback_stack(source=source)\n",
      "  File \"C:\\Users\\yakup\\NLP_Tasarim\\templates\\index.html\", line 17, in top-level template code\n",
      "    <a href=\"{{ url_for('developers') }}\" class=\"btn btn-info mt-3 text-white\">Geliştiriciler</a>\n",
      "  File \"C:\\Users\\yakup\\anaconda3\\Lib\\site-packages\\flask\\app.py\", line 2031, in url_for\n",
      "    return self.handle_url_build_error(error, endpoint, values)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yakup\\anaconda3\\Lib\\site-packages\\flask\\app.py\", line 2020, in url_for\n",
      "    rv = url_adapter.build(  # type: ignore[union-attr]\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yakup\\anaconda3\\Lib\\site-packages\\werkzeug\\routing\\map.py\", line 917, in build\n",
      "    raise BuildError(endpoint, values, method, self)\n",
      "werkzeug.routing.exceptions.BuildError: Could not build url for endpoint 'developers'. Did you mean 'index' instead?\n",
      "127.0.0.1 - - [13/Jan/2024 17:54:55] \"GET /?__debugger__=yes&cmd=resource&f=style.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [13/Jan/2024 17:54:55] \"GET /?__debugger__=yes&cmd=resource&f=debugger.js HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [13/Jan/2024 17:54:55] \"GET /?__debugger__=yes&cmd=resource&f=console.png HTTP/1.1\" 304 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, request\n",
    "import re\n",
    "import random\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "\n",
    "def get_random_sentence(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        sentences = [line.strip() for line in file]\n",
    "    return random.choice(sentences)\n",
    "def sentence_splitter(text):\n",
    "    sentence_endings = r\"[.!?]\"\n",
    "    sentences = re.split(sentence_endings, text)\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    return sentences\n",
    "\n",
    "def tokenizer(text):\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "def parcala_ve_kontrol_et(input_str):\n",
    "    ek_dosyalar = {\n",
    "        \n",
    "        \"Opt\":[\"yalım\",\"yelim\",\"alım\",\"elim\"],\n",
    "        \"Opt+A2sg\":[\"yasın\",\"yesin\",\"asın\",\"esin\"],\n",
    "        \"Opt+A2pl\":[\"yasınız\",\"yesiniz\",\"asınız\",\"esiniz\"],\n",
    "        \"Opt+A1sg\":[\"yayım\",\"yeyim\",\"ayım\",\"eyim\"],\n",
    "        ## noun suffixes\n",
    "        \"Pl\": [\"lar\", \"ler\"],\n",
    "        ## case suffixes\n",
    "        \"Dat\": [\"ya\",\"ye\",\"e\",\"a\",\"na\",\"ne\"],\n",
    "        \"Loc\": [\"da\", \"de\", \"ta\", \"te\",\"nde\",\"nda\"],\n",
    "        \"Abl\": [\"dan\", \"den\", \"tan\", \"ten\",\"ndan\",\"nden\"],\n",
    "        \"Gen\": [\"nın\", \"nin\",\"nun\",\"nün\",\"ın\",\"in\",\"un\", \"ün\",\"yın\",\"yin\",\"yun\",\"yün\",\"ım\",\"im\",\"um\",\"üm\",\"m\"],\n",
    "        \"Acc\": [\"yı\", \"yi\", \"yu\", \"yü\", \"ı\", \"i\", \"u\",\"ü\",\"nı\",\"ni\",\"nu\",\"nü\"],\n",
    "        \"Inst\": [\"yla\", \"yle\",\"la\",\"le\"],\n",
    "        \"P1sg\": [\"ım\",\"im\",\"um\",\"üm\",\"m\",\"yum\"],\n",
    "        \"P2sg\": [\"ın\",\"in\",\"un\",\"ün\",\"n\",\"yun\"],\n",
    "        \"P3sg\": [\"sı\", \"si\", \"su\", \"sü\",\"ı\",\"i\",\"u\",\"ü\",\"yu\"],\n",
    "        \"P1pl\": [\"ımız\", \"imiz\", \"umuz\", \"ümüz\",\"mız\",\"miz\",\"muz\",\"müz\",\"yumuz\"],\n",
    "        \"P2pl\": [\"ınız\",\"iniz\",\"unuz\",\"ünüz\",\"nız\",\"niz\",\"nuz\",\"nüz\",\"yunuz\"],\n",
    "        \"P3pl\": [\"ı\", \"i\",\"u\",\"ü\",\"ları\",\"leri\"],\n",
    "        \"Dim\": [\"cık\", \"cik\",\"cuk\",\"cük\",\"çık\",\"çik\",\"çuk\",\"çük\",\"cığ\",\"ciğ\",\"cuğ\",\"cüğ\",\"çığ\",\"çiğ\",\"çuğ\",\"çüğ\"],\n",
    "        \"Dim2\": [\"cağız\", \"ceğiz\"],\n",
    "        \"With\":[\"lı\",\"li\",\"lu\",\"lü\"],\n",
    "        \"Without\": [\"sız\", \"siz\",\"suz\",\"süz\"],\n",
    "        \"Since\":[\"dır\",\"dir\",\"dur\",\"dür\",\"tır\",\"tir\",\"tur\",\"tür\"],\n",
    "        \"Related\": [\"sal\", \"sel\"],\n",
    "        \"Ness\":[\"lık\",\"lik\",\"luk\",\"lük\",\"lığ\",\"liğ\",\"luğ\",\"lüğ\"],\n",
    "        \"Acquire\": [\"lan\", \"len\"],\n",
    "        \"Apply\":[\"la\",\"le\"],\n",
    "        \"Resemb\": [\"ımsı\", \"imsi\",\"umsu\",\"ümsü\",\"msı\",\"msi\",\"msu\",\"msü\"],\n",
    "        \"Rel\":[\"kı\",\"ki\",\"ku\",\"kü\"],\n",
    "        \"By\": [\"ca\", \"ce\"],\n",
    "        \"Cmp\":[\"ca\",\"ce\"],\n",
    "        \"Agt\":[\"cı\",\"ci\",\"cu\",\"cü\",\"yıcı\",\"yici\",\"yucu\",\"yücü\"],\n",
    "        ## Adjective Suffixes\n",
    "        \"Become\":[\"laş\",\"leş\"],\n",
    "        \"Ly\":[\"ca\",\"ce\"],\n",
    "        \"Quite\":[\"ca\",\"ce\"],\n",
    "        ## number-person agreement suffixes\n",
    "        \"A1sg\": [\"yım\", \"yim\",\"yum\",\"yüm\",\"m\",\"ım\",\"im\",\"um\",\"üm\"],\n",
    "        \"A2sg\":[\"sın\",\"sin\",\"sun\",\"sün\",\"n\",\"ın\",\"in\",\"un\",\"ün\"],\n",
    "        \"A3sg\":[\"\"],\n",
    "        \"A1pl\": [\"yız\",\"yiz\",\"k\",\"yuz\",\"yüz\",\"ız\",\"iz\",\"uz\",\"üz\"],\n",
    "        \"A2pl\":[\"sınız\",\"siniz\",\"sunuz\",\"sünüz\",\"nız\",\"niz\",\"nuz\",\"nüz\",\"ınız\",\"iniz\",\"unuz\",\"ünüz\"],\n",
    "        \"A3pl\": [\"lar\",\"ler\"],\n",
    "        ### Numeral suffixes\n",
    "        \"Grouping\": [\"şar\",\"şer\",\"ız\",\"iz\",\"ar\",\"er\"],\n",
    "        \"Ordinal\":[\"ıncı\",\"inci\",\"uncu\",\"üncü\",\"ncı\",\"nci\",\"ncu\",\"ncü\"],\n",
    "        \"Division\":[\"da\",\"de\",\"ta\",\"te\"],\n",
    "        ### verbal suffixes\n",
    "        \"Aor\":[\"ır,ir,ur,ür,ar,er,z\"],\n",
    "        \"Prog\":[\"ıyor\",\"iyor\",\"uyor\",\"üyor\"],\n",
    "        \"Prog2\":[\"makta\",\"mekte\"],\n",
    "        \"Fut\":[\"yacak\",\"yecek\",\"yacağ\",\"yeceğ\",\"acak\",\"ecek\",\"acağ\",\"eceğ\"],\n",
    "        \"Past\":[\"dı\",\"di\",\"du\",\"dü\",\"tı\",\"ti\",\"tu\",\"tü\"],\n",
    "        \"Evid\":[\"mış\",\"miş\",\"muş\",\"müş\"],\n",
    "        \"Neg\":[\"ma,me,m\"],\n",
    "        \"Cond\":[\"se\",\"sa\"],\n",
    "        \"Necess\":[\"meli\",\"malı\"],\n",
    "        \"Opt\":[\"ye\",\"ya\",\"e\",\"a\"],\n",
    "        \"Imp\":[\"sın\",\"sin\",\"sun\",\"sün\",\"yın\",\"yin\",\"yun\",\"yün\",\"ın\",\"in\",\"un\",\"ün\",\"sınlar\",\"sinler\",\"sünler\",\"sunlar\",\"yınız\",\"yiniz\",\"yunuz\",\"yünüz\",\"ınız\",\"iniz\",\"unuz\",\"ünüz\"],\n",
    "        ### copular suffixes\n",
    "        \"PastCop\":[\"ydı\",\"ydi\",\"ydu\",\"ydü\",\"dı\",\"di\",\"du\",\"dü\"],\n",
    "        \"EvidCop\":[\"ymış\",\"ymiş\",\"ymuş\",\"ymüş\",\"mış\",\"miş\",\"muş\",\"müş\"],\n",
    "        \"CondCop\":[\"ysa\",\"yse\",\"sa\",\"se\"],\n",
    "        \"While\":[\"yken\",\"ken\"],\n",
    "        \"Cop\":[\"dır\",\"dir\",\"dur\",\"dür\",\"tır\",\"tir\",\"tur\",\"tür\"],\n",
    "        ### Derivational Suffixes\n",
    "        \"Pass\":[\"ıl\",\"il\",\"ul\",\"ül\",\"ın\",\"in\",\"un\",\"ün\",\"n\",\"nıl\",\"nil\",\"nul\",\"nül\"],\n",
    "        \"Caus\":[\"dır\",\"dir\",\"dur\",\"dür\",\"tır\",\"tir\",\"tur\",\"tür\",\"t\"],\n",
    "        \"Recip\":[\"yış\",\"yiş\",\"yuş\",\"yüş\",\"ış\",\"iş\",\"uş\",\"üş\",\"ş\"],\n",
    "        \"Reflex\":[\"ın\",\"in\",\"un\",\"ün\"],\n",
    "        \"Abil\":[\"yabıl\",\"yabil\",\"yabul\",\"yabül\",\"yebil\",\"yebul\",\"yebül\",\"abıl\",\"ebil\",\"abul\",\"abül\",\"ebıl\",\"ebul\",\"ebül\"],\n",
    "        \"NegAbil\":[\"yama\",\"yeme\",\"ama\",\"eme\"],\n",
    "        \"Cont\":[\"yadur\",\"yedur\",\"adur\",\"edur\"],\n",
    "        \"EverSince\":[\"yagel\",\"yegel\",\"agel\",\"egel\"],\n",
    "        \"Cont2\":[\"yagör\",\"yegör\",\"agör\",\"egör\"],\n",
    "        \"Almost\":[\"yayaz\",\"yeyez\",\"yayez\",\"yeyaz\",\"ayaz\",\"eyez\",\"ayez\",\"eyaz\"],\n",
    "        \"Hastily\":[\"yıver\",\"yiver\",\"ıver\",\"iver\"],\n",
    "        \"Stay\":[\"yakal\",\"yekal\",\"akal\",\"ekal\"],\n",
    "        \"Inf\":[\"mak\",\"mek\"],\n",
    "        \"Inf2\":[\"ma\",\"me\"],\n",
    "        \"Inf3\":[\"yış\",\"yiş\",\"yuş\",\"yüş\",\"ış\",\"iş\",\"uş\",\"üş\"],\n",
    "        \"FutPart\":[\"yacak\",\"yecek\",\"yacağ\",\"yeceğ\",\"acak\",\"ecek\",\"acağ\",\"eceğ\"],\n",
    "        \"EvidPart\":[\"mış\",\"miş\",\"muş\",\"müş\"],\n",
    "        \"PastPart\":[\"dık\",\"dik\",\"duk\",\"dük\",\"tık\",\"tik\",\"tuk\",\"tük\",\"dığ\",\"diğ\",\"duğ\",\"düğ\",\"tığ\",\"tiğ\",\"tuğ\",\"tüğ\"],\n",
    "        \"PresPart\":[\"yan\",\"yen\",\"an\",\"en\"],\n",
    "        \"AorPart\":[\"ır\",\"ir\",\"ur\",\"ür\",\"ar\",\"er\",\"r\"],\n",
    "        \"AfterDoing\":[\"yıp\",\"yip\",\"yup\",\"yüp\",\"ıp\",\"ip\",\"up\",\"üp\"],\n",
    "        \"Agt\":[\"yıcı\",\"yici\",\"yucu\",\"yücü\",\"ıcı\",\"ici\",\"ucu\",\"ücü\"],\n",
    "        \"FeelLike\":[\"yası\",\"yesi\",\"ası\",\"esi\"],\n",
    "        \"SinceDoing\":[\"yalı\",\"yeli\",\"alı\",\"eli\"],\n",
    "        \"AsLongAs\":[\"dıkca\",\"dikce\",\"dıkça\",\"dikçe\",\"tıkca\",\"tikce\",\"tıkça\",\"tikçe\"],\n",
    "        \"JustAfter\":[\"yınca\",\"yince\",\"ınca\",\"ince\"],\n",
    "        \"ByDoing\":[\"yarak\",\"yerek\",\"arak\",\"erek\"],\n",
    "        \"WithoutDoing\":[\"madan\",\"meden\",\"adan\",\"eden\"],\n",
    "        \"WithoutDoing2\":[\"maksızın\",\"meksizin\",\"aksızın\",\"eksizin\"],\n",
    "        \"RatherThanInsteadOfDoing\":[\"maktansa\",\"mektense\",\"aktansa\",\"ektense\"],\n",
    "        \"UnableToDo\":[\"yamadan\",\"yemeden\",\"amadan\",\"emeden\"],\n",
    "        \"ActOf\":[\"maca\",\"mece\"],\n",
    "        \"NotState\":[\"mazlık\",\"mezlik\",\"mazlığ\",\"mezliğ\"],\n",
    "        \"AsIf\":[\"casına\",\"cesine\",\"çasına\",\"çesine\"],\n",
    "        \"Adamantly\":[\"yasıya\",\"yesiye\",\"asıya\",\"esiye\"],\n",
    "\n",
    "    }\n",
    "    ozel_durumlar = {\n",
    "        #\"ımız\": [\"ım\", \"ız\"],\n",
    "        #\"sinler\":[\"s\",\"i\",\"n\",\"ler\"],\n",
    "        #\"leri\":[\"ler\",\"i\"],\n",
    "        #\"sının\":[\"sı\",\"nın\"],\n",
    "        #\"elim\":[\"e\",\"lim\"],\n",
    "\n",
    "    }\n",
    "\n",
    "    sonuclar = []\n",
    "    en_uzun_ekler=[]\n",
    "\n",
    "    while input_str:\n",
    "        bulundu = False\n",
    "        ozelBulundu = False\n",
    "        en_uzun_ek = \"\"\n",
    "        ek_adi_list = []  \n",
    "\n",
    "        for ozel_ek, ayrilma_sekli in ozel_durumlar.items():\n",
    "            if input_str.startswith(ozel_ek):\n",
    "                for ayrilma in ayrilma_sekli:\n",
    "                    for ad, ekler in ek_dosyalar.items():\n",
    "                        if ayrilma in ekler and input_str.startswith(ayrilma):\n",
    "                            en_uzun_ek = ayrilma\n",
    "                            ek_adi = f\"({ad})\"\n",
    "                            ozelBulundu = True\n",
    "                            if ozelBulundu:\n",
    "                                input_str = input_str[len(en_uzun_ek):].strip()\n",
    "                                ek_adi_list.append(ek_adi)\n",
    "\n",
    "        if not ozelBulundu:\n",
    "            for ad, ekler in ek_dosyalar.items():\n",
    "                for ek in ekler:\n",
    "                    if input_str.startswith(ek) and len(ek) >= len(en_uzun_ek):\n",
    "                        en_uzun_ek = ek\n",
    "                        ek_adi = f\"({ad})\"\n",
    "                        bulundu = True\n",
    "                        \n",
    "                \n",
    "                \n",
    "            if bulundu:\n",
    "                input_str = input_str[len(en_uzun_ek):].strip()\n",
    "                for ad,ekler in ek_dosyalar.items():\n",
    "\n",
    "                    if en_uzun_ek in ekler:\n",
    "                        #print(en_uzun_ek,ekler,\"yakup\")\n",
    "                        ek_adi_list.append(f\"({ad})\")\n",
    "                    \n",
    "                #ek_adi_list.append(ek_adi)\n",
    "            else:\n",
    "                ek_adi_list.append(f\"({input_str})\")\n",
    "\n",
    "        if ek_adi_list:\n",
    "            sonuclar.append((f\"-{en_uzun_ek} \", ek_adi_list))\n",
    "\n",
    "    return sonuclar\n",
    "\n",
    "\n",
    "\n",
    "def find_and_process_suffixes_v2(base_word, dataset_path):\n",
    "    def find_suffixes(word, suffixes):\n",
    "        found_suffixes = [suffix for suffix in suffixes if word.endswith(suffix)]\n",
    "        return found_suffixes\n",
    "\n",
    "    def process_dataset(base_word, suffixes):\n",
    "        matches = []\n",
    "        suffix_categories = []  \n",
    "\n",
    "        with open(dataset_path, 'r', encoding='utf-8') as dataset_file:\n",
    "            for line in dataset_file:\n",
    "                parts = line.strip().split()\n",
    "\n",
    "                if len(parts[0]) >= 2:\n",
    "                    current_word, word_type = parts[0], parts[1:]\n",
    "                    \n",
    "                    if base_word == current_word and not suffixes:\n",
    "                        matches.append(current_word)\n",
    "                        suffix_categories.append(f\"({word_type})\")\n",
    "                        \n",
    "                        \n",
    "                    for suffix in suffixes:\n",
    "                        if base_word == current_word:\n",
    "                            matches.append(current_word)\n",
    "                            suffix_categories.append(f\"({word_type})\")\n",
    "\n",
    "                            sonuclar = parcala_ve_kontrol_et(suffix)\n",
    "                            for ek, dosya_adi in sonuclar:\n",
    "        \n",
    "                                matches.append(ek)\n",
    "                                suffix_categories.append(f\"({dosya_adi})\")\n",
    "                        else:\n",
    "                            if base_word.endswith('b'):\n",
    "                                base_word = base_word[:-1] + 'p'\n",
    "                            elif base_word.endswith('c'):\n",
    "                                base_word = base_word[:-1] + 'ç'\n",
    "                            elif base_word.endswith('d'):\n",
    "                                base_word = base_word[:-1] + 't'\n",
    "                            elif base_word.endswith('ğ'):\n",
    "                                base_word = base_word[:-1] + 'k'\n",
    "                            if base_word == current_word:\n",
    "                                matches.append(current_word)\n",
    "                                suffix_categories.append(f\"({word_type})\")\n",
    "\n",
    "                                sonuclar = parcala_ve_kontrol_et(suffix)\n",
    "                                for ek, dosya_adi in sonuclar:\n",
    "                                    matches.append(ek)\n",
    "                                    suffix_categories.append(f\"({dosya_adi})\")\n",
    "\n",
    "        return matches, suffix_categories\n",
    "\n",
    "    with open('C:/endings.txt', 'r', encoding='utf-8') as endings_file:\n",
    "        suffixes = [line.strip() for line in endings_file]\n",
    "\n",
    "    eksizEslesme, suffix_categories = process_dataset(base_word, [])\n",
    "    \n",
    "    if any('CL_FIIL' in category for category in suffix_categories):\n",
    "        return 'Bu bir fiil oldugu icin burasi lokmandan gelecek'\n",
    "\n",
    "    all_suffixes = find_suffixes(base_word, suffixes)\n",
    "\n",
    "    if eksizEslesme:\n",
    "        return ''.join(eksizEslesme), ''.join(suffix_categories)\n",
    "    elif all_suffixes:\n",
    "        result_suffixes = []\n",
    "        for suffix in sorted(all_suffixes, key=len, reverse=True):\n",
    "            remaining_word = base_word[:-len(suffix)]\n",
    "            matches, suffix_categories = process_dataset(remaining_word, [suffix])\n",
    "            \n",
    "            \n",
    "            if matches:\n",
    "\n",
    "                if any('CL_FIIL' in category for category in suffix_categories):\n",
    "                    return 'Bu bir fiil oldugu icin burasi lokmandan gelecek'\n",
    "                result_suffixes.extend(matches)\n",
    "                result_suffixes.extend(suffix_categories)\n",
    "\n",
    "        return ''.join(result_suffixes)\n",
    "    else:\n",
    "        return f\"{base_word} kelimesi için eşleşme bulunamadı.\"\n",
    "        \n",
    "@app.route(\"/\", methods=[\"GET\", \"POST\"])\n",
    "def index():\n",
    "    results = []\n",
    "\n",
    "    if request.method == \"POST\":\n",
    "        user_input = request.form.get(\"user_input\")\n",
    "        user_input = user_input.lower()\n",
    "\n",
    "        sentences = sentence_splitter(user_input)\n",
    "\n",
    "        for sentence in sentences:\n",
    "            tokens = tokenizer(sentence)\n",
    "            processed_tokens = []\n",
    "\n",
    "            for token in tokens:\n",
    "                result = find_and_process_suffixes_v2(token, 'C:/dataset.txt')\n",
    "                processed_tokens.append(result)\n",
    "\n",
    "            results.append(processed_tokens)\n",
    "    return render_template(\"index.html\", results=results)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True, use_reloader=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12f5ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, request\n",
    "import re\n",
    "%run FiilEkAyrimi.ipynb\n",
    "#from FiilEkAyrimi import deger\n",
    "from difflib import get_close_matches\n",
    "import numpy as np\n",
    "%run Untitled1.ipynb\n",
    "import random\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "## global tanımlamalar (ekler)\n",
    "\n",
    "ek_dosyalar = {\n",
    "        \"Opt\":[\"yalım\",\"yelim\",\"alım\",\"elim\"],\n",
    "        \"Opt+A2sg\":[\"yasın\",\"yesin\",\"asın\",\"esin\"],\n",
    "        \"Opt+A2pl\":[\"yasınız\",\"yesiniz\",\"asınız\",\"esiniz\"],\n",
    "        \"Opt+A1sg\":[\"yayım\",\"yeyim\",\"ayım\",\"eyim\"],\n",
    "        ## noun suffixes\n",
    "        \"Pl\": [\"lar\", \"ler\"],\n",
    "        ## case suffixes\n",
    "        \"Dat\": [\"ya\",\"ye\",\"e\",\"a\",\"na\",\"ne\"],\n",
    "        \"Loc\": [\"da\",\"de\",\"ta\",\"te\",\"nde\",\"nda\"],\n",
    "        \"Abl\": [\"dan\",\"den\",\"tan\",\"ten\",\"ndan\",\"nden\"],\n",
    "        \"Gen\": [\"nın\",\"nin\",\"nun\",\"nün\",\"ın\",\"in\",\"un\",\"ün\",\"yın\",\"yin\",\"yun\",\"yün\",\"ım\",\"im\",\"um\",\"üm\",\"m\"],\n",
    "        \"Acc\": [\"yı\",\"yi\",\"yu\",\"yü\",\"ı\",\"i\",\"u\",\"ü\",\"nı\",\"ni\",\"nu\",\"nü\"],\n",
    "        \"Inst\": [\"yla\",\"yle\",\"la\",\"le\"],\n",
    "        \"P1sg\": [\"ım\",\"im\",\"um\",\"üm\",\"m\",\"yum\"],\n",
    "        \"P2sg\": [\"ın\",\"in\",\"un\",\"ün\",\"n\",\"yun\"],\n",
    "        \"P3sg\": [\"sı\",\"si\",\"su\",\"sü\",\"ı\",\"i\",\"u\",\"ü\",\"yu\"],\n",
    "        \"P1pl\": [\"ımız\",\"imiz\",\"umuz\",\"ümüz\",\"mız\",\"miz\",\"muz\",\"müz\",\"yumuz\"],\n",
    "        \"P2pl\": [\"ınız\",\"iniz\",\"unuz\",\"ünüz\",\"nız\",\"niz\",\"nuz\",\"nüz\",\"yunuz\"],\n",
    "        \"P3pl\": [\"ı\", \"i\",\"u\",\"ü\",\"ları\",\"leri\"],\n",
    "        \"Dim\": [\"cık\",\"cik\",\"cuk\",\"cük\",\"çık\",\"çik\",\"çuk\",\"çük\",\"cığ\",\"ciğ\",\"cuğ\",\"cüğ\",\"çığ\",\"çiğ\",\"çuğ\",\"çüğ\"],\n",
    "        \"Dim2\": [\"cağız\", \"ceğiz\"],\n",
    "        \"With\":[\"lı\",\"li\",\"lu\",\"lü\"],\n",
    "        \"Without\": [\"sız\", \"siz\",\"suz\",\"süz\"],\n",
    "        \"Since\":[\"dır\",\"dir\",\"dur\",\"dür\",\"tır\",\"tir\",\"tur\",\"tür\"],\n",
    "        \"Related\": [\"sal\", \"sel\"],\n",
    "        \"Ness\":[\"lık\",\"lik\",\"luk\",\"lük\",\"lığ\",\"liğ\",\"luğ\",\"lüğ\"],\n",
    "        \"Acquire\": [\"lan\", \"len\"],\n",
    "        \"Apply\":[\"la\",\"le\"],\n",
    "        \"Resemb\": [\"ımsı\",\"imsi\",\"umsu\",\"ümsü\",\"msı\",\"msi\",\"msu\",\"msü\"],\n",
    "        \"Rel\":[\"kı\",\"ki\",\"ku\",\"kü\"],\n",
    "        \"By\": [\"ca\",\"ce\"],\n",
    "        \"Cmp\":[\"ca\",\"ce\"],\n",
    "        \"Agt\":[\"cı\",\"ci\",\"cu\",\"cü\",\"yıcı\",\"yici\",\"yucu\",\"yücü\"],\n",
    "        ## Adjective Suffixes\n",
    "        \"Become\":[\"laş\",\"leş\"],\n",
    "        \"Ly\":[\"ca\",\"ce\"],\n",
    "        \"Quite\":[\"ca\",\"ce\"],\n",
    "        ## number-person agreement suffixes\n",
    "        \"A1sg\": [\"yım\",\"yim\",\"yum\",\"yüm\",\"m\",\"ım\",\"im\",\"um\",\"üm\"],\n",
    "        \"A2sg\":[\"sın\",\"sin\",\"sun\",\"sün\",\"n\",\"ın\",\"in\",\"un\",\"ün\"],\n",
    "        \"A3sg\":[\"\"],\n",
    "        \"A1pl\": [\"yız\",\"yiz\",\"k\",\"yuz\",\"yüz\",\"ız\",\"iz\",\"uz\",\"üz\"],\n",
    "        \"A2pl\":[\"sınız\",\"siniz\",\"sunuz\",\"sünüz\",\"nız\",\"niz\",\"nuz\",\"nüz\",\"ınız\",\"iniz\",\"unuz\",\"ünüz\"],\n",
    "        \"A3pl\": [\"lar\",\"ler\"],\n",
    "        ### Numeral suffixes\n",
    "        \"Grouping\": [\"şar\",\"şer\",\"ız\",\"iz\",\"ar\",\"er\"],\n",
    "        \"Ordinal\":[\"ıncı\",\"inci\",\"uncu\",\"üncü\",\"ncı\",\"nci\",\"ncu\",\"ncü\"],\n",
    "        \"Division\":[\"da\",\"de\",\"ta\",\"te\"],\n",
    "\n",
    "        ### Derivational Suffixes\n",
    "        \n",
    "        \"Trap_Eki\":[\"a\",\"b\",\"c\",\"ç\",\"d\",\"e\",\"f\",\"g\",\"ğ\",\"h\",\"ı\",\"i\",\"j\",\"k\",\"l\",\"m\",\"n\",\"o\",\"ö\",\"p\",\"r\",\"s\",\"ş\",\"t\",\"u\",\"ü\",\"v\",\"y\",\"z\"],\n",
    "        \n",
    "    }\n",
    "\n",
    "\n",
    "## global tanimlamalar sonu\n",
    "\n",
    "def get_random_sentence(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        sentences = [line.strip() for line in file]\n",
    "    return random.choice(sentences)\n",
    "\n",
    "def find_longest_root(input_word, root_list):\n",
    "    longest_root = ''\n",
    "    for root in root_list:\n",
    "        if input_word.startswith(root) and len(root) > len(longest_root):\n",
    "            longest_root = root\n",
    "   \n",
    "    return longest_root\n",
    "\n",
    "def correct_input(input_word, root_list, suffix_list):\n",
    "    longest_root = find_longest_root(input_word, root_list)\n",
    "    print(longest_root,\"bu longest_root\")\n",
    "    geriyeKalanEk = input_word[len(longest_root):]\n",
    "    print(geriyeKalanEk,\"geriye kalan ek\")\n",
    "        \n",
    "    if(geriyeKalanEk):\n",
    "        suffix_correction = get_close_matches(geriyeKalanEk, suffix_list, n=1, cutoff=0.4)\n",
    "        corrected_suffix = suffix_correction[0] if suffix_correction else geriyeKalanEk\n",
    "        corrected_word = longest_root + corrected_suffix\n",
    "        \n",
    "    else:\n",
    "        corrected_word = input_word\n",
    "\n",
    "    return corrected_word\n",
    "\n",
    "\n",
    "def sentence_splitter(text):\n",
    "    sentence_endings = r\"[.!?]\"\n",
    "    sentences = re.split(sentence_endings, text)\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    return sentences\n",
    "\n",
    "def tokenizer(text):\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "def parcala_ve_kontrol_et(input_str):\n",
    "    \n",
    "    ozel_durumlar = {\n",
    "        #\"ımız\": [\"ım\", \"ız\"],\n",
    "        #\"sinler\":[\"s\",\"i\",\"n\",\"ler\"],\n",
    "        #\"leri\":[\"ler\",\"i\"],\n",
    "        #\"sının\":[\"sı\",\"nın\"],\n",
    "        #\"elim\":[\"e\",\"lim\"],\n",
    "\n",
    "    }\n",
    "    \n",
    "\n",
    "    sonuclar = []\n",
    "    en_uzun_ekler = []\n",
    "\n",
    "    while input_str:\n",
    "        bulundu = False\n",
    "        ozelBulundu = False\n",
    "        en_uzun_ek = \"\"\n",
    "        ek_adi_list = []\n",
    "\n",
    "        # Özel durum kontrolü\n",
    "        for ozel_ek, ayrilma_sekli in ozel_durumlar.items():\n",
    "            if input_str.startswith(ozel_ek):\n",
    "                for ayrilma in ayrilma_sekli:\n",
    "                    for ad, ekler in ek_dosyalar.items():\n",
    "                        if ayrilma in ekler and input_str.startswith(ayrilma):\n",
    "                            en_uzun_ek = ayrilma\n",
    "                            ek_adi = f\"({ad})\"\n",
    "                            ozelBulundu = True\n",
    "                            if ozelBulundu:\n",
    "                                input_str = input_str[len(en_uzun_ek):].strip()\n",
    "                                ek_adi_list.append(ek_adi)\n",
    "\n",
    "        if not ozelBulundu:\n",
    "            for ad, ekler in ek_dosyalar.items():\n",
    "                for ek in ekler:\n",
    "                    if input_str.startswith(ek) and len(ek) >= len(en_uzun_ek):\n",
    "                        en_uzun_ek = ek\n",
    "                        ek_adi = f\"({ad})\"\n",
    "                        bulundu = True\n",
    "\n",
    "            if bulundu:\n",
    "                input_str = input_str[len(en_uzun_ek):].strip()\n",
    "                for ad, ekler in ek_dosyalar.items():\n",
    "                    if en_uzun_ek in ekler:\n",
    "                        ek_adi_list.append(f\"({ad})\")\n",
    "            else:\n",
    "                ek_adi_list.append(f\"({input_str})\")\n",
    "\n",
    "        if ek_adi_list:\n",
    "            sonuclar.append((f\"-{en_uzun_ek} \", ek_adi_list))\n",
    "            en_uzun_ek = \"\"\n",
    "            ek_adi_list = []\n",
    "\n",
    "    print(sonuclar,\"YKAUP BABA SONUCLAR\")\n",
    "    return sonuclar\n",
    "\n",
    "\n",
    "def endswithKontrolu(input_str):\n",
    "    \n",
    "    ozel_durumlar = {\n",
    "        #\"ımız\": [\"ım\", \"ız\"],\n",
    "        #\"sinler\":[\"s\",\"i\",\"n\",\"ler\"],\n",
    "        #\"leri\":[\"ler\",\"i\"],\n",
    "        #\"sının\":[\"sı\",\"nın\"],\n",
    "        #\"elim\":[\"e\",\"lim\"],\n",
    "\n",
    "    }\n",
    "\n",
    "    sonuclar = []\n",
    "    en_uzun_ekler = []\n",
    "    \n",
    "    while input_str:\n",
    "        bulundu = False\n",
    "        ozelBulundu = False\n",
    "        en_uzun_ek = \"\"\n",
    "        ek_adi_list = []\n",
    "\n",
    "        # Özel durum kontrolü\n",
    "        for ozel_ek, ayrilma_sekli in ozel_durumlar.items():\n",
    "            if input_str.startswith(ozel_ek):\n",
    "                for ayrilma in ayrilma_sekli:\n",
    "                    for ad, ekler in ek_dosyalar.items():\n",
    "                        if ayrilma in ekler and input_str.startswith(ayrilma):\n",
    "                            en_uzun_ek = ayrilma\n",
    "                            ek_adi = f\"({ad})\"\n",
    "                            ozelBulundu = True\n",
    "                            if ozelBulundu:\n",
    "                                input_str = input_str[len(en_uzun_ek):].strip()\n",
    "                                ek_adi_list.append(ek_adi)\n",
    "\n",
    "        if not ozelBulundu:\n",
    "            for ad, ekler in ek_dosyalar.items():\n",
    "                for ek in ekler:\n",
    "                    if input_str.endswith(ek) and len(ek) >= len(en_uzun_ek):\n",
    "                        en_uzun_ek = ek\n",
    "                        ek_adi = f\"({ad})\"\n",
    "                        bulundu = True\n",
    "\n",
    "            if bulundu:\n",
    "                input_str = input_str[:-len(en_uzun_ek)].strip()\n",
    "                for ad, ekler in ek_dosyalar.items():\n",
    "                    if en_uzun_ek in ekler:\n",
    "                        ek_adi_list.append(f\"({ad})\")\n",
    "            else:\n",
    "                ek_adi_list.append(f\"({input_str})\")\n",
    "\n",
    "        if ek_adi_list:\n",
    "            sonuclar.insert(0, (f\"-{en_uzun_ek} \", ek_adi_list))\n",
    "            en_uzun_ek = \"\"\n",
    "            ek_adi_list = []\n",
    "\n",
    "    print(sonuclar,\"PUKAY BABA SONUCLAR\")\n",
    "    return sonuclar\n",
    "    \n",
    "def find_and_process_suffixes_v2(base_word, dataset_path):\n",
    "    def find_suffixes(word, suffixes):\n",
    "        found_suffixes = [suffix for suffix in suffixes if word.endswith(suffix)]\n",
    "        for x in found_suffixes:\n",
    "            print(x,\"yakup ek\")\n",
    "        return found_suffixes\n",
    "\n",
    "    def process_dataset(base_word, suffixes):\n",
    "        matches = []\n",
    "        suffix_categories = []\n",
    "        matches2 = []\n",
    "        suffix_categories2 = []\n",
    "        new_base_word=\"\"\n",
    "        \n",
    "        \n",
    "        print(base_word,\"process_dataya gelen base_word\")\n",
    "        print(suffixes,\"bu da process_dataya gelen suffixes\")\n",
    "        with open(dataset_path, 'r', encoding='utf-8') as dataset_file:\n",
    "            for line in dataset_file:\n",
    "                parts = line.strip().split()\n",
    "\n",
    "                if len(parts[0]):\n",
    "                    current_word, word_type = parts[0], parts[1:]\n",
    "                    \n",
    "                    if not suffixes:\n",
    "                        if base_word == current_word:\n",
    "                            matches.append(current_word)\n",
    "                            suffix_categories.append(f\"({word_type})\")\n",
    "                    \n",
    "                                \n",
    "                        \n",
    "                        \n",
    "                    for suffix in suffixes:\n",
    "\n",
    "                        \n",
    "                        if base_word == current_word:\n",
    "\n",
    "                            matches.append(current_word)\n",
    "                            suffix_categories.append(f\"({word_type})\")\n",
    "                            matches2.append(current_word)\n",
    "                            suffix_categories2.append(f\"({word_type})\")\n",
    "                            \n",
    "                            \n",
    "                            if any('CL_FIIL' in category for category in word_type):\n",
    "                                return matches, [\"CL_FIIL\"],\"\",\"\"\n",
    "                                print(\"eyw\")\n",
    "                            else:\n",
    "                                sonuclar = parcala_ve_kontrol_et(suffix)\n",
    "                                print(sonuclar,\"bunlar da sonuclar\")\n",
    "                                sonuclar2 = endswithKontrolu(suffix)\n",
    "                                print(sonuclar2,\"SONUCLAR 2\")\n",
    "                                for ek, dosya_adi in sonuclar:\n",
    "                                    matches.append(ek)\n",
    "                                    suffix_categories.append(f\"({dosya_adi})\")\n",
    "                                for ek,dosya_adi in sonuclar2:\n",
    "                                    print(\"BURAYA GIRIYON MU PEKI\")\n",
    "                                    matches2.append(ek)\n",
    "                                    suffix_categories2.append(f\"({dosya_adi})\")\n",
    "                        else:\n",
    "                            if base_word.endswith('b'):\n",
    "                                new_base_word = base_word[:-1] + 'p'\n",
    "                            elif base_word.endswith('c'):\n",
    "                                new_base_word = base_word[:-1] + 'ç'\n",
    "                            elif base_word.endswith('d'):\n",
    "                                new_base_word = base_word[:-1] + 't'\n",
    "                            elif base_word.endswith('ğ'):\n",
    "                                new_base_word = base_word[:-1] + 'k'\n",
    "                            if new_base_word == current_word:\n",
    "                                matches.append(current_word)\n",
    "                                suffix_categories.append(f\"({word_type})\")\n",
    "\n",
    "                                sonuclar = parcala_ve_kontrol_et(suffix)\n",
    "                                print(sonuclar,\"bunlar da sonuclar\")\n",
    "                                sonuclar2 = endswithKontrolu(suffix)\n",
    "                                print(sonuclar2,\"SONUCLAR 2\")\n",
    "                                for ek, dosya_adi in sonuclar:\n",
    "                                    matches.append(ek)\n",
    "                                    suffix_categories.append(f\"({dosya_adi})\")\n",
    "                                for ek,dosya_adi in sonuclar2:\n",
    "                                    print(\"BURAYA GIRIYON MU PEKI\")\n",
    "                                    matches2.append(ek)\n",
    "                                    suffix_categories2.append(f\"({dosya_adi})\")\n",
    "                                    \n",
    "        print(matches2,\"RETURN ETMEDEN ONCEKI MATCHES2\")\n",
    "        return matches, suffix_categories,matches2,suffix_categories2\n",
    "\n",
    "    ihtimalVarmi = False\n",
    "    fiilDondu=False\n",
    "    \n",
    "    unlu_dusmesi={\n",
    "        \"ağz\":\"ağız\",\n",
    "        \"aln\":\"alın\",\n",
    "        \"burn\":\"burun\",\n",
    "        \"bağr\":\"bağır\",\n",
    "        \"beyn\":\"beyin\",\n",
    "        \"göğs\":\"göğüs\",\n",
    "        \"karn\":\"karın\",\n",
    "        \"omz\":\"omuz\",\n",
    "        \"oğl\":\"oğul\",\n",
    "        \"gönl\":\"gönül\",\n",
    "        \"akl\":\"akıl\",\n",
    "        \"fikr\":\"fikir\",\n",
    "        \"cism\":\"cisim\",\n",
    "        \"ayr\":\"ayır\",\n",
    "        \"devr\":\"devir\",\n",
    "        \"çevr\":\"çevir\",\n",
    "        \"sıyr\":\"sıyır\",\n",
    "        \"kıvr\":\"kıvır\",\n",
    "        \"devr\":\"devir\",\n",
    "        \"kayb\":\"kayıp\",\n",
    "        \"haps\":\"hapis\",\n",
    "        \"sabr\":\"sabır\",\n",
    "        \"ufk\":\"ufuk\",\n",
    "        \"benz\":\"beniz\",\n",
    "        \"boyn\":\"boyun\",\n",
    "        \"bağr\":\"bağır\",\n",
    "        \"genz\":\"geniz\",\n",
    "        \"gönl\":\"gönül\",\n",
    "        \"oğl\":\"oğul\",\n",
    "        \n",
    "    }\n",
    "    \n",
    "    fiil_ekleri={\n",
    "        \"alım\",\n",
    "        \"elim\",\n",
    "        \"yım\",\n",
    "        \"yim\",\n",
    "        \"yum\",\n",
    "        \"m\",\n",
    "        \"ım\",\n",
    "        \"im\",\n",
    "        \"um\",\"üm\",\"laş\",\"leş\",\"sın\",\"sin\",\"sun\",\"sün\",\"n\",\"ın\",\"in\",\"un\",\"ün\",\"yız\",\"yiz\",\"k\",\"yuz\",\"yüz\",\n",
    "        \"ız\",\n",
    "        \"iz\",\n",
    "        \"uz\",\n",
    "        \"üz\",\n",
    "        \"sınız\",\n",
    "        \"siniz\",\n",
    "        \"sunuz\",\n",
    "        \"sünüz\",\n",
    "        \"ınız\",\n",
    "        \"iniz\",\n",
    "        \"unuz\",\n",
    "        \"ünüz\",\n",
    "        \"nız\",\n",
    "        \"niz\",\n",
    "        \"nuz\",\n",
    "        \"nüz\",\n",
    "        \"lar\",\n",
    "        \"ler\",\n",
    "        \"r\",\n",
    "        \"ar\",\n",
    "        \"er\",\n",
    "        \"ür\",\n",
    "        \"ir\",\n",
    "        \"ır\",\n",
    "        \"ur\",\n",
    "        \"iyor\",\"ıyor\",\"üyor\",\"uyor\",\"yor\",\"makta\",\"mekte\",\"acak\",\"ecek\",\"acağ\",\"eceğ\",\"yacak\",\"yecek\",\"yacağ\",\"yeceğ\",\n",
    "        \"tı\", \"dı\", \"ti\", \"di\", \"du\",\"tu\",\"tü\",\"dü\",\n",
    "        \"mış\", \"miş\", \"muş\", \"müş\",\n",
    "        \"ma\",\"me\",\"m\",\n",
    "        \"se\",\"sa\",\n",
    "        \"malı\",\"meli\",\n",
    "        \"a\",\"e\",\"ye\",\"ya\",\n",
    "        \"sin\" ,\"sın\",\"sün\" ,\"sun\",\"in\",\"ın\",\"ün\",\"un\",\"yün\",\"yin\",\"yın\",\"yun\",\"iniz\",\"ınız\",\"ünüz\",\"unuz\",\"yünüz\",\"yiniz\",\"yınız\",\"yunuz\",\n",
    "        \"tı\", \"dı\", \"ti\", \"di\", \"du\",\"tu\",\"tü\",\"dü\",\"ytı\",\"ydı\", \"yti\", \"ydi\", \"ydu\",\"ytu\",\"ytü\",\"ydü\",\n",
    "        \"mış\", \"miş\", \"muş\", \"müş\",\"ymış\", \"ymiş\", \"ymuş\", \"ymüş\",\n",
    "        \"ysa\",\"yse\",\"se\",\"sa\",\n",
    "        \"ken\",\"yken\",\n",
    "        \"il\", \"ıl\", \"ül\", \"ul\", \"n\", \"in\",\"ın\",\"ün\",\"un\",\"nıl\",\"nil\",\"nül\",\"nul\",\n",
    "        \"iş\",\"ış\",\"üş\",\"uş\",\"yüş\",\"yuş\",\"yiş\",\"yış\",\"ş\",\n",
    "        \"n\", \"in\",\"ın\",\"ün\",\"un\",\n",
    "        \"ebil\", \"abil\",\"yebil\",\"yabil\",\n",
    "        \"eme\", \"ama\",\"yeme\",\"yama\",\n",
    "        \"edur\", \"adur\",\"yedur\",\"yadur\",\n",
    "        \"egel\", \"agel\",\"yegel\",\"yagel\",\n",
    "        \"egör\", \"agör\",\"yegör\",\"yagör\",\n",
    "        \"eyaz\", \"ayaz\",\"yeyaz\",\"yayaz\",\n",
    "        \"uver\", \"üver\",\"iver\",\"ıver\",\"yuver\", \"yüver\",\"yiver\",\"yıver\",\n",
    "        \"ekal\", \"akal\",\"yekal\",\"yakal\",\n",
    "        \"mek\",\"mak\",\n",
    "        \"ma\",\"me\",\n",
    "        \"acak\", \"ecek\", \"acağ\", \"eceğ\",\"yacak\", \"yecek\", \"yacağ\", \"yeceğ\",\n",
    "        \"miş\",\"muş\",\"müş\",\"mış\",\n",
    "        \"tığ\", \"dığ\", \"tiğ\", \"diğ\", \"duğ\",\"tuğ\",\"tüğ\",\"düğ\",\n",
    "        \"an\",\"en\",\"yan\",\"yen\",\n",
    "        \"r\",\"ar\",\"er\",\"ür\",\"ir\",\"ır\",\"ur\",\n",
    "        \"ip\", \"ıp\",\"up\",\"üp\",\"yip\", \"yıp\",\"yup\",\"yüp\",\n",
    "        \"ucu\", \"ici\",\"ıcı\",\"ücü\",\"yucu\", \"yici\",\"yıcı\",\"yücü\",\n",
    "        \"esi\",\"ası\",\"yesi\",\"yası\",\n",
    "        \"eli\", \"alı\",\"yeli\",\"yalı\",\n",
    "        \"dikçe\", \"dıkça\",\"tikçe\",\"tıkça\",\"dükçe\",\"tükçe\",\"dukça\",\"tukça\",\n",
    "        \"ınca\",\"ince\",\"yınca\",\"yince\",\"unca\",\"yunca\",\"yünce\",\"ünce\",\n",
    "        \"arak\",\"erek\",\"yarak\",\"yerek\",\n",
    "        \"madan\",\"meden\",\n",
    "        \"maksızın\",\"meksizin\",\n",
    "        \"maktansa\",\"mektense\",\n",
    "        \"yemeden\",\"yamadan\",\"emeden\",\"amadan\",\n",
    "        \"maca\",\"mece\",\n",
    "        \"mazlık\",\"mazlığ\",\"mezlik\",\"mezliğ\",\n",
    "        \"casına\",\"çasına\",\"cesine\",\"çesine\",\n",
    "        \"esiye\",\"asıya\",\"yesiye\",\"yasıya\",\n",
    "        \"dir\",\"tir\",\"dır\",\"tır\",\"dur\",\"tur\",\"dür\",\"tür\",\n",
    "    }\n",
    "    \n",
    "    with open('C:/endings.txt', 'r', encoding='utf-8') as endings_file:\n",
    "        suffixes = [line.strip() for line in endings_file]\n",
    "\n",
    "    eksizEslesme, suffix_categories,eksizEslesme2,suffix_categories2 = process_dataset(base_word, [])\n",
    "        \n",
    "    if any('CL_FIIL' in category for category in suffix_categories):\n",
    "        #return 'Bu bir fiil oldugu icin burasi lokmandan gelecek'\n",
    "        return deger(base_word)\n",
    "\n",
    "    all_suffixes = find_suffixes(base_word, suffixes)\n",
    "\n",
    "    if eksizEslesme:\n",
    "        return ''.join(eksizEslesme), ''.join(suffix_categories)\n",
    "    elif all_suffixes:\n",
    "        result_suffixes = []\n",
    "        \n",
    "        for suffix in sorted(all_suffixes, key=len, reverse=True):\n",
    "            print(suffix,\"yakuk\")\n",
    "            remaining_word = base_word[:-len(suffix)]\n",
    "            print(remaining_word,\"bu da remaining word\")\n",
    "            if remaining_word.endswith((\"iyor\", \"iyo\", \"üyor\", \"üyo\")):\n",
    "                if(not fiilDondu):\n",
    "                    remaining_word = remaining_word.replace(\"iyor\", \"e\").replace(\"iyo\", \"e\").replace(\"üyor\", \"e\").replace(\"üyo\", \"e\", 1)\n",
    "                    print(remaining_word,\"yeni remaining word bu\")\n",
    "            if remaining_word.endswith((\"ıyor\", \"ıyo\", \"uyor\", \"uyo\")):\n",
    "                if(not fiilDondu):\n",
    "                    remaining_word = remaining_word.replace(\"ıyor\", \"a\").replace(\"ıyo\", \"a\").replace(\"uyor\", \"a\").replace(\"uyo\", \"a\", 1)\n",
    "                    print(remaining_word,\"yeni remaining word bu\")\n",
    "                    \n",
    "            if remaining_word in unlu_dusmesi:\n",
    "                remaining_word = unlu_dusmesi[remaining_word]\n",
    "            \n",
    "            matches, suffix_categories,matches2,suffix_categories2 = process_dataset(remaining_word, [suffix])\n",
    "            print(matches,\"bu matches\")\n",
    "            print(suffix_categories,\"bu sofik kategori\")\n",
    "            print(matches2,\"BU MATCHES2\")\n",
    "\n",
    "            if matches:\n",
    "\n",
    "                if any(any(keyword in category for keyword in ('CL_FIIL', 'Apply', 'Become')) for category in suffix_categories):\n",
    "                    #return 'Bu bir fiil oldugu icin burasi lokmandan gelecek'\n",
    "                    print(\"GIRDIK BURAYA\")\n",
    "                    print(result_suffixes,\"yauk result_suffixes\")\n",
    "                    if(ihtimalVarmi):\n",
    "                            result_suffixes.extend('<br>')\n",
    "                            result_suffixes.extend(f' <br> Eşleşen diğer bir seçenek: ')\n",
    "                            result_suffixes.extend(deger(base_word))\n",
    "                            result_suffixes.extend(' ')\n",
    "                            tempBaseWord=base_word\n",
    "                            fiilDondu=True\n",
    "                            return ''.join(result_suffixes)\n",
    "                        \n",
    "                    else:\n",
    "                        ihtimalVarmi = True\n",
    "                        print(deger(base_word),\"SFAJGJSDAGJASDLGASDKGJKSDLJL\")\n",
    "                        result_suffixes.extend(deger(base_word))\n",
    "                        result_suffixes.extend(' ')\n",
    "                        result_suffixes.extend('<br>')\n",
    "                        fiilDondu=True\n",
    "                        return ''.join(result_suffixes)\n",
    "                    \n",
    "                else:\n",
    "                    print(\"SIGARA SARDIM\")\n",
    "                    if(ihtimalVarmi):\n",
    "                        #if all(\"Trap_Eki\" not in suffix_category for suffix_category in suffix_categories):\n",
    "                            result_suffixes.extend('<br>')\n",
    "                            result_suffixes.extend(f' <br> Eşleşen diğer bir seçenek: ')\n",
    "                            result_suffixes.extend(matches)\n",
    "                            result_suffixes.extend(suffix_categories)\n",
    "                            result_suffixes.extend(' ')\n",
    "                            if(matches != matches2 and suffix_categories != suffix_categories2):\n",
    "                                result_suffixes.extend('<br>')\n",
    "                                result_suffixes.extend('<br>')\n",
    "                                result_suffixes.extend(matches2)\n",
    "                                result_suffixes.extend(suffix_categories2)\n",
    "                                result_suffixes.extend(' ')\n",
    "                    else:\n",
    "                        #if all(\"Trap_Eki\" not in suffix_category for suffix_category in suffix_categories):\n",
    "                        ihtimalVarmi=True\n",
    "                        result_suffixes.extend(matches)\n",
    "                        result_suffixes.extend(suffix_categories)\n",
    "                        result_suffixes.extend(' ')\n",
    "                        if(matches != matches2 and suffix_categories != suffix_categories2):\n",
    "                            result_suffixes.extend('<br>')\n",
    "                            result_suffixes.extend('<br>')\n",
    "                            result_suffixes.extend(matches2)\n",
    "                            result_suffixes.extend(suffix_categories2)\n",
    "                            result_suffixes.extend(' ')\n",
    "                        \n",
    "\n",
    "            else:\n",
    "                #kontrolVar=deger(base_word)\n",
    "                #if(kontrolVar):\n",
    "                    #result_suffixes.extend(deger(base_word))\n",
    "               # else:\n",
    "                    #return f\"{base_word} kelimesi için eşleşme bulunamadı.\"\n",
    "                print(\"sebze\")\n",
    "\n",
    "                        \n",
    "\n",
    "        return ''.join(result_suffixes)\n",
    "\n",
    "        \n",
    "        \n",
    "@app.route(\"/\", methods=[\"GET\", \"POST\"])\n",
    "def index():\n",
    "    results = []\n",
    "    root_list_path = 'C:/dataset.txt'\n",
    "    suffix_list_path = 'C:/endings.txt'\n",
    "    with open(root_list_path, 'r', encoding='utf-8') as root_file:\n",
    "        root_list = [line.strip().split()[0] for line in root_file]\n",
    "\n",
    "    with open(suffix_list_path, 'r', encoding='utf-8') as suffix_file:\n",
    "        suffix_list = [line.strip() for line in suffix_file]\n",
    "\n",
    "    if request.method == \"POST\":\n",
    "        if request.form.get(\"generate_random_sentence\"):\n",
    "            user_input = get_random_sentence('C:/turkceCumleler.txt')\n",
    "        else:\n",
    "            user_input = request.form.get(\"user_input\", \"\")\n",
    "    else:\n",
    "        user_input = \"\"\n",
    "\n",
    "    user_input = user_input.lower()\n",
    "    user_input = user_input.replace(\"'\", ' ').replace('\"', '').replace(',',' ').replace(';',' ')\n",
    "    normalizasyonAktif = request.form.get(\"normalizasyonAktif\")\n",
    "\n",
    "    sentences = sentence_splitter(user_input)\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokens = tokenizer(sentence)\n",
    "        processed_tokens = []\n",
    "\n",
    "        for token in tokens:\n",
    "            if normalizasyonAktif == \"True\":\n",
    "                corrected_word = correct_input(token, root_list, suffix_list)\n",
    "                result = find_and_process_suffixes_v2(corrected_word, 'C:/dataset.txt')\n",
    "                processed_tokens.append((corrected_word, result))\n",
    "            elif normalizasyonAktif == \"False\":\n",
    "                result = find_and_process_suffixes_v2(token, 'C:/dataset.txt')\n",
    "                processed_tokens.append((token, result))\n",
    "\n",
    "        results.append(processed_tokens)\n",
    "            \n",
    "    return render_template(\"index.html\", results=results, user_input=user_input)\n",
    "\n",
    "\n",
    "@app.route('/developers')\n",
    "def developers():\n",
    "    return render_template('developers.html')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True, use_reloader=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "625aa90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i̇\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "kelime=\"İ\"\n",
    "print(kelime.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "bcd468c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: alacam, Yanlış - Düzeltme: alacam\n",
      "Input: gidiyom, Yanlış - Düzeltme: gidiıyorum\n",
      "Input: okuyon, Yanlış - Düzeltme: okuyın\n",
      "Input: anlatıyon, Yanlış - Düzeltme: anlatıyın\n",
      "Input: geliyon, Yanlış - Düzeltme: geliyor\n",
      "Input: yapıyon, Yanlış - Düzeltme: yapıyın\n",
      "Input: görüyon, Yanlış - Düzeltme: görüyın\n",
      "Input: alıyom, Doğru - Düzeltme:  alıyorum\n",
      "Input: öpüyoz, Doğru - Düzeltme:  öpüyoruz\n",
      "Input: gelmıyon, Yanlış - Düzeltme: gelmıyor\n",
      "Input: okuma, Yanlış - Düzeltme: okuma\n",
      "Input: gelmeyon, Yanlış - Düzeltme: gelmeyın\n",
      "Input: yazıyosun, Yanlış - Düzeltme: yazııyorsun\n",
      "Input: geldim, Doğru - Düzeltme:  geldim\n",
      "Input: alıştık, Doğru - Düzeltme:  alıştık\n",
      "Input: bakıyon, Yanlış - Düzeltme: bakıyın\n",
      "Input: gelelim, Doğru - Düzeltme:  gelelim\n",
      "Input: geleceksin, Doğru - Düzeltme:  geleceksin\n",
      "Input: yapalım, Doğru - Düzeltme:  yapalım\n",
      "Input: anlatsam, Doğru - Düzeltme:  anlatsam\n",
      "Input: yapmamalısın, Doğru - Düzeltme:  yapmamalısın\n",
      "Input: yapmalısın, Doğru - Düzeltme:  yapmalısın\n",
      "Input: geldiysem, Doğru - Düzeltme:  geldiysem\n",
      "Input: geldiysek, Doğru - Düzeltme:  geldiysek\n",
      "Input: geldikten, Doğru - Düzeltme:  geldikten\n",
      "Input: yapıyosak, Yanlış - Düzeltme: yapıysak\n",
      "Input: geldiysek, Doğru - Düzeltme:  geldiysek\n",
      "Input: yapıyorsan, Yanlış - Düzeltme: yapııyorsan\n",
      "Input: geldiysen, Doğru - Düzeltme:  geldiysen\n",
      "Input: geldiyseniz, Doğru - Düzeltme:  geldiyseniz\n",
      "Input: yapıyorsanız, Yanlış - Düzeltme: yapııyorsanız\n",
      "Input: yapıyorsak, Yanlış - Düzeltme: yapııyorsak\n",
      "Input: geldiysek, Doğru - Düzeltme:  geldiysek\n",
      "Input: geldik, Doğru - Düzeltme:  geldik\n",
      "Input: geldiler, Doğru - Düzeltme:  geldiler\n",
      "Input: gelmeliyim, Doğru - Düzeltme:  gelmeliyim\n",
      "Input: geldilerse, Doğru - Düzeltme:  geldilerse\n",
      "Input: geldiklerinde, Doğru - Düzeltme:  geldiklerinde\n",
      "Input: geldiklerini, Doğru - Düzeltme:  geldiklerini\n",
      "Input: geldiklerim, Doğru - Düzeltme:  geldiklerim\n",
      "Input: geldiklerime, Doğru - Düzeltme:  geldiklerime\n",
      "Input: geldiklerimiz, Doğru - Düzeltme:  geldiklerimiz\n",
      "Input: geldikleriniz, Doğru - Düzeltme:  geldikleriniz\n",
      "Input: geldiklerine, Doğru - Düzeltme:  geldiklerine\n",
      "Input: geldiklerimizin, Doğru - Düzeltme:  geldiklerimizin\n",
      "Input: geldiklerinizin, Doğru - Düzeltme:  geldiklerinizin\n",
      "Input: geldiklerinizi, Doğru - Düzeltme:  geldiklerinizi\n",
      "Input: geldiklerinizden, Doğru - Düzeltme:  geldiklerinizden\n"
     ]
    }
   ],
   "source": [
    "from difflib import get_close_matches\n",
    "\n",
    "def find_longest_root(input_word, root_list):\n",
    "    longest_root = ''\n",
    "    for root in root_list:\n",
    "        if input_word.startswith(root) and len(root) > len(longest_root):\n",
    "            longest_root = root\n",
    "    \n",
    "    return longest_root\n",
    "\n",
    "def correct_input(input_word, root_list, suffix_list):\n",
    "    longest_root = find_longest_root(input_word, root_list)\n",
    "    geriyeKalanEk = input_word[len(longest_root):]\n",
    "        \n",
    "    if geriyeKalanEk:\n",
    "        suffix_correction = get_close_matches(geriyeKalanEk, suffix_list, n=1, cutoff=0.6)\n",
    "        corrected_suffix = suffix_correction[0] if suffix_correction else geriyeKalanEk\n",
    "        corrected_word = longest_root + corrected_suffix\n",
    "    else:\n",
    "        corrected_word = input_word\n",
    "\n",
    "    return corrected_word\n",
    "\n",
    "root_list_path = 'C:/dataset.txt'\n",
    "suffix_list_path = 'C:/endings.txt'\n",
    "\n",
    "with open(root_list_path, 'r', encoding='utf-8') as root_file:\n",
    "    root_list = [line.strip().split()[0] for line in root_file]\n",
    "\n",
    "with open(suffix_list_path, 'r', encoding='utf-8') as suffix_file:\n",
    "    suffix_list = [line.strip() for line in suffix_file]\n",
    "\n",
    "simulated_inputs = [\n",
    "    \"alacam\", \"gidiyom\", \"okuyon\", \"anlatıyon\", \"geliyon\", \n",
    "    \"yapıyon\", \"görüyon\", \"alıyom\", \"öpüyoz\", \"gelmıyon\",\n",
    "    \"okuma\", \"gelmeyon\", \"yazıyosun\", \"geldim\", \"alıştık\",\n",
    "    \"bakıyon\", \"gelelim\", \"geleceksin\", \"yapalım\", \"anlatsam\",\n",
    "    \"yapmamalısın\", \"yapmalısın\", \"geldiysem\", \"geldiysek\", \n",
    "    \"geldikten\", \"yapıyosak\", \"geldiysek\", \"yapıyorsan\", \"geldiysen\",\n",
    "    \"geldiyseniz\", \"yapıyorsanız\", \"yapıyorsak\", \"geldiysek\",\n",
    "    \"geldik\", \"geldiler\", \"gelmeliyim\", \"geldilerse\", \"geldiklerinde\",\n",
    "    \"geldiklerini\", \"geldiklerim\", \"geldiklerime\", \"geldiklerimiz\", \n",
    "    \"geldikleriniz\", \"geldiklerine\", \"geldiklerimizin\", \"geldiklerinizin\",\n",
    "    \"geldiklerinizi\", \"geldiklerinizden\"\n",
    "]\n",
    "\n",
    "dogru_inputs = [\n",
    "    \"alacağım\", \"giriyorum\", \"okuyorsun\", \"anlatıyorsun\", \"geliyorsun\", \n",
    "    \"yapıyorsun\", \"görüyorsun\", \"alıyorum\", \"öpüyoruz\", \"gelmiyorsun\", \n",
    "    \"okumam\", \"gelmiyorsun\", \"yazıyorsun\", \"geldim\", \"alıştık\", \n",
    "    \"bakıyorsun\", \"gelelim\", \"geleceksin\", \"yapalım\", \"anlatsam\", \n",
    "    \"yapmamalısın\", \"yapmalısın\", \"geldiysem\", \"geldiysek\", \n",
    "    \"geldikten\", \"yapıyorsak\", \"geldiysek\", \"yapıyorsan\", \"geldiysen\", \n",
    "    \"geldiyseniz\", \"yapıyorsanız\", \"yapıyorsak\", \"geldiysek\", \n",
    "    \"geldik\", \"geldiler\", \"gelmeliyim\", \"geldilerse\", \"geldiklerinde\", \n",
    "    \"geldiklerini\", \"geldiklerim\", \"geldiklerime\", \"geldiklerimiz\", \n",
    "    \"geldikleriniz\", \"geldiklerine\", \"geldiklerimizin\", \"geldiklerinizin\", \n",
    "    \"geldiklerinizi\", \"geldiklerinizden\"\n",
    "]\n",
    "sayac=0\n",
    "for input_word in simulated_inputs:\n",
    "    corrected_word = correct_input(input_word, root_list, suffix_list)\n",
    "    \n",
    "    if corrected_word == dogru_inputs[sayac]:\n",
    "        print(f\"Input: {input_word}, Doğru - Düzeltme:  {corrected_word}\")\n",
    "    else:\n",
    "        print(f\"Input: {input_word}, Yanlış - Düzeltme: {corrected_word}\")\n",
    "    sayac=sayac+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5971bd9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a104f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0ad3ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parcala_ve_kontrol_et(input_str):\n",
    "    ek_dosyalar = {\n",
    "        \n",
    "        \"Opt\":[\"yalım\",\"yelim\",\"alım\",\"elim\"],\n",
    "        \"Opt+A2sg\":[\"yasın\",\"yesin\",\"asın\",\"esin\"],\n",
    "        \"Opt+A2pl\":[\"yasınız\",\"yesiniz\",\"asınız\",\"esiniz\"],\n",
    "        \"Opt+A1sg\":[\"yayım\",\"yeyim\",\"ayım\",\"eyim\"],\n",
    "        ## noun suffixes\n",
    "        \"Pl\": [\"lar\", \"ler\"],\n",
    "        ## case suffixes\n",
    "        \"Dat\": [\"ya\",\"ye\",\"e\",\"a\",\"na\",\"ne\"],\n",
    "        \"Loc\": [\"da\", \"de\", \"ta\", \"te\",\"nde\",\"nda\"],\n",
    "        \"Abl\": [\"dan\", \"den\", \"tan\", \"ten\",\"ndan\",\"nden\"],\n",
    "        \"Gen\": [\"nın\", \"nin\",\"nun\",\"nün\",\"ın\",\"in\",\"un\", \"ün\",\"yın\",\"yin\",\"yun\",\"yün\",\"ım\",\"im\",\"um\",\"üm\",\"m\"],\n",
    "        \"Acc\": [\"yı\", \"yi\", \"yu\", \"yü\", \"ı\", \"i\", \"u\",\"ü\",\"nı\",\"ni\",\"nu\",\"nü\"],\n",
    "        \"Inst\": [\"yla\", \"yle\",\"la\",\"le\"],\n",
    "        \"P1sg\": [\"ım\",\"im\",\"um\",\"üm\",\"m\",\"yum\"],\n",
    "        \"P2sg\": [\"ın\",\"in\",\"un\",\"ün\",\"n\",\"yun\"],\n",
    "        \"P3sg\": [\"sı\", \"si\", \"su\", \"sü\",\"ı\",\"i\",\"u\",\"ü\",\"yu\"],\n",
    "        \"P1pl\": [\"ımız\", \"imiz\", \"umuz\", \"ümüz\",\"mız\",\"miz\",\"muz\",\"müz\",\"yumuz\"],\n",
    "        \"P2pl\": [\"ınız\",\"iniz\",\"unuz\",\"ünüz\",\"nız\",\"niz\",\"nuz\",\"nüz\",\"yunuz\"],\n",
    "        \"P3pl\": [\"ı\", \"i\",\"u\",\"ü\",\"ları\",\"leri\"],\n",
    "        \"Dim\": [\"cık\", \"cik\",\"cuk\",\"cük\",\"çık\",\"çik\",\"çuk\",\"çük\",\"cığ\",\"ciğ\",\"cuğ\",\"cüğ\",\"çığ\",\"çiğ\",\"çuğ\",\"çüğ\"],\n",
    "        \"Dim2\": [\"cağız\", \"ceğiz\"],\n",
    "        \"With\":[\"lı\",\"li\",\"lu\",\"lü\"],\n",
    "        \"Without\": [\"sız\", \"siz\",\"suz\",\"süz\"],\n",
    "        \"Since\":[\"dır\",\"dir\",\"dur\",\"dür\",\"tır\",\"tir\",\"tur\",\"tür\"],\n",
    "        \"Related\": [\"sal\", \"sel\"],\n",
    "        \"Ness\":[\"lık\",\"lik\",\"luk\",\"lük\",\"lığ\",\"liğ\",\"luğ\",\"lüğ\"],\n",
    "        \"Acquire\": [\"lan\", \"len\"],\n",
    "        \"Apply\":[\"la\",\"le\"],\n",
    "        \"Resemb\": [\"ımsı\", \"imsi\",\"umsu\",\"ümsü\",\"msı\",\"msi\",\"msu\",\"msü\"],\n",
    "        \"Rel\":[\"kı\",\"ki\",\"ku\",\"kü\"],\n",
    "        \"By\": [\"ca\", \"ce\"],\n",
    "        \"Cmp\":[\"ca\",\"ce\"],\n",
    "        \"Agt\":[\"cı\",\"ci\",\"cu\",\"cü\",\"yıcı\",\"yici\",\"yucu\",\"yücü\"],\n",
    "        ## Adjective Suffixes\n",
    "        \"Become\":[\"laş\",\"leş\"],\n",
    "        \"Ly\":[\"ca\",\"ce\"],\n",
    "        \"Quite\":[\"ca\",\"ce\"],\n",
    "        ## number-person agreement suffixes\n",
    "        \"A1sg\": [\"yım\", \"yim\",\"yum\",\"yüm\",\"m\",\"ım\",\"im\",\"um\",\"üm\"],\n",
    "        \"A2sg\":[\"sın\",\"sin\",\"sun\",\"sün\",\"n\",\"ın\",\"in\",\"un\",\"ün\"],\n",
    "        \"A3sg\":[\"\"],\n",
    "        \"A1pl\": [\"yız\",\"yiz\",\"k\",\"yuz\",\"yüz\",\"ız\",\"iz\",\"uz\",\"üz\"],\n",
    "        \"A2pl\":[\"sınız\",\"siniz\",\"sunuz\",\"sünüz\",\"nız\",\"niz\",\"nuz\",\"nüz\",\"ınız\",\"iniz\",\"unuz\",\"ünüz\"],\n",
    "        \"A3pl\": [\"lar\",\"ler\"],\n",
    "        ### Numeral suffixes\n",
    "        \"Grouping\": [\"şar\",\"şer\",\"ız\",\"iz\",\"ar\",\"er\"],\n",
    "        \"Ordinal\":[\"ıncı\",\"inci\",\"uncu\",\"üncü\",\"ncı\",\"nci\",\"ncu\",\"ncü\"],\n",
    "        \"Division\":[\"da\",\"de\",\"ta\",\"te\"],\n",
    "        ### verbal suffixes\n",
    "        \"Aor\":[\"ır,ir,ur,ür,ar,er,z\"],\n",
    "        \"Prog\":[\"ıyor\",\"iyor\",\"uyor\",\"üyor\"],\n",
    "        \"Prog2\":[\"makta\",\"mekte\"],\n",
    "        \"Fut\":[\"yacak\",\"yecek\",\"yacağ\",\"yeceğ\",\"acak\",\"ecek\",\"acağ\",\"eceğ\"],\n",
    "        \"Past\":[\"dı\",\"di\",\"du\",\"dü\",\"tı\",\"ti\",\"tu\",\"tü\"],\n",
    "        \"Evid\":[\"mış\",\"miş\",\"muş\",\"müş\"],\n",
    "        \"Neg\":[\"ma,me,m\"],\n",
    "        \"Cond\":[\"se\",\"sa\"],\n",
    "        \"Necess\":[\"meli\",\"malı\"],\n",
    "        \"Opt\":[\"ye\",\"ya\",\"e\",\"a\"],\n",
    "        \"Imp\":[\"sın\",\"sin\",\"sun\",\"sün\",\"yın\",\"yin\",\"yun\",\"yün\",\"ın\",\"in\",\"un\",\"ün\",\"sınlar\",\"sinler\",\"sünler\",\"sunlar\",\"yınız\",\"yiniz\",\"yunuz\",\"yünüz\",\"ınız\",\"iniz\",\"unuz\",\"ünüz\"],\n",
    "        ### copular suffixes\n",
    "        \"PastCop\":[\"ydı\",\"ydi\",\"ydu\",\"ydü\",\"dı\",\"di\",\"du\",\"dü\"],\n",
    "        \"EvidCop\":[\"ymış\",\"ymiş\",\"ymuş\",\"ymüş\",\"mış\",\"miş\",\"muş\",\"müş\"],\n",
    "        \"CondCop\":[\"ysa\",\"yse\",\"sa\",\"se\"],\n",
    "        \"While\":[\"yken\",\"ken\"],\n",
    "        \"Cop\":[\"dır\",\"dir\",\"dur\",\"dür\",\"tır\",\"tir\",\"tur\",\"tür\"],\n",
    "        ### Derivational Suffixes\n",
    "        \"Pass\":[\"ıl\",\"il\",\"ul\",\"ül\",\"ın\",\"in\",\"un\",\"ün\",\"n\",\"nıl\",\"nil\",\"nul\",\"nül\"],\n",
    "        \"Caus\":[\"dır\",\"dir\",\"dur\",\"dür\",\"tır\",\"tir\",\"tur\",\"tür\",\"t\"],\n",
    "        \"Recip\":[\"yış\",\"yiş\",\"yuş\",\"yüş\",\"ış\",\"iş\",\"uş\",\"üş\",\"ş\"],\n",
    "        \"Reflex\":[\"ın\",\"in\",\"un\",\"ün\"],\n",
    "        \"Abil\":[\"yabıl\",\"yabil\",\"yabul\",\"yabül\",\"yebil\",\"yebul\",\"yebül\",\"abıl\",\"ebil\",\"abul\",\"abül\",\"ebıl\",\"ebul\",\"ebül\"],\n",
    "        \"NegAbil\":[\"yama\",\"yeme\",\"ama\",\"eme\"],\n",
    "        \"Cont\":[\"yadur\",\"yedur\",\"adur\",\"edur\"],\n",
    "        \"EverSince\":[\"yagel\",\"yegel\",\"agel\",\"egel\"],\n",
    "        \"Cont2\":[\"yagör\",\"yegör\",\"agör\",\"egör\"],\n",
    "        \"Almost\":[\"yayaz\",\"yeyez\",\"yayez\",\"yeyaz\",\"ayaz\",\"eyez\",\"ayez\",\"eyaz\"],\n",
    "        \"Hastily\":[\"yıver\",\"yiver\",\"ıver\",\"iver\"],\n",
    "        \"Stay\":[\"yakal\",\"yekal\",\"akal\",\"ekal\"],\n",
    "        \"Inf\":[\"mak\",\"mek\"],\n",
    "        \"Inf2\":[\"ma\",\"me\"],\n",
    "        \"Inf3\":[\"yış\",\"yiş\",\"yuş\",\"yüş\",\"ış\",\"iş\",\"uş\",\"üş\"],\n",
    "        \"FutPart\":[\"yacak\",\"yecek\",\"yacağ\",\"yeceğ\",\"acak\",\"ecek\",\"acağ\",\"eceğ\"],\n",
    "        \"EvidPart\":[\"mış\",\"miş\",\"muş\",\"müş\"],\n",
    "        \"PastPart\":[\"dık\",\"dik\",\"duk\",\"dük\",\"tık\",\"tik\",\"tuk\",\"tük\",\"dığ\",\"diğ\",\"duğ\",\"düğ\",\"tığ\",\"tiğ\",\"tuğ\",\"tüğ\"],\n",
    "        \"PresPart\":[\"yan\",\"yen\",\"an\",\"en\"],\n",
    "        \"AorPart\":[\"ır\",\"ir\",\"ur\",\"ür\",\"ar\",\"er\",\"r\"],\n",
    "        \"AfterDoing\":[\"yıp\",\"yip\",\"yup\",\"yüp\",\"ıp\",\"ip\",\"up\",\"üp\"],\n",
    "        \"Agt\":[\"yıcı\",\"yici\",\"yucu\",\"yücü\",\"ıcı\",\"ici\",\"ucu\",\"ücü\"],\n",
    "        \"FeelLike\":[\"yası\",\"yesi\",\"ası\",\"esi\"],\n",
    "        \"SinceDoing\":[\"yalı\",\"yeli\",\"alı\",\"eli\"],\n",
    "        \"AsLongAs\":[\"dıkca\",\"dikce\",\"dıkça\",\"dikçe\",\"tıkca\",\"tikce\",\"tıkça\",\"tikçe\"],\n",
    "        \"JustAfter\":[\"yınca\",\"yince\",\"ınca\",\"ince\"],\n",
    "        \"ByDoing\":[\"yarak\",\"yerek\",\"arak\",\"erek\"],\n",
    "        \"WithoutDoing\":[\"madan\",\"meden\",\"adan\",\"eden\"],\n",
    "        \"WithoutDoing2\":[\"maksızın\",\"meksizin\",\"aksızın\",\"eksizin\"],\n",
    "        \"RatherThanInsteadOfDoing\":[\"maktansa\",\"mektense\",\"aktansa\",\"ektense\"],\n",
    "        \"UnableToDo\":[\"yamadan\",\"yemeden\",\"amadan\",\"emeden\"],\n",
    "        \"ActOf\":[\"maca\",\"mece\"],\n",
    "        \"NotState\":[\"mazlık\",\"mezlik\",\"mazlığ\",\"mezliğ\"],\n",
    "        \"AsIf\":[\"casına\",\"cesine\",\"çasına\",\"çesine\"],\n",
    "        \"Adamantly\":[\"yasıya\",\"yesiye\",\"asıya\",\"esiye\"],\n",
    "\n",
    "    }\n",
    "    ozel_durumlar = {\n",
    "        #\"ımız\": [\"ım\", \"ız\"],\n",
    "        #\"sinler\":[\"s\",\"i\",\"n\",\"ler\"],\n",
    "        #\"leri\":[\"ler\",\"i\"],\n",
    "        #\"sının\":[\"sı\",\"nın\"],\n",
    "        #\"elim\":[\"e\",\"lim\"],\n",
    "\n",
    "    }\n",
    "\n",
    "    sonuclar = []\n",
    "    en_uzun_ekler=[]\n",
    "\n",
    "    while input_str:\n",
    "        bulundu = False\n",
    "        ozelBulundu = False\n",
    "        en_uzun_ek = \"\"\n",
    "        ek_adi_list = []  \n",
    "\n",
    "        for ozel_ek, ayrilma_sekli in ozel_durumlar.items():\n",
    "            if input_str.startswith(ozel_ek):\n",
    "                for ayrilma in ayrilma_sekli:\n",
    "                    for ad, ekler in ek_dosyalar.items():\n",
    "                        if ayrilma in ekler and input_str.startswith(ayrilma):\n",
    "                            en_uzun_ek = ayrilma\n",
    "                            ek_adi = f\"({ad})\"\n",
    "                            ozelBulundu = True\n",
    "                            if ozelBulundu:\n",
    "                                input_str = input_str[len(en_uzun_ek):].strip()\n",
    "                                ek_adi_list.append(ek_adi)\n",
    "\n",
    "        if not ozelBulundu:\n",
    "            for ad, ekler in ek_dosyalar.items():\n",
    "                for ek in ekler:\n",
    "                    if (input_str.startswith(ek) or input_str.endswith(ek)) and len(ek) >= len(en_uzun_ek):\n",
    "                        print(ek,\"bu da ek\")\n",
    "                        en_uzun_ek = ek\n",
    "                        ek_adi = f\"({ad})\"\n",
    "                        bulundu = True\n",
    "                        \n",
    "                \n",
    "                \n",
    "            if bulundu:\n",
    "                input_str = input_str[len(en_uzun_ek):].strip()\n",
    "                for ad,ekler in ek_dosyalar.items():\n",
    "\n",
    "                    if en_uzun_ek in ekler:\n",
    "                        #print(en_uzun_ek,ekler,\"yakup\")\n",
    "                        ek_adi_list.append(f\"({ad})\")\n",
    "                    \n",
    "                #ek_adi_list.append(ek_adi)\n",
    "            else:\n",
    "                ek_adi_list.append(f\"({input_str})\")\n",
    "\n",
    "        if ek_adi_list:\n",
    "            sonuclar.append((f\"-{en_uzun_ek} \", ek_adi_list))\n",
    "\n",
    "    return sonuclar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5e107090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bir kelime girin: asdjgsdajgiyom\n",
      "as yakup longest root\n",
      "asdjgsdajgiyom bu input_word\n",
      "1 e bu da longest suffix uzunlugu\n",
      "asdjgsdajgiyo bu da rootun ici\n",
      "asdjgsdajgiyo m yakup root ve longest suffix\n",
      "asm asm yakup corrected word ve corrected word 2\n",
      "Düzeltme önerisi: asm\n"
     ]
    }
   ],
   "source": [
    "from difflib import get_close_matches\n",
    "\n",
    "def find_longest_root(input_word, root_list):\n",
    "    longest_root = ''\n",
    "    for root in root_list:\n",
    "        if input_word.startswith(root) and len(root) > len(longest_root):\n",
    "            longest_root = root\n",
    "    \n",
    "    print(longest_root,\"yakup longest root\")\n",
    "    return longest_root\n",
    "\n",
    "def separate_root_and_suffix(input_word, suffix_list):\n",
    "    longest_suffix = ''\n",
    "    for suffix in suffix_list:\n",
    "        if input_word.endswith(suffix) and len(suffix) > len(longest_suffix):\n",
    "            longest_suffix = suffix\n",
    "\n",
    "    if longest_suffix:\n",
    "        \n",
    "        root = input_word[:-len(longest_suffix)]\n",
    "        print(input_word,\"bu input_word\")\n",
    "        print(len(longest_suffix),\"e bu da longest suffix uzunlugu\")\n",
    "        print(input_word[:-len(longest_suffix)],\"bu da rootun ici\")\n",
    "        print(root,longest_suffix,\"yakup root ve longest suffix\")\n",
    "        \n",
    "        return root, longest_suffix\n",
    "\n",
    "    return input_word, ''\n",
    "\n",
    "def correct_input(input_word, root_list, suffix_list):\n",
    "    longest_root = find_longest_root(input_word, root_list)\n",
    "    root,longest_suffix = separate_root_and_suffix(input_word,suffix_list)\n",
    "    eksizKelime = input_word[:-len(longest_suffix)]\n",
    "    sadeceEkliKelime = input_word[len(longest_root):]\n",
    "    \n",
    "    if longest_root == eksizKelime:\n",
    "        print(\"demek ki kelimenin kök doğru\")\n",
    "        return input_word\n",
    "    else:\n",
    "        root_correction = get_close_matches(longest_root, root_list, n=1, cutoff=0.6)\n",
    "        corrected_root = root_correction[0] if root_correction else longest_root\n",
    "        root_correction2 = get_close_matches(root, root_list, n=1, cutoff=0.6)\n",
    "        corrected_root2 = root_correction[0] if root_correction else longest_root\n",
    "\n",
    "    if sadeceEkliKelime == longest_suffix:\n",
    "        print(\"ekler dogru\")\n",
    "    else:\n",
    "        suffix_correction = get_close_matches(longest_suffix, suffix_list, n=1, cutoff=0.8)\n",
    "        corrected_suffix = suffix_correction[0] if suffix_correction else suffix\n",
    "        suffix_correction2 = get_close_matches(sadeceEkliKelime, suffix_list, n=1, cutoff=0.8)\n",
    "        corrected_suffix2 = suffix_correction[0] if suffix_correction else suffix\n",
    "   \n",
    "\n",
    "    \n",
    "    corrected_word = corrected_root + corrected_suffix\n",
    "    corrected_word2 = corrected_root2 + corrected_suffix2\n",
    "\n",
    "    print(corrected_word,corrected_word2,\"yakup corrected word ve corrected word 2\")\n",
    "    return corrected_word\n",
    "\n",
    "root_list_path = 'C:/dataset.txt'\n",
    "suffix_list_path = 'C:/endings.txt'\n",
    "\n",
    "with open(root_list_path, 'r', encoding='utf-8') as root_file:\n",
    "    root_list = [line.strip().split()[0] for line in root_file]\n",
    "\n",
    "with open(suffix_list_path, 'r', encoding='utf-8') as suffix_file:\n",
    "    suffix_list = [line.strip() for line in suffix_file]\n",
    "\n",
    "user_input = input(\"Bir kelime girin: \").lower()\n",
    "\n",
    "corrected_word = correct_input(user_input, root_list, suffix_list)\n",
    "print(f\"Düzeltme önerisi: {corrected_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7e5d8655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bir kelime girin: ayağındaki\n",
      "ğındaki geriye kalan ek\n",
      "Düzeltme önerisi: ayaındaki\n"
     ]
    }
   ],
   "source": [
    "from difflib import get_close_matches\n",
    "\n",
    "def find_longest_root(input_word, root_list):\n",
    "    longest_root = ''\n",
    "    for root in root_list:\n",
    "        if input_word.startswith(root) and len(root) > len(longest_root):\n",
    "            longest_root = root\n",
    "    \n",
    "    return longest_root\n",
    "\n",
    "def correct_input(input_word, root_list, suffix_list):\n",
    "    longest_root = find_longest_root(input_word, root_list)\n",
    "    geriyeKalanEk = input_word[len(longest_root):]\n",
    "    print(geriyeKalanEk,\"geriye kalan ek\")\n",
    "        \n",
    "    if(geriyeKalanEk):\n",
    "        suffix_correction = get_close_matches(geriyeKalanEk, suffix_list, n=1, cutoff=0.4)\n",
    "        corrected_suffix = suffix_correction[0] if suffix_correction else geriyeKalanEk\n",
    "        corrected_word = longest_root + corrected_suffix\n",
    "        \n",
    "    else:\n",
    "        corrected_word = input_word\n",
    "\n",
    "    return corrected_word\n",
    "\n",
    "root_list_path = 'C:/dataset.txt'\n",
    "suffix_list_path = 'C:/endings.txt'\n",
    "\n",
    "with open(root_list_path, 'r', encoding='utf-8') as root_file:\n",
    "    root_list = [line.strip().split()[0] for line in root_file]\n",
    "\n",
    "with open(suffix_list_path, 'r', encoding='utf-8') as suffix_file:\n",
    "    suffix_list = [line.strip() for line in suffix_file]\n",
    "\n",
    "user_input = input(\"Bir kelime girin: \").lower()\n",
    "\n",
    "corrected_word = correct_input(user_input, root_list, suffix_list)\n",
    "print(f\"Düzeltme önerisi: {corrected_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3aa61281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kupba\n"
     ]
    }
   ],
   "source": [
    "kelime=\"yakupba\"\n",
    "ek=\"ba\"\n",
    "print(kelime[len(ek):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391e10b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from difflib import get_close_matches\n",
    "\n",
    "class WordCorrector:\n",
    "    def __init__(self, word_list):\n",
    "        self.word_list = word_list\n",
    "\n",
    "    def correct_input(self, input_word):\n",
    "        # Daha karmaşık bir modelle değiştirilebilir\n",
    "        closest_match = get_close_matches(input_word, self.word_list, n=1, cutoff=0.8)\n",
    "        if closest_match:\n",
    "            return closest_match[0]\n",
    "        else:\n",
    "            return input_word\n",
    "\n",
    "# Modeli eğit ve kaydet\n",
    "word_list_path = 'C:/ALL_NEW_TURKISH_WORDS.txt'\n",
    "with open(word_list_path, 'r', encoding='utf-8') as word_file:\n",
    "    word_list = [line.strip() for line in word_file]\n",
    "\n",
    "corrector = WordCorrector(word_list)\n",
    "\n",
    "# Modeli pickle ile kaydet\n",
    "with open('word_corrector_model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(corrector, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1350bbf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a595e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeli yükle\n",
    "with open('word_corrector_model.pkl', 'rb') as model_file:\n",
    "    loaded_corrector = pickle.load(model_file)\n",
    "\n",
    "# Kullanım\n",
    "user_input = input(\"Bir kelime girin: \").lower()\n",
    "corrected_word = loaded_corrector.correct_input(user_input)\n",
    "print(f\"Düzeltme önerisi: {corrected_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1714ee55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def turkish_spell_checker(input_text):\n",
    "    # Türkçe dil modelini kullanarak metni analiz et\n",
    "    blob = TextBlob(input_text)\n",
    "    \n",
    "    corrected_text = \"\"\n",
    "    \n",
    "    # Her kelimenin düzeltilmiş halini al\n",
    "    for word in blob.words:\n",
    "        corrected_text += str(word.correct()) + \" \"\n",
    "    \n",
    "    return corrected_text.strip()\n",
    "\n",
    "# Kullanıcıdan metin girişi al\n",
    "user_input = input(\"Türkçe metin giriniz: \")\n",
    "\n",
    "# Yazım yanlışlarını düzelt\n",
    "corrected_input = turkish_spell_checker(user_input)\n",
    "\n",
    "# Sonucu ekrana yazdır\n",
    "print(\"Düzeltildi:\", corrected_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d874aa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9d94c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\yakup\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\yakup\\AppData\\Local\\Temp\\ipykernel_10164\\932408031.py\", line 153, in <module>\n",
      "    result = parcala_ve_kontrol_et(input_str)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yakup\\AppData\\Local\\Temp\\ipykernel_10164\\932408031.py\", line -1, in parcala_ve_kontrol_et\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\yakup\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2120, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yakup\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yakup\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yakup\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yakup\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yakup\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yakup\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yakup\\anaconda3\\Lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yakup\\anaconda3\\Lib\\site-packages\\stack_data\\core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yakup\\anaconda3\\Lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yakup\\anaconda3\\Lib\\site-packages\\stack_data\\core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yakup\\anaconda3\\Lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yakup\\anaconda3\\Lib\\site-packages\\stack_data\\core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"C:\\Users\\yakup\\anaconda3\\Lib\\site-packages\\executing\\executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "def parcala_ve_kontrol_et(input_str):\n",
    "    ek_dosyalar = {\n",
    "        \n",
    "        \"Opt\":[\"yalım\",\"yelim\",\"alım\",\"elim\"],\n",
    "        \"Opt+A2sg\":[\"yasın\",\"yesin\",\"asın\",\"esin\"],\n",
    "        \"Opt+A2pl\":[\"yasınız\",\"yesiniz\",\"asınız\",\"esiniz\"],\n",
    "        \"Opt+A1sg\":[\"yayım\",\"yeyim\",\"ayım\",\"eyim\"],\n",
    "        ## noun suffixes\n",
    "        \"Pl\": [\"lar\", \"ler\"],\n",
    "        ## case suffixes\n",
    "        \"Dat\": [\"ya\",\"ye\",\"e\",\"a\",\"na\",\"ne\"],\n",
    "        \"Loc\": [\"da\", \"de\", \"ta\", \"te\",\"nde\",\"nda\"],\n",
    "        \"Abl\": [\"dan\", \"den\", \"tan\", \"ten\",\"ndan\",\"nden\"],\n",
    "        \"Gen\": [\"nın\", \"nin\",\"nun\",\"nün\",\"ın\",\"in\",\"un\", \"ün\",\"yın\",\"yin\",\"yun\",\"yün\",\"ım\",\"im\",\"um\",\"üm\",\"m\"],\n",
    "        \"Acc\": [\"yı\", \"yi\", \"yu\", \"yü\", \"ı\", \"i\", \"u\",\"ü\",\"nı\",\"ni\",\"nu\",\"nü\"],\n",
    "        \"Inst\": [\"yla\", \"yle\",\"la\",\"le\"],\n",
    "        \"P1sg\": [\"ım\",\"im\",\"um\",\"üm\",\"m\",\"yum\"],\n",
    "        \"P2sg\": [\"ın\",\"in\",\"un\",\"ün\",\"n\",\"yun\"],\n",
    "        \"P3sg\": [\"sı\", \"si\", \"su\", \"sü\",\"ı\",\"i\",\"u\",\"ü\",\"yu\"],\n",
    "        \"P1pl\": [\"ımız\", \"imiz\", \"umuz\", \"ümüz\",\"mız\",\"miz\",\"muz\",\"müz\",\"yumuz\"],\n",
    "        \"P2pl\": [\"ınız\",\"iniz\",\"unuz\",\"ünüz\",\"nız\",\"niz\",\"nuz\",\"nüz\",\"yunuz\"],\n",
    "        \"P3pl\": [\"ı\", \"i\",\"u\",\"ü\",\"ları\",\"leri\"],\n",
    "        \"Dim\": [\"cık\", \"cik\",\"cuk\",\"cük\",\"çık\",\"çik\",\"çuk\",\"çük\",\"cığ\",\"ciğ\",\"cuğ\",\"cüğ\",\"çığ\",\"çiğ\",\"çuğ\",\"çüğ\"],\n",
    "        \"Dim2\": [\"cağız\", \"ceğiz\"],\n",
    "        \"With\":[\"lı\",\"li\",\"lu\",\"lü\"],\n",
    "        \"Without\": [\"sız\", \"siz\",\"suz\",\"süz\"],\n",
    "        \"Since\":[\"dır\",\"dir\",\"dur\",\"dür\",\"tır\",\"tir\",\"tur\",\"tür\"],\n",
    "        \"Related\": [\"sal\", \"sel\"],\n",
    "        \"Ness\":[\"lık\",\"lik\",\"luk\",\"lük\",\"lığ\",\"liğ\",\"luğ\",\"lüğ\"],\n",
    "        \"Acquire\": [\"lan\", \"len\"],\n",
    "        \"Apply\":[\"la\",\"le\"],\n",
    "        \"Resemb\": [\"ımsı\", \"imsi\",\"umsu\",\"ümsü\",\"msı\",\"msi\",\"msu\",\"msü\"],\n",
    "        \"Rel\":[\"kı\",\"ki\",\"ku\",\"kü\"],\n",
    "        \"By\": [\"ca\", \"ce\"],\n",
    "        \"Cmp\":[\"ca\",\"ce\"],\n",
    "        \"Agt\":[\"cı\",\"ci\",\"cu\",\"cü\",\"yıcı\",\"yici\",\"yucu\",\"yücü\"],\n",
    "        ## Adjective Suffixes\n",
    "        \"Become\":[\"laş\",\"leş\"],\n",
    "        \"Ly\":[\"ca\",\"ce\"],\n",
    "        \"Quite\":[\"ca\",\"ce\"],\n",
    "        ## number-person agreement suffixes\n",
    "        \"A1sg\": [\"yım\", \"yim\",\"yum\",\"yüm\",\"m\",\"ım\",\"im\",\"um\",\"üm\"],\n",
    "        \"A2sg\":[\"sın\",\"sin\",\"sun\",\"sün\",\"n\",\"ın\",\"in\",\"un\",\"ün\"],\n",
    "        \"A3sg\":[\"\"],\n",
    "        \"A1pl\": [\"yız\",\"yiz\",\"k\",\"yuz\",\"yüz\",\"ız\",\"iz\",\"uz\",\"üz\"],\n",
    "        \"A2pl\":[\"sınız\",\"siniz\",\"sunuz\",\"sünüz\",\"nız\",\"niz\",\"nuz\",\"nüz\",\"ınız\",\"iniz\",\"unuz\",\"ünüz\"],\n",
    "        \"A3pl\": [\"lar\",\"ler\"],\n",
    "        ### Numeral suffixes\n",
    "        \"Grouping\": [\"şar\",\"şer\",\"ız\",\"iz\",\"ar\",\"er\"],\n",
    "        \"Ordinal\":[\"ıncı\",\"inci\",\"uncu\",\"üncü\",\"ncı\",\"nci\",\"ncu\",\"ncü\"],\n",
    "        \"Division\":[\"da\",\"de\",\"ta\",\"te\"],\n",
    "        ### verbal suffixes\n",
    "        \"Aor\":[\"ır,ir,ur,ür,ar,er,z\"],\n",
    "        \"Prog\":[\"ıyor\",\"iyor\",\"uyor\",\"üyor\"],\n",
    "        \"Prog2\":[\"makta\",\"mekte\"],\n",
    "        \"Fut\":[\"yacak\",\"yecek\",\"yacağ\",\"yeceğ\",\"acak\",\"ecek\",\"acağ\",\"eceğ\"],\n",
    "        \"Past\":[\"dı\",\"di\",\"du\",\"dü\",\"tı\",\"ti\",\"tu\",\"tü\"],\n",
    "        \"Evid\":[\"mış\",\"miş\",\"muş\",\"müş\"],\n",
    "        \"Neg\":[\"ma,me,m\"],\n",
    "        \"Cond\":[\"se\",\"sa\"],\n",
    "        \"Necess\":[\"meli\",\"malı\"],\n",
    "        \"Opt\":[\"ye\",\"ya\",\"e\",\"a\"],\n",
    "        \"Imp\":[\"sın\",\"sin\",\"sun\",\"sün\",\"yın\",\"yin\",\"yun\",\"yün\",\"ın\",\"in\",\"un\",\"ün\",\"sınlar\",\"sinler\",\"sünler\",\"sunlar\",\"yınız\",\"yiniz\",\"yunuz\",\"yünüz\",\"ınız\",\"iniz\",\"unuz\",\"ünüz\"],\n",
    "        ### copular suffixes\n",
    "        \"PastCop\":[\"ydı\",\"ydi\",\"ydu\",\"ydü\",\"dı\",\"di\",\"du\",\"dü\"],\n",
    "        \"EvidCop\":[\"ymış\",\"ymiş\",\"ymuş\",\"ymüş\",\"mış\",\"miş\",\"muş\",\"müş\"],\n",
    "        \"CondCop\":[\"ysa\",\"yse\",\"sa\",\"se\"],\n",
    "        \"While\":[\"yken\",\"ken\"],\n",
    "        \"Cop\":[\"dır\",\"dir\",\"dur\",\"dür\",\"tır\",\"tir\",\"tur\",\"tür\"],\n",
    "        ### Derivational Suffixes\n",
    "        \"Pass\":[\"ıl\",\"il\",\"ul\",\"ül\",\"ın\",\"in\",\"un\",\"ün\",\"n\",\"nıl\",\"nil\",\"nul\",\"nül\"],\n",
    "        \"Caus\":[\"dır\",\"dir\",\"dur\",\"dür\",\"tır\",\"tir\",\"tur\",\"tür\",\"t\"],\n",
    "        \"Recip\":[\"yış\",\"yiş\",\"yuş\",\"yüş\",\"ış\",\"iş\",\"uş\",\"üş\",\"ş\"],\n",
    "        \"Reflex\":[\"ın\",\"in\",\"un\",\"ün\"],\n",
    "        \"Abil\":[\"yabıl\",\"yabil\",\"yabul\",\"yabül\",\"yebil\",\"yebul\",\"yebül\",\"abıl\",\"ebil\",\"abul\",\"abül\",\"ebıl\",\"ebul\",\"ebül\"],\n",
    "        \"NegAbil\":[\"yama\",\"yeme\",\"ama\",\"eme\"],\n",
    "        \"Cont\":[\"yadur\",\"yedur\",\"adur\",\"edur\"],\n",
    "        \"EverSince\":[\"yagel\",\"yegel\",\"agel\",\"egel\"],\n",
    "        \"Cont2\":[\"yagör\",\"yegör\",\"agör\",\"egör\"],\n",
    "        \"Almost\":[\"yayaz\",\"yeyez\",\"yayez\",\"yeyaz\",\"ayaz\",\"eyez\",\"ayez\",\"eyaz\"],\n",
    "        \"Hastily\":[\"yıver\",\"yiver\",\"ıver\",\"iver\"],\n",
    "        \"Stay\":[\"yakal\",\"yekal\",\"akal\",\"ekal\"],\n",
    "        \"Inf\":[\"mak\",\"mek\"],\n",
    "        \"Inf2\":[\"ma\",\"me\"],\n",
    "        \"Inf3\":[\"yış\",\"yiş\",\"yuş\",\"yüş\",\"ış\",\"iş\",\"uş\",\"üş\"],\n",
    "        \"FutPart\":[\"yacak\",\"yecek\",\"yacağ\",\"yeceğ\",\"acak\",\"ecek\",\"acağ\",\"eceğ\"],\n",
    "        \"EvidPart\":[\"mış\",\"miş\",\"muş\",\"müş\"],\n",
    "        \"PastPart\":[\"dık\",\"dik\",\"duk\",\"dük\",\"tık\",\"tik\",\"tuk\",\"tük\",\"dığ\",\"diğ\",\"duğ\",\"düğ\",\"tığ\",\"tiğ\",\"tuğ\",\"tüğ\"],\n",
    "        \"PresPart\":[\"yan\",\"yen\",\"an\",\"en\"],\n",
    "        \"AorPart\":[\"ır\",\"ir\",\"ur\",\"ür\",\"ar\",\"er\",\"r\"],\n",
    "        \"AfterDoing\":[\"yıp\",\"yip\",\"yup\",\"yüp\",\"ıp\",\"ip\",\"up\",\"üp\"],\n",
    "        \"Agt\":[\"yıcı\",\"yici\",\"yucu\",\"yücü\",\"ıcı\",\"ici\",\"ucu\",\"ücü\"],\n",
    "        \"FeelLike\":[\"yası\",\"yesi\",\"ası\",\"esi\"],\n",
    "        \"SinceDoing\":[\"yalı\",\"yeli\",\"alı\",\"eli\"],\n",
    "        \"AsLongAs\":[\"dıkca\",\"dikce\",\"dıkça\",\"dikçe\",\"tıkca\",\"tikce\",\"tıkça\",\"tikçe\"],\n",
    "        \"JustAfter\":[\"yınca\",\"yince\",\"ınca\",\"ince\"],\n",
    "        \"ByDoing\":[\"yarak\",\"yerek\",\"arak\",\"erek\"],\n",
    "        \"WithoutDoing\":[\"madan\",\"meden\",\"adan\",\"eden\"],\n",
    "        \"WithoutDoing2\":[\"maksızın\",\"meksizin\",\"aksızın\",\"eksizin\"],\n",
    "        \"RatherThanInsteadOfDoing\":[\"maktansa\",\"mektense\",\"aktansa\",\"ektense\"],\n",
    "        \"UnableToDo\":[\"yamadan\",\"yemeden\",\"amadan\",\"emeden\"],\n",
    "        \"ActOf\":[\"maca\",\"mece\"],\n",
    "        \"NotState\":[\"mazlık\",\"mezlik\",\"mazlığ\",\"mezliğ\"],\n",
    "        \"AsIf\":[\"casına\",\"cesine\",\"çasına\",\"çesine\"],\n",
    "        \"Adamantly\":[\"yasıya\",\"yesiye\",\"asıya\",\"esiye\"],\n",
    "\n",
    "    }\n",
    "    ozel_durumlar = {\n",
    "        #\"ımız\": [\"ım\", \"ız\"],\n",
    "        #\"sinler\":[\"s\",\"i\",\"n\",\"ler\"],\n",
    "        #\"leri\":[\"ler\",\"i\"],\n",
    "        #\"sının\":[\"sı\",\"nın\"],\n",
    "        #\"elim\":[\"e\",\"lim\"],\n",
    "\n",
    "    }\n",
    "\n",
    "    sonuclar = []\n",
    "\n",
    "    while input_str:\n",
    "        en_uzun_ekler = []\n",
    "        ek_adi_list = []\n",
    "\n",
    "        for ad, ekler in ek_dosyalar.items():\n",
    "            for ek in ekler:\n",
    "                if input_str.startswith(ek):\n",
    "                    en_uzun_ekler.append(ek)\n",
    "\n",
    "        for ozel_ek, ayrilma_sekli in ozel_durumlar.items():\n",
    "            if input_str.startswith(ozel_ek):\n",
    "                for ayrilma in ayrilma_sekli:\n",
    "                    for ad, ekler in ek_dosyalar.items():\n",
    "                        if ayrilma in ekler and input_str.startswith(ayrilma):\n",
    "                            en_uzun_ekler.append(ayrilma)\n",
    "\n",
    "        if en_uzun_ekler:\n",
    "            for en_uzun_ek in en_uzun_ekler:\n",
    "                input_str_copy = input_str\n",
    "                ek_adi_list_copy = []\n",
    "\n",
    "                for ad, ekler in ek_dosyalar.items():\n",
    "                    if en_uzun_ek in ekler:\n",
    "                        ek_adi_list_copy.append(f\"({ad})\")\n",
    "\n",
    "                sonuclar.append((f\"-{en_uzun_ek} \", ek_adi_list_copy))\n",
    "                input_str_copy = input_str_copy[len(en_uzun_ek):].strip()\n",
    "\n",
    "            input_str = input_str[len(en_uzun_ek):].strip()\n",
    "\n",
    "    return sonuclar\n",
    "\n",
    "# Kullanım örneği:\n",
    "input_str = \"ımsın\"\n",
    "result = parcala_ve_kontrol_et(input_str)\n",
    "for res in result:\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "040d2518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bir kelime veya cümle giriniz: ımsın\n",
      "Girilen metnin ekleri:\n",
      "ı\n",
      "n\n",
      "ım\n",
      "ımsı\n",
      "sın\n"
     ]
    }
   ],
   "source": [
    "def ek_ayir(input_str, ek_listesi):\n",
    "    sonuclar = []\n",
    "\n",
    "    for ek in ek_listesi:\n",
    "        if input_str.endswith(ek):\n",
    "            sonuclar.append(ek)\n",
    "        if input_str.startswith(ek):\n",
    "            sonuclar.append(ek)\n",
    "\n",
    "    return sonuclar\n",
    "\n",
    "# Ek listesi\n",
    "ekler = [\"ı\", \"m\", \"sı\", \"n\", \"ım\", \"ımsı\", \"sın\"]\n",
    "\n",
    "# Kullanıcıdan input alınması\n",
    "input_metni = input(\"Bir kelime veya cümle giriniz: \")\n",
    "\n",
    "# Ekleri ayırma fonksiyonunun çağrılması\n",
    "ayrilmis_ekler = ek_ayir(input_metni, ekler)\n",
    "\n",
    "# Sonuçların ekrana yazdırılması\n",
    "print(\"Girilen metnin ekleri:\")\n",
    "for ek in ayrilmis_ekler:\n",
    "    print(ek)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f44ad4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lütfen bir kelime veya ifade girin: lerin\n",
      "Bu kelime şu şekilde ayrılabilir:\n",
      "lerin-,\n",
      "lerin-,\n",
      "ler-in,\n",
      "leri-n,\n"
     ]
    }
   ],
   "source": [
    "def ek_ayirma(input_str, ek_listesi):\n",
    "    sonuclar = set()\n",
    "\n",
    "    for i in range(len(input_str)):\n",
    "        for ek in ek_listesi:\n",
    "            if input_str[i:].startswith(ek):\n",
    "                sonuclar.add(ek)\n",
    "\n",
    "    sirali_sonuclar = sorted(list(sonuclar), key=len)\n",
    "    \n",
    "    print(f\"Bu kelime şu şekilde ayrılabilir:\")\n",
    "    onceki_ayrim = \"\"\n",
    "    for sirali_ek in sirali_sonuclar:\n",
    "        if sirali_ek == input_str or sirali_ek == onceki_ayrim:\n",
    "            continue  # Tam kelimeyi veya aynı ayrımı tekrar ekleme\n",
    "        ayrilmis_kelime = input_str.split(sirali_ek)\n",
    "        ayrilmis_kelime_str = f\"{sirali_ek}-\".join(ayrilmis_kelime)\n",
    "        print(f\"{ayrilmis_kelime_str},\")\n",
    "        onceki_ayrim = sirali_ek\n",
    "\n",
    "# Ek listesi\n",
    "ek_listesi = [\"ler\",\"leri\",\"in\",\"n\"]\n",
    "\n",
    "# Kullanıcıdan input alınması\n",
    "input_str = input(\"Lütfen bir kelime veya ifade girin: \")\n",
    "\n",
    "# Ekleri ayırma fonksiyonunu çağırma\n",
    "ek_ayirma(input_str, ek_listesi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e6818a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf8e830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f07e270d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bir kelime veya cümle girin: lerin\n",
      "Ayrılmış ekler:\n",
      " + leri (tursuEki EK) + n (mantarEki EK)\n",
      " + ler (sebzeEki EK) + in (sebzeEki EK)\n"
     ]
    }
   ],
   "source": [
    "def ek_ayir(input_str, ek_dosyalar):\n",
    "    sonuclar = set()\n",
    "\n",
    "    for ek_turu1, ek_listesi1 in ek_dosyalar.items():\n",
    "        for ek1 in ek_listesi1:\n",
    "            for ek_turu2, ek_listesi2 in ek_dosyalar.items():\n",
    "                for ek2 in ek_listesi2:\n",
    "                    if input_str.endswith(ek1 + ek2):\n",
    "                        birlesik_ek = (ek1, ek_turu1, ek2, ek_turu2)\n",
    "                        sonuclar.add((input_str[:len(input_str) - len(ek1 + ek2)], birlesik_ek))\n",
    "\n",
    "    return sonuclar\n",
    "\n",
    "def main():\n",
    "    ek_dosyalar = {\n",
    "        \"sebzeEki\": [\"ler\", \"in\"],\n",
    "        \"tursuEki\": [\"leri\"],\n",
    "        \"mantarEki\": [\"n\"],\n",
    "    }\n",
    "\n",
    "    input_str = input(\"Bir kelime veya cümle girin: \")\n",
    "\n",
    "    sonuclar = ek_ayir(input_str, ek_dosyalar)\n",
    "\n",
    "    if sonuclar:\n",
    "        print(\"Ayrılmış ekler:\")\n",
    "        for kalan, birlesik_ek in sonuclar:\n",
    "            ek1, ek_turu1, ek2, ek_turu2 = birlesik_ek\n",
    "            print(f\"{kalan} + {ek1} ({ek_turu1} EK) + {ek2} ({ek_turu2} EK)\")\n",
    "    else:\n",
    "        print(\"Girilen kelime veya cümlede belirtilen ek bulunamadı.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53030658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sebzeler\n"
     ]
    }
   ],
   "source": [
    "input_str = \"sebzelerin\"\n",
    "en_uzun_ek = \"in\"\n",
    "\n",
    "\n",
    "input_str = input_str[:-len(en_uzun_ek)].strip()\n",
    "\n",
    "print(input_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a0bca2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n\u001b[0;32m      5\u001b[0m cumle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mo sene bu sene olacak umarım\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 7\u001b[0m deger\u001b[38;5;241m=\u001b[39mtokenizer(cumle)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m deger:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(x)\n",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m, in \u001b[0;36mtokenizer\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenizer\u001b[39m(text):\n\u001b[1;32m----> 2\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n",
      "\u001b[1;31mTypeError\u001b[0m: 'str' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "def tokenizer(text):\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "cumle=\"o sene bu sene olacak umarım\"\n",
    "\n",
    "deger=tokenizer(cumle)\n",
    "\n",
    "for x in deger:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e40780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897098ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
